[INFO ] [2018-04-26 17:06:53] [Logging$class:logInfo:54] Running Spark version 2.2.1
[ERROR] [2018-04-26 17:06:54] [Logging$class:logError:91] Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:376)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:36)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
[INFO ] [2018-04-26 17:06:54] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 17:09:29] [Logging$class:logInfo:54] Running Spark version 2.2.1
[ERROR] [2018-04-26 17:09:30] [Logging$class:logError:91] Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:376)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:36)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
[INFO ] [2018-04-26 17:09:30] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 17:22:30] [Logging$class:logInfo:54] Running Spark version 2.2.1
[ERROR] [2018-04-26 17:22:31] [Logging$class:logError:91] Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:376)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:36)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
[INFO ] [2018-04-26 17:22:31] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 17:22:59] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 17:23:00] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 17:23:00] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 17:23:00] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 17:23:00] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 17:23:00] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 17:23:00] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56326.
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-e2a7188a-11f1-480b-80ea-77872ecd7cb5
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56335.
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Server created on 192.168.0.152:56335
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56335, None)
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56335 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56335, None)
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56335, None)
[INFO ] [2018-04-26 17:23:01] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56335, None)
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 17:23:02] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-8c9237f0-f91e-4457-bf6e-0497ac902954
[INFO ] [2018-04-26 17:26:22] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 17:26:22] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 17:26:22] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 17:26:22] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 17:26:22] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 17:26:22] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 17:26:22] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56381.
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-e448dc00-3120-45fe-a67e-f5016c56656e
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56390.
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Server created on 192.168.0.152:56390
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56390, None)
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56390 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56390, None)
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56390, None)
[INFO ] [2018-04-26 17:26:23] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56390, None)
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 17:26:24] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-254fa948-1b24-476c-9a62-b65a3fc0fd45
[INFO ] [2018-04-26 17:27:14] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56422.
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-870ff706-5900-4b30-a467-23524a23c05d
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 17:27:15] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56431.
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Server created on 192.168.0.152:56431
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56431, None)
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56431 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56431, None)
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56431, None)
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56431, None)
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 17:27:16] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-0ed3733c-ec42-4071-a0e7-a3cea03e44af
[INFO ] [2018-04-26 17:44:09] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 17:44:09] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 17:44:09] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 17:44:09] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 17:44:09] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 17:44:09] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 17:44:09] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 17:44:10] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 57847.
[INFO ] [2018-04-26 17:44:10] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 17:44:10] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 17:44:10] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 17:44:10] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 17:44:10] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-7327d3a3-2db6-47a1-a770-58bd1525f2ac
[INFO ] [2018-04-26 17:44:10] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57856.
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Server created on 192.168.0.152:57856
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 57856, None)
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:57856 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 57856, None)
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 57856, None)
[INFO ] [2018-04-26 17:44:11] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 57856, None)
[INFO ] [2018-04-26 17:44:15] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:57856 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:57856 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:57856 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 17:44:16] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 17:44:19] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 17:44:19] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 17:44:19] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 17:44:20] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 17:44:30] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-37ebe7f7-25d3-4913-84fe-b613801ff752
[INFO ] [2018-04-26 18:04:49] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 59871.
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-ffb024fd-5753-477d-b69f-4fd841bbf87b
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59880.
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Server created on 192.168.0.152:59880
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:04:50] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 59880, None)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:59880 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 59880, None)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 59880, None)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 59880, None)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:59880 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:59880 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:59880 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:04:51] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:04:53] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:04:53] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:04:53] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:04:53] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 18:07:07] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:07:08] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:07:08] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:07:08] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:07:08] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:07:08] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:07:08] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 59925.
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-1e550b74-e721-4f3e-97f6-7dcf3a3ee188
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59934.
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Server created on 192.168.0.152:59934
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 59934, None)
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:59934 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 59934, None)
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 59934, None)
[INFO ] [2018-04-26 18:07:09] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 59934, None)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:59934 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:59934 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:59934 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:07:10] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:07:11] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:07:11] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:07:11] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:07:12] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 18:09:24] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 18:09:24] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:09:24] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 18:09:25] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 18:09:25] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 18:09:25] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 18:09:25] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 18:09:25] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 18:09:25] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 18:09:25] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-16dad1bc-7d32-48d2-86a6-66e2860c905d
[INFO ] [2018-04-26 18:17:53] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:17:54] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:17:54] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:17:54] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:17:54] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:17:54] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:17:54] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 60902.
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-837b3ba1-36b8-4fb0-9419-5a4dc9674e37
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60912.
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Server created on 192.168.0.152:60912
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 60912, None)
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:60912 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 60912, None)
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 60912, None)
[INFO ] [2018-04-26 18:17:55] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 60912, None)
[INFO ] [2018-04-26 18:17:56] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:60912 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:60912 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:60912 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:17:57] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:17:59] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:17:59] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:17:59] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:17:59] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[ERROR] [2018-04-26 18:18:47] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 92 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
[ERROR] [2018-04-26 18:19:29] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 92 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
[ERROR] [2018-04-26 18:20:11] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 89 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
[ERROR] [2018-04-26 18:20:53] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 89 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 18:20:53] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-3b4d8d7b-d68b-4138-87e9-c2c622b30984
[INFO ] [2018-04-26 18:23:14] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 61232.
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-b1b1e7e3-159e-4b16-ad87-f183227b6d64
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:23:15] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61241.
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Server created on 192.168.0.152:61241
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 61241, None)
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:61241 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 61241, None)
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 61241, None)
[INFO ] [2018-04-26 18:23:16] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 61241, None)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:61241 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:61241 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:61241 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:23:17] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:23:19] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:23:19] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:23:19] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:23:19] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[ERROR] [2018-04-26 18:24:03] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 92 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
[ERROR] [2018-04-26 18:24:45] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 92 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 94 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 110 more
[ERROR] [2018-04-26 18:25:27] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 89 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
[ERROR] [2018-04-26 18:26:09] [Log4JLogger:error:125] Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:516)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	... 89 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
Nested Throwables StackTrace:
java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:mysql://192.168.0.15:3306/hive?createDatabaseIfNotExist=true, username = hive. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
------

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:587)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:344)
	at com.dr.banner.posProductProcessor$.etlProductPosOperation(posProductProcessor.scala:64)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:68)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:355)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2490)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2527)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2309)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:834)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:419)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
	... 91 more
Caused by: java.net.ConnectException: Connection timed out: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:258)
	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:305)
	... 107 more
[INFO ] [2018-04-26 18:26:09] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 18:26:09] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:26:09] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 18:26:10] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 18:26:10] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 18:26:10] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 18:26:10] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 18:26:10] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 18:26:10] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 18:26:10] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-f407af93-ae9f-4214-9779-ee97761abf21
[INFO ] [2018-04-26 18:26:57] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:26:58] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:26:58] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:26:58] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:26:58] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:26:58] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:26:58] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 61300.
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-29dc78ae-959e-446c-9366-6b184f631572
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61310.
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Server created on 192.168.0.152:61310
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 61310, None)
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:61310 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 61310, None)
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 61310, None)
[INFO ] [2018-04-26 18:26:59] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 61310, None)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:61310 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:61310 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:61310 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:27:01] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:27:03] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:27:03] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:27:03] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:27:03] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[ERROR] [2018-04-26 18:27:07] [ObjectStore:checkSchema:6684] Version information found in metastore differs 2.1.0 from expected schema version 1.2.0. Schema verififcation is disabled hive.metastore.schema.verification so setting version.
[INFO ] [2018-04-26 18:27:08] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:27:09] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:27:09] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-26 18:27:10] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-26 18:27:11] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 18:27:12] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-00620355-c154-45c0-a1b8-595b4d82946f
[INFO ] [2018-04-26 18:32:40] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:32:41] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:32:41] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:32:41] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:32:41] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:32:41] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:32:41] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 61401.
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-c04de61c-06e2-4b9f-908c-9f6c305af1ad
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:32:42] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61412.
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Server created on 192.168.0.152:61412
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 61412, None)
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:61412 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 61412, None)
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 61412, None)
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 61412, None)
[INFO ] [2018-04-26 18:32:43] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:61412 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:61412 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:61412 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:32:44] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:32:45] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:32:45] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:32:45] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:32:46] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 18:32:50] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:32:50] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:32:50] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:32:51] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 18:32:52] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 18:32:52] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 18:32:52] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 18:32:52] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 18:32:52] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 18:32:52] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 18:32:52] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-6523c328-10cd-439f-b2bd-987278930778
[INFO ] [2018-04-26 18:41:24] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 62562.
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-46431cd8-8e8c-403d-85db-cf943f349dda
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:41:25] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62571.
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Server created on 192.168.0.152:62571
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 62571, None)
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:62571 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 62571, None)
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 62571, None)
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 62571, None)
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:62571 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:41:26] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:41:27] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:41:27] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:62571 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:41:27] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:41:27] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:41:27] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:41:27] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:62571 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:41:27] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:41:28] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:41:28] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:41:28] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:41:28] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 18:41:34] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-2f986033-59c3-45ac-ac47-b9633d6e0e84
[INFO ] [2018-04-26 18:49:13] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 18:49:13] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 18:49:13] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 18:49:13] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 18:49:13] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 18:49:13] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 18:49:13] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 62690.
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-1353c309-7025-412c-9839-c0c305d762e0
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62699.
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Server created on 192.168.0.152:62699
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 62699, None)
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:62699 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 62699, None)
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 62699, None)
[INFO ] [2018-04-26 18:49:14] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 62699, None)
[INFO ] [2018-04-26 18:49:15] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:62699 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:62699 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:62699 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:49:16] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 18:49:18] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 18:49:18] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 18:49:18] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 18:49:18] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[ERROR] [2018-04-26 18:49:21] [ObjectStore:checkSchema:6684] Version information found in metastore differs 2.1.0 from expected schema version 1.2.0. Schema verififcation is disabled hive.metastore.schema.verification so setting version.
[INFO ] [2018-04-26 18:49:22] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-26 18:49:23] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-26 18:49:24] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-26 18:49:29] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 18:49:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 18:49:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Code generated in 225.226048 ms
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 226.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.1 MB)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:62699 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Created broadcast 3 from 
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 11.9 KB, free 1991.1 MB)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:62699 (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-26 18:49:30] [Logging$class:logInfo:54] Code generated in 16.299802 ms
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 73022 bytes result sent to driver
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 525 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.546 s
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.647511 s
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Code generated in 10.43861 ms
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Removed broadcast_4_piece0 on 192.168.0.152:62699 in memory (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 64.5 MB, free 1926.6 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 204.8 KB, free 1926.4 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:62699 (size: 204.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Created broadcast 5 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Code generated in 30.790236 ms
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Code generated in 18.7605 ms
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:101
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Registering RDD 25 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Registering RDD 29 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:101) with 200 output partitions
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 17.3 KB, free 1926.4 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1926.3 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:62699 (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Block broadcast_7 stored as values in memory (estimated size 12.3 KB, free 1926.3 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.3 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Added broadcast_7_piece0 in memory on 192.168.0.152:62699 (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Code generated in 23.450057 ms
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Code generated in 15.559743 ms
[ERROR] [2018-04-26 18:49:31] [Logging$class:logError:91] Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$1$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:28)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:26)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:20)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-26 18:49:31] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[WARN ] [2018-04-26 18:49:32] [Logging$class:logWarning:66] Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$1$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:28)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:26)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:20)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-26 18:49:32] [Logging$class:logError:70] Task 0 in stage 1.0 failed 1 times; aborting job
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Cancelling stage 1
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:101) failed in 0.262 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$1$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:28)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:26)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:20)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Stage 2 was cancelled
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:101) failed in 0.250 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$1$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:28)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:26)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:20)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Executor is trying to kill task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:101, took 0.302791 s
[ERROR] [2018-04-26 18:49:32] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$1$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:28)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:26)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:20)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:101)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$1$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:28)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:26)
	at com.dr.banner.posProductProcessor$$anonfun$1.apply(posProductProcessor.scala:20)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Code generated in 20.82549 ms
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Code generated in 13.987495 ms
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Executor killed task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[WARN ] [2018-04-26 18:49:32] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 18:49:32] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-bd4e6ffc-efc5-42ff-aaf1-6cae3e559d26
[INFO ] [2018-04-26 19:09:05] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 19:09:06] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 19:09:06] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 19:09:06] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 19:09:06] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 19:09:06] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 19:09:06] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 62988.
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-406823de-07a9-494b-a131-0900645fb92a
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62997.
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Server created on 192.168.0.152:62997
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 62997, None)
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:62997 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 62997, None)
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 62997, None)
[INFO ] [2018-04-26 19:09:07] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 62997, None)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:62997 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:62997 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:62997 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:09:08] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:09:10] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 19:09:10] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 19:09:10] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 19:09:10] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 19:09:14] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-26 19:09:15] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-26 19:09:16] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-26 19:09:20] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Code generated in 211.985786 ms
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 226.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:62997 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Created broadcast 3 from 
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 11.9 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:62997 (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:09:21] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Code generated in 13.187779 ms
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 73065 bytes result sent to driver
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 452 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.469 s
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.552780 s
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Code generated in 11.674433 ms
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 64.5 MB, free 1926.6 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 204.8 KB, free 1926.4 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:62997 (size: 204.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Created broadcast 5 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Code generated in 30.278235 ms
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Code generated in 17.668534 ms
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:101
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Registering RDD 29 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Registering RDD 25 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:101) with 200 output partitions
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 12.3 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:62997 (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Block broadcast_7 stored as values in memory (estimated size 17.3 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Added broadcast_7_piece0 in memory on 192.168.0.152:62997 (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-26 19:09:22] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Code generated in 12.850976 ms
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Code generated in 15.878422 ms
[ERROR] [2018-04-26 19:09:23] [Logging$class:logError:91] Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$7$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:83)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:81)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:75)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[WARN ] [2018-04-26 19:09:23] [Logging$class:logWarning:66] Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$7$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:83)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:81)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:75)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-26 19:09:23] [Logging$class:logError:70] Task 0 in stage 1.0 failed 1 times; aborting job
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Cancelling stage 1
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:101) failed in 0.214 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$7$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:83)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:81)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:75)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Stage 2 was cancelled
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:101) failed in 0.221 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$7$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:83)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:81)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:75)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Code generated in 58.566996 ms
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:101, took 0.280419 s
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Executor is trying to kill task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[ERROR] [2018-04-26 19:09:23] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$7$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:83)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:81)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:75)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:101)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
	at com.dr.banner.posProductProcessor$$anonfun$7$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:83)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:81)
	at com.dr.banner.posProductProcessor$$anonfun$7.apply(posProductProcessor.scala:75)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Code generated in 29.106601 ms
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Executor killed task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Cleaned accumulator 11
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Cleaned accumulator 8
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Cleaned accumulator 9
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Cleaned accumulator 16
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Cleaned accumulator 19
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[WARN ] [2018-04-26 19:09:23] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Removed broadcast_5_piece0 on 192.168.0.152:62997 in memory (size: 204.8 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 19:09:23] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-e1905e30-c5f5-4578-8fc4-2cdd479f7730
[INFO ] [2018-04-26 19:11:48] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 63051.
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-ce858b61-ad22-4af6-b859-45e658153453
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 19:11:49] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63060.
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Server created on 192.168.0.152:63060
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 63060, None)
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:63060 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 63060, None)
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 63060, None)
[INFO ] [2018-04-26 19:11:50] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 63060, None)
[INFO ] [2018-04-26 19:11:51] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 19:11:51] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 19:11:51] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:63060 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 19:11:51] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:63060 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:63060 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:11:52] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:11:54] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 19:11:54] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 19:11:54] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 19:11:54] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 19:11:59] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 19:11:59] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 19:11:59] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-26 19:11:59] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-26 19:12:00] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-26 19:12:05] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:05] [Logging$class:logInfo:54] Code generated in 259.816639 ms
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 226.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:63060 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Created broadcast 3 from 
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 11.9 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:63060 (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Code generated in 10.773524 ms
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 73022 bytes result sent to driver
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 420 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.437 s
[INFO ] [2018-04-26 19:12:06] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.514654 s
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Code generated in 8.54504 ms
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 64.5 MB, free 1926.6 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 204.8 KB, free 1926.4 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:63060 (size: 204.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Created broadcast 5 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Code generated in 28.359746 ms
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Code generated in 85.248016 ms
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Removed broadcast_4_piece0 on 192.168.0.152:63060 in memory (size: 6.0 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:101
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Registering RDD 25 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Registering RDD 29 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:101) with 200 output partitions
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 17.3 KB, free 1926.4 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:63060 (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Block broadcast_7 stored as values in memory (estimated size 12.3 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Code generated in 122.48092 ms
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Code generated in 26.216595 ms
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Added broadcast_7_piece0 in memory on 192.168.0.152:63060 (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:12:07] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1410 bytes result sent to driver
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 744 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:101) finished in 0.749 s
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 9.155211 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 11.78091 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2). 1367 bytes result sent to driver
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2) in 224 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:101) finished in 0.668 s
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] running: Set()
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Submitting ResultStage 3 (MapPartitionsRDD[34] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Block broadcast_8 stored as values in memory (estimated size 113.8 KB, free 1926.2 MB)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Block broadcast_8_piece0 stored as bytes in memory (estimated size 42.8 KB, free 1926.2 MB)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Added broadcast_8_piece0 in memory on 192.168.0.152:63060 (size: 42.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[34] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Adding task set 3.0 with 200 tasks
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Running task 0.0 in stage 3.0 (TID 3)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Started 0 remote fetches in 5 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 41.690627 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 18.244346 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 9.604155 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 13.867047 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 16.355685 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 5.987683 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 8.261477 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 8.869005 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 6.214232 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 32.116679 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Code generated in 18.651756 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191208_0003_m_000000_0
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3). 3732 bytes result sent to driver
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Running task 1.0 in stage 3.0 (TID 4)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3) in 374 ms on localhost (executor driver) (1/200)
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:08] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Removed broadcast_6_piece0 on 192.168.0.152:63060 in memory (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Removed broadcast_7_piece0 on 192.168.0.152:63060 in memory (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000001_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4). 3732 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 2.0 in stage 3.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 2.0 in stage 3.0 (TID 5)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4) in 69 ms on localhost (executor driver) (2/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000002_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 3.0 in stage 3.0 (TID 6, localhost, executor driver, partition 3, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5) in 32 ms on localhost (executor driver) (3/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 3.0 in stage 3.0 (TID 6)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000003_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 4.0 in stage 3.0 (TID 7, localhost, executor driver, partition 4, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 4.0 in stage 3.0 (TID 7)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6) in 33 ms on localhost (executor driver) (4/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000004_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 5.0 in stage 3.0 (TID 8, localhost, executor driver, partition 5, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7) in 33 ms on localhost (executor driver) (5/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 5.0 in stage 3.0 (TID 8)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000005_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 6.0 in stage 3.0 (TID 9, localhost, executor driver, partition 6, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8) in 110 ms on localhost (executor driver) (6/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 6.0 in stage 3.0 (TID 9)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000006_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 7.0 in stage 3.0 (TID 10, localhost, executor driver, partition 7, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9) in 45 ms on localhost (executor driver) (7/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 7.0 in stage 3.0 (TID 10)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000007_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 8.0 in stage 3.0 (TID 11, localhost, executor driver, partition 8, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 8.0 in stage 3.0 (TID 11)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10) in 41 ms on localhost (executor driver) (8/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000008_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 9.0 in stage 3.0 (TID 12, localhost, executor driver, partition 9, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11) in 30 ms on localhost (executor driver) (9/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 9.0 in stage 3.0 (TID 12)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000009_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 10.0 in stage 3.0 (TID 13, localhost, executor driver, partition 10, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 10.0 in stage 3.0 (TID 13)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12) in 28 ms on localhost (executor driver) (10/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000010_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 11.0 in stage 3.0 (TID 14, localhost, executor driver, partition 11, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 11.0 in stage 3.0 (TID 14)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13) in 30 ms on localhost (executor driver) (11/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000011_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 12.0 in stage 3.0 (TID 15, localhost, executor driver, partition 12, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14) in 26 ms on localhost (executor driver) (12/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 12.0 in stage 3.0 (TID 15)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000012_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 13.0 in stage 3.0 (TID 16, localhost, executor driver, partition 13, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 13.0 in stage 3.0 (TID 16)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15) in 30 ms on localhost (executor driver) (13/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000013_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 14.0 in stage 3.0 (TID 17, localhost, executor driver, partition 14, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16) in 40 ms on localhost (executor driver) (14/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 14.0 in stage 3.0 (TID 17)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000014_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 15.0 in stage 3.0 (TID 18, localhost, executor driver, partition 15, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17) in 46 ms on localhost (executor driver) (15/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 15.0 in stage 3.0 (TID 18)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000015_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 16.0 in stage 3.0 (TID 19, localhost, executor driver, partition 16, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18) in 95 ms on localhost (executor driver) (16/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 16.0 in stage 3.0 (TID 19)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000016_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 17.0 in stage 3.0 (TID 20, localhost, executor driver, partition 17, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19) in 109 ms on localhost (executor driver) (17/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 17.0 in stage 3.0 (TID 20)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 7 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000017_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 18.0 in stage 3.0 (TID 21, localhost, executor driver, partition 18, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20) in 38 ms on localhost (executor driver) (18/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 18.0 in stage 3.0 (TID 21)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000018_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 19.0 in stage 3.0 (TID 22, localhost, executor driver, partition 19, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 19.0 in stage 3.0 (TID 22)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21) in 115 ms on localhost (executor driver) (19/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000019_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 20.0 in stage 3.0 (TID 23, localhost, executor driver, partition 20, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 20.0 in stage 3.0 (TID 23)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22) in 58 ms on localhost (executor driver) (20/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 3 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000020_0
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Starting task 21.0 in stage 3.0 (TID 24, localhost, executor driver, partition 21, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Running task 21.0 in stage 3.0 (TID 24)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23) in 59 ms on localhost (executor driver) (21/200)
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191209_0003_m_000021_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 22.0 in stage 3.0 (TID 25, localhost, executor driver, partition 22, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 22.0 in stage 3.0 (TID 25)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24) in 33 ms on localhost (executor driver) (22/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000022_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 23.0 in stage 3.0 (TID 26, localhost, executor driver, partition 23, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 23.0 in stage 3.0 (TID 26)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25) in 25 ms on localhost (executor driver) (23/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000023_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 24.0 in stage 3.0 (TID 27, localhost, executor driver, partition 24, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26) in 36 ms on localhost (executor driver) (24/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 24.0 in stage 3.0 (TID 27)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000024_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 25.0 in stage 3.0 (TID 28, localhost, executor driver, partition 25, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27) in 41 ms on localhost (executor driver) (25/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 25.0 in stage 3.0 (TID 28)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000025_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 26.0 in stage 3.0 (TID 29, localhost, executor driver, partition 26, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 26.0 in stage 3.0 (TID 29)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28) in 32 ms on localhost (executor driver) (26/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000026_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 27.0 in stage 3.0 (TID 30, localhost, executor driver, partition 27, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 27.0 in stage 3.0 (TID 30)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29) in 20 ms on localhost (executor driver) (27/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000027_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 28.0 in stage 3.0 (TID 31, localhost, executor driver, partition 28, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30) in 29 ms on localhost (executor driver) (28/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 28.0 in stage 3.0 (TID 31)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000028_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 29.0 in stage 3.0 (TID 32, localhost, executor driver, partition 29, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 29.0 in stage 3.0 (TID 32)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31) in 39 ms on localhost (executor driver) (29/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 6 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000029_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 30.0 in stage 3.0 (TID 33, localhost, executor driver, partition 30, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32) in 48 ms on localhost (executor driver) (30/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 30.0 in stage 3.0 (TID 33)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000030_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 31.0 in stage 3.0 (TID 34, localhost, executor driver, partition 31, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 31.0 in stage 3.0 (TID 34)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33) in 44 ms on localhost (executor driver) (31/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000031_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 32.0 in stage 3.0 (TID 35, localhost, executor driver, partition 32, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34) in 18 ms on localhost (executor driver) (32/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 32.0 in stage 3.0 (TID 35)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000032_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 33.0 in stage 3.0 (TID 36, localhost, executor driver, partition 33, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35) in 24 ms on localhost (executor driver) (33/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 33.0 in stage 3.0 (TID 36)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000033_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 34.0 in stage 3.0 (TID 37, localhost, executor driver, partition 34, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36) in 20 ms on localhost (executor driver) (34/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 34.0 in stage 3.0 (TID 37)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000034_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 35.0 in stage 3.0 (TID 38, localhost, executor driver, partition 35, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 35.0 in stage 3.0 (TID 38)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37) in 17 ms on localhost (executor driver) (35/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000035_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 36.0 in stage 3.0 (TID 39, localhost, executor driver, partition 36, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38) in 26 ms on localhost (executor driver) (36/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 36.0 in stage 3.0 (TID 39)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000036_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 37.0 in stage 3.0 (TID 40, localhost, executor driver, partition 37, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39) in 23 ms on localhost (executor driver) (37/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 37.0 in stage 3.0 (TID 40)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000037_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 38.0 in stage 3.0 (TID 41, localhost, executor driver, partition 38, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40) in 26 ms on localhost (executor driver) (38/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 38.0 in stage 3.0 (TID 41)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000038_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 39.0 in stage 3.0 (TID 42, localhost, executor driver, partition 39, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41) in 20 ms on localhost (executor driver) (39/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 39.0 in stage 3.0 (TID 42)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000039_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 40.0 in stage 3.0 (TID 43, localhost, executor driver, partition 40, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 40.0 in stage 3.0 (TID 43)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42) in 18 ms on localhost (executor driver) (40/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000040_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 41.0 in stage 3.0 (TID 44, localhost, executor driver, partition 41, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 41.0 in stage 3.0 (TID 44)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43) in 16 ms on localhost (executor driver) (41/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000041_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 42.0 in stage 3.0 (TID 45, localhost, executor driver, partition 42, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 42.0 in stage 3.0 (TID 45)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44) in 20 ms on localhost (executor driver) (42/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000042_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 43.0 in stage 3.0 (TID 46, localhost, executor driver, partition 43, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 43.0 in stage 3.0 (TID 46)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45) in 25 ms on localhost (executor driver) (43/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000043_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 44.0 in stage 3.0 (TID 47, localhost, executor driver, partition 44, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 44.0 in stage 3.0 (TID 47)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46) in 19 ms on localhost (executor driver) (44/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000044_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 45.0 in stage 3.0 (TID 48, localhost, executor driver, partition 45, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 45.0 in stage 3.0 (TID 48)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47) in 19 ms on localhost (executor driver) (45/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000045_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 46.0 in stage 3.0 (TID 49, localhost, executor driver, partition 46, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48) in 26 ms on localhost (executor driver) (46/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 46.0 in stage 3.0 (TID 49)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000046_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 47.0 in stage 3.0 (TID 50, localhost, executor driver, partition 47, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49) in 24 ms on localhost (executor driver) (47/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 47.0 in stage 3.0 (TID 50)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000047_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 48.0 in stage 3.0 (TID 51, localhost, executor driver, partition 48, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50) in 18 ms on localhost (executor driver) (48/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 48.0 in stage 3.0 (TID 51)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000048_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 49.0 in stage 3.0 (TID 52, localhost, executor driver, partition 49, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 49.0 in stage 3.0 (TID 52)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51) in 29 ms on localhost (executor driver) (49/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000049_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 50.0 in stage 3.0 (TID 53, localhost, executor driver, partition 50, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 50.0 in stage 3.0 (TID 53)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52) in 22 ms on localhost (executor driver) (50/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000050_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 51.0 in stage 3.0 (TID 54, localhost, executor driver, partition 51, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 51.0 in stage 3.0 (TID 54)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53) in 27 ms on localhost (executor driver) (51/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000051_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 52.0 in stage 3.0 (TID 55, localhost, executor driver, partition 52, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 52.0 in stage 3.0 (TID 55)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54) in 19 ms on localhost (executor driver) (52/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000052_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 53.0 in stage 3.0 (TID 56, localhost, executor driver, partition 53, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55) in 24 ms on localhost (executor driver) (53/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 53.0 in stage 3.0 (TID 56)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000053_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 54.0 in stage 3.0 (TID 57, localhost, executor driver, partition 54, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56) in 24 ms on localhost (executor driver) (54/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 54.0 in stage 3.0 (TID 57)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000054_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 55.0 in stage 3.0 (TID 58, localhost, executor driver, partition 55, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 55.0 in stage 3.0 (TID 58)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57) in 22 ms on localhost (executor driver) (55/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000055_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 56.0 in stage 3.0 (TID 59, localhost, executor driver, partition 56, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58) in 61 ms on localhost (executor driver) (56/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 56.0 in stage 3.0 (TID 59)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000056_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 57.0 in stage 3.0 (TID 60, localhost, executor driver, partition 57, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59) in 18 ms on localhost (executor driver) (57/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 57.0 in stage 3.0 (TID 60)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000057_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 58.0 in stage 3.0 (TID 61, localhost, executor driver, partition 58, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60) in 28 ms on localhost (executor driver) (58/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 58.0 in stage 3.0 (TID 61)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000058_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 59.0 in stage 3.0 (TID 62, localhost, executor driver, partition 59, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61) in 20 ms on localhost (executor driver) (59/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 59.0 in stage 3.0 (TID 62)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000059_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 60.0 in stage 3.0 (TID 63, localhost, executor driver, partition 60, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 60.0 in stage 3.0 (TID 63)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62) in 20 ms on localhost (executor driver) (60/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000060_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 61.0 in stage 3.0 (TID 64, localhost, executor driver, partition 61, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63) in 17 ms on localhost (executor driver) (61/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 61.0 in stage 3.0 (TID 64)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191210_0003_m_000061_0
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Starting task 62.0 in stage 3.0 (TID 65, localhost, executor driver, partition 62, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64) in 18 ms on localhost (executor driver) (62/200)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Running task 62.0 in stage 3.0 (TID 65)
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000062_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 63.0 in stage 3.0 (TID 66, localhost, executor driver, partition 63, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65) in 24 ms on localhost (executor driver) (63/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 63.0 in stage 3.0 (TID 66)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000063_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 64.0 in stage 3.0 (TID 67, localhost, executor driver, partition 64, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66) in 22 ms on localhost (executor driver) (64/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 64.0 in stage 3.0 (TID 67)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000064_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 65.0 in stage 3.0 (TID 68, localhost, executor driver, partition 65, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67) in 21 ms on localhost (executor driver) (65/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 65.0 in stage 3.0 (TID 68)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000065_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 66.0 in stage 3.0 (TID 69, localhost, executor driver, partition 66, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68) in 21 ms on localhost (executor driver) (66/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 66.0 in stage 3.0 (TID 69)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000066_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 67.0 in stage 3.0 (TID 70, localhost, executor driver, partition 67, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 67.0 in stage 3.0 (TID 70)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69) in 30 ms on localhost (executor driver) (67/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000067_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 68.0 in stage 3.0 (TID 71, localhost, executor driver, partition 68, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70) in 25 ms on localhost (executor driver) (68/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 68.0 in stage 3.0 (TID 71)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000068_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 69.0 in stage 3.0 (TID 72, localhost, executor driver, partition 69, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 69.0 in stage 3.0 (TID 72)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71) in 23 ms on localhost (executor driver) (69/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000069_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 70.0 in stage 3.0 (TID 73, localhost, executor driver, partition 70, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72) in 20 ms on localhost (executor driver) (70/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 70.0 in stage 3.0 (TID 73)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000070_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 71.0 in stage 3.0 (TID 74, localhost, executor driver, partition 71, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73) in 24 ms on localhost (executor driver) (71/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 71.0 in stage 3.0 (TID 74)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000071_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 72.0 in stage 3.0 (TID 75, localhost, executor driver, partition 72, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 72.0 in stage 3.0 (TID 75)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74) in 23 ms on localhost (executor driver) (72/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000072_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 73.0 in stage 3.0 (TID 76, localhost, executor driver, partition 73, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75) in 21 ms on localhost (executor driver) (73/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 73.0 in stage 3.0 (TID 76)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000073_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 74.0 in stage 3.0 (TID 77, localhost, executor driver, partition 74, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 74.0 in stage 3.0 (TID 77)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76) in 27 ms on localhost (executor driver) (74/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000074_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 75.0 in stage 3.0 (TID 78, localhost, executor driver, partition 75, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 75.0 in stage 3.0 (TID 78)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77) in 18 ms on localhost (executor driver) (75/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000075_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 76.0 in stage 3.0 (TID 79, localhost, executor driver, partition 76, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 76.0 in stage 3.0 (TID 79)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78) in 17 ms on localhost (executor driver) (76/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000076_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 77.0 in stage 3.0 (TID 80, localhost, executor driver, partition 77, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79) in 17 ms on localhost (executor driver) (77/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 77.0 in stage 3.0 (TID 80)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000077_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 78.0 in stage 3.0 (TID 81, localhost, executor driver, partition 78, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80) in 17 ms on localhost (executor driver) (78/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 78.0 in stage 3.0 (TID 81)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000078_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 79.0 in stage 3.0 (TID 82, localhost, executor driver, partition 79, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 79.0 in stage 3.0 (TID 82)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81) in 16 ms on localhost (executor driver) (79/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000079_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 80.0 in stage 3.0 (TID 83, localhost, executor driver, partition 80, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82) in 15 ms on localhost (executor driver) (80/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 80.0 in stage 3.0 (TID 83)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000080_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 81.0 in stage 3.0 (TID 84, localhost, executor driver, partition 81, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83) in 18 ms on localhost (executor driver) (81/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 81.0 in stage 3.0 (TID 84)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000081_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 82.0 in stage 3.0 (TID 85, localhost, executor driver, partition 82, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84) in 25 ms on localhost (executor driver) (82/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 82.0 in stage 3.0 (TID 85)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000082_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 83.0 in stage 3.0 (TID 86, localhost, executor driver, partition 83, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 83.0 in stage 3.0 (TID 86)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85) in 19 ms on localhost (executor driver) (83/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000083_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 84.0 in stage 3.0 (TID 87, localhost, executor driver, partition 84, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 84.0 in stage 3.0 (TID 87)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86) in 20 ms on localhost (executor driver) (84/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000084_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 85.0 in stage 3.0 (TID 88, localhost, executor driver, partition 85, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87) in 19 ms on localhost (executor driver) (85/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 85.0 in stage 3.0 (TID 88)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000085_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 86.0 in stage 3.0 (TID 89, localhost, executor driver, partition 86, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 86.0 in stage 3.0 (TID 89)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88) in 16 ms on localhost (executor driver) (86/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000086_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 87.0 in stage 3.0 (TID 90, localhost, executor driver, partition 87, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 87.0 in stage 3.0 (TID 90)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89) in 15 ms on localhost (executor driver) (87/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000087_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 88.0 in stage 3.0 (TID 91, localhost, executor driver, partition 88, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 88.0 in stage 3.0 (TID 91)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90) in 12 ms on localhost (executor driver) (88/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000088_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 89.0 in stage 3.0 (TID 92, localhost, executor driver, partition 89, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 89.0 in stage 3.0 (TID 92)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91) in 12 ms on localhost (executor driver) (89/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000089_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 90.0 in stage 3.0 (TID 93, localhost, executor driver, partition 90, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 90.0 in stage 3.0 (TID 93)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92) in 13 ms on localhost (executor driver) (90/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000090_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 91.0 in stage 3.0 (TID 94, localhost, executor driver, partition 91, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 91.0 in stage 3.0 (TID 94)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93) in 12 ms on localhost (executor driver) (91/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000091_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 92.0 in stage 3.0 (TID 95, localhost, executor driver, partition 92, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 92.0 in stage 3.0 (TID 95)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94) in 12 ms on localhost (executor driver) (92/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000092_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 93.0 in stage 3.0 (TID 96, localhost, executor driver, partition 93, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 93.0 in stage 3.0 (TID 96)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95) in 15 ms on localhost (executor driver) (93/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000093_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 94.0 in stage 3.0 (TID 97, localhost, executor driver, partition 94, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 94.0 in stage 3.0 (TID 97)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96) in 16 ms on localhost (executor driver) (94/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000094_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 95.0 in stage 3.0 (TID 98, localhost, executor driver, partition 95, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97) in 18 ms on localhost (executor driver) (95/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 95.0 in stage 3.0 (TID 98)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000095_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 96.0 in stage 3.0 (TID 99, localhost, executor driver, partition 96, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98) in 17 ms on localhost (executor driver) (96/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 96.0 in stage 3.0 (TID 99)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000096_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 97.0 in stage 3.0 (TID 100, localhost, executor driver, partition 97, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 97.0 in stage 3.0 (TID 100)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99) in 15 ms on localhost (executor driver) (97/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000097_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 98.0 in stage 3.0 (TID 101, localhost, executor driver, partition 98, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 98.0 in stage 3.0 (TID 101)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100) in 14 ms on localhost (executor driver) (98/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000098_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 99.0 in stage 3.0 (TID 102, localhost, executor driver, partition 99, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 99.0 in stage 3.0 (TID 102)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101) in 15 ms on localhost (executor driver) (99/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000099_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 100.0 in stage 3.0 (TID 103, localhost, executor driver, partition 100, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 100.0 in stage 3.0 (TID 103)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102) in 14 ms on localhost (executor driver) (100/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000100_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 101.0 in stage 3.0 (TID 104, localhost, executor driver, partition 101, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103) in 18 ms on localhost (executor driver) (101/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 101.0 in stage 3.0 (TID 104)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000101_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 102.0 in stage 3.0 (TID 105, localhost, executor driver, partition 102, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 102.0 in stage 3.0 (TID 105)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104) in 20 ms on localhost (executor driver) (102/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000102_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 103.0 in stage 3.0 (TID 106, localhost, executor driver, partition 103, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105) in 18 ms on localhost (executor driver) (103/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 103.0 in stage 3.0 (TID 106)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000103_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 104.0 in stage 3.0 (TID 107, localhost, executor driver, partition 104, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106) in 21 ms on localhost (executor driver) (104/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 104.0 in stage 3.0 (TID 107)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000104_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 105.0 in stage 3.0 (TID 108, localhost, executor driver, partition 105, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107) in 29 ms on localhost (executor driver) (105/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 105.0 in stage 3.0 (TID 108)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000105_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 106.0 in stage 3.0 (TID 109, localhost, executor driver, partition 106, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 106.0 in stage 3.0 (TID 109)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108) in 22 ms on localhost (executor driver) (106/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000106_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 107.0 in stage 3.0 (TID 110, localhost, executor driver, partition 107, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109) in 23 ms on localhost (executor driver) (107/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 107.0 in stage 3.0 (TID 110)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000107_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 108.0 in stage 3.0 (TID 111, localhost, executor driver, partition 108, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 108.0 in stage 3.0 (TID 111)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110) in 25 ms on localhost (executor driver) (108/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000108_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 109.0 in stage 3.0 (TID 112, localhost, executor driver, partition 109, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111) in 18 ms on localhost (executor driver) (109/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 109.0 in stage 3.0 (TID 112)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000109_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 110.0 in stage 3.0 (TID 113, localhost, executor driver, partition 110, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 110.0 in stage 3.0 (TID 113)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112) in 15 ms on localhost (executor driver) (110/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000110_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 111.0 in stage 3.0 (TID 114, localhost, executor driver, partition 111, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 111.0 in stage 3.0 (TID 114)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113) in 18 ms on localhost (executor driver) (111/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000111_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 112.0 in stage 3.0 (TID 115, localhost, executor driver, partition 112, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114) in 12 ms on localhost (executor driver) (112/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 112.0 in stage 3.0 (TID 115)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000112_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 113.0 in stage 3.0 (TID 116, localhost, executor driver, partition 113, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115) in 40 ms on localhost (executor driver) (113/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 113.0 in stage 3.0 (TID 116)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000113_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 114.0 in stage 3.0 (TID 117, localhost, executor driver, partition 114, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116) in 14 ms on localhost (executor driver) (114/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 114.0 in stage 3.0 (TID 117)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000114_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 115.0 in stage 3.0 (TID 118, localhost, executor driver, partition 115, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117) in 14 ms on localhost (executor driver) (115/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 115.0 in stage 3.0 (TID 118)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000115_0
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Starting task 116.0 in stage 3.0 (TID 119, localhost, executor driver, partition 116, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Running task 116.0 in stage 3.0 (TID 119)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118) in 17 ms on localhost (executor driver) (116/200)
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191211_0003_m_000116_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 117.0 in stage 3.0 (TID 120, localhost, executor driver, partition 117, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 117.0 in stage 3.0 (TID 120)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119) in 20 ms on localhost (executor driver) (117/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000117_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 118.0 in stage 3.0 (TID 121, localhost, executor driver, partition 118, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120) in 17 ms on localhost (executor driver) (118/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 118.0 in stage 3.0 (TID 121)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000118_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 119.0 in stage 3.0 (TID 122, localhost, executor driver, partition 119, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 119.0 in stage 3.0 (TID 122)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121) in 15 ms on localhost (executor driver) (119/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000119_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 120.0 in stage 3.0 (TID 123, localhost, executor driver, partition 120, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122) in 15 ms on localhost (executor driver) (120/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 120.0 in stage 3.0 (TID 123)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000120_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 121.0 in stage 3.0 (TID 124, localhost, executor driver, partition 121, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123) in 19 ms on localhost (executor driver) (121/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 121.0 in stage 3.0 (TID 124)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000121_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 122.0 in stage 3.0 (TID 125, localhost, executor driver, partition 122, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 122.0 in stage 3.0 (TID 125)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124) in 20 ms on localhost (executor driver) (122/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000122_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 123.0 in stage 3.0 (TID 126, localhost, executor driver, partition 123, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125) in 21 ms on localhost (executor driver) (123/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 123.0 in stage 3.0 (TID 126)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000123_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 124.0 in stage 3.0 (TID 127, localhost, executor driver, partition 124, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 124.0 in stage 3.0 (TID 127)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126) in 18 ms on localhost (executor driver) (124/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000124_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 125.0 in stage 3.0 (TID 128, localhost, executor driver, partition 125, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 125.0 in stage 3.0 (TID 128)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127) in 14 ms on localhost (executor driver) (125/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000125_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 126.0 in stage 3.0 (TID 129, localhost, executor driver, partition 126, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 126.0 in stage 3.0 (TID 129)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128) in 14 ms on localhost (executor driver) (126/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000126_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 127.0 in stage 3.0 (TID 130, localhost, executor driver, partition 127, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 127.0 in stage 3.0 (TID 130)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129) in 15 ms on localhost (executor driver) (127/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000127_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 128.0 in stage 3.0 (TID 131, localhost, executor driver, partition 128, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 128.0 in stage 3.0 (TID 131)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130) in 16 ms on localhost (executor driver) (128/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000128_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 129.0 in stage 3.0 (TID 132, localhost, executor driver, partition 129, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131) in 17 ms on localhost (executor driver) (129/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 129.0 in stage 3.0 (TID 132)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000129_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 130.0 in stage 3.0 (TID 133, localhost, executor driver, partition 130, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 130.0 in stage 3.0 (TID 133)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132) in 18 ms on localhost (executor driver) (130/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000130_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 131.0 in stage 3.0 (TID 134, localhost, executor driver, partition 131, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 131.0 in stage 3.0 (TID 134)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133) in 16 ms on localhost (executor driver) (131/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000131_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 132.0 in stage 3.0 (TID 135, localhost, executor driver, partition 132, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 132.0 in stage 3.0 (TID 135)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134) in 13 ms on localhost (executor driver) (132/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000132_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 133.0 in stage 3.0 (TID 136, localhost, executor driver, partition 133, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135) in 20 ms on localhost (executor driver) (133/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 133.0 in stage 3.0 (TID 136)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000133_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 134.0 in stage 3.0 (TID 137, localhost, executor driver, partition 134, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136) in 21 ms on localhost (executor driver) (134/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 134.0 in stage 3.0 (TID 137)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000134_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 135.0 in stage 3.0 (TID 138, localhost, executor driver, partition 135, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137) in 21 ms on localhost (executor driver) (135/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 135.0 in stage 3.0 (TID 138)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000135_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 136.0 in stage 3.0 (TID 139, localhost, executor driver, partition 136, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 136.0 in stage 3.0 (TID 139)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138) in 18 ms on localhost (executor driver) (136/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000136_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 137.0 in stage 3.0 (TID 140, localhost, executor driver, partition 137, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 137.0 in stage 3.0 (TID 140)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139) in 14 ms on localhost (executor driver) (137/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000137_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 138.0 in stage 3.0 (TID 141, localhost, executor driver, partition 138, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 138.0 in stage 3.0 (TID 141)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140) in 17 ms on localhost (executor driver) (138/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000138_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 139.0 in stage 3.0 (TID 142, localhost, executor driver, partition 139, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141) in 14 ms on localhost (executor driver) (139/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 139.0 in stage 3.0 (TID 142)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000139_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 140.0 in stage 3.0 (TID 143, localhost, executor driver, partition 140, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142) in 17 ms on localhost (executor driver) (140/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 140.0 in stage 3.0 (TID 143)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000140_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 141.0 in stage 3.0 (TID 144, localhost, executor driver, partition 141, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 141.0 in stage 3.0 (TID 144)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143) in 17 ms on localhost (executor driver) (141/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000141_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 142.0 in stage 3.0 (TID 145, localhost, executor driver, partition 142, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 142.0 in stage 3.0 (TID 145)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144) in 24 ms on localhost (executor driver) (142/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000142_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 143.0 in stage 3.0 (TID 146, localhost, executor driver, partition 143, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145) in 19 ms on localhost (executor driver) (143/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 143.0 in stage 3.0 (TID 146)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000143_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 144.0 in stage 3.0 (TID 147, localhost, executor driver, partition 144, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 144.0 in stage 3.0 (TID 147)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146) in 25 ms on localhost (executor driver) (144/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000144_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 145.0 in stage 3.0 (TID 148, localhost, executor driver, partition 145, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147) in 20 ms on localhost (executor driver) (145/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 145.0 in stage 3.0 (TID 148)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000145_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 146.0 in stage 3.0 (TID 149, localhost, executor driver, partition 146, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 146.0 in stage 3.0 (TID 149)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148) in 16 ms on localhost (executor driver) (146/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000146_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 147.0 in stage 3.0 (TID 150, localhost, executor driver, partition 147, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 147.0 in stage 3.0 (TID 150)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149) in 23 ms on localhost (executor driver) (147/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000147_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 148.0 in stage 3.0 (TID 151, localhost, executor driver, partition 148, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150) in 19 ms on localhost (executor driver) (148/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 148.0 in stage 3.0 (TID 151)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000148_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 149.0 in stage 3.0 (TID 152, localhost, executor driver, partition 149, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 149.0 in stage 3.0 (TID 152)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151) in 14 ms on localhost (executor driver) (149/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000149_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 150.0 in stage 3.0 (TID 153, localhost, executor driver, partition 150, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152) in 13 ms on localhost (executor driver) (150/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 150.0 in stage 3.0 (TID 153)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000150_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 151.0 in stage 3.0 (TID 154, localhost, executor driver, partition 151, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153) in 14 ms on localhost (executor driver) (151/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 151.0 in stage 3.0 (TID 154)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000151_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 152.0 in stage 3.0 (TID 155, localhost, executor driver, partition 152, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 152.0 in stage 3.0 (TID 155)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154) in 15 ms on localhost (executor driver) (152/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000152_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 153.0 in stage 3.0 (TID 156, localhost, executor driver, partition 153, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155) in 12 ms on localhost (executor driver) (153/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 153.0 in stage 3.0 (TID 156)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000153_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 154.0 in stage 3.0 (TID 157, localhost, executor driver, partition 154, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 154.0 in stage 3.0 (TID 157)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156) in 17 ms on localhost (executor driver) (154/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000154_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 155.0 in stage 3.0 (TID 158, localhost, executor driver, partition 155, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157) in 19 ms on localhost (executor driver) (155/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 155.0 in stage 3.0 (TID 158)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000155_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 156.0 in stage 3.0 (TID 159, localhost, executor driver, partition 156, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158) in 21 ms on localhost (executor driver) (156/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 156.0 in stage 3.0 (TID 159)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 4 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000156_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 157.0 in stage 3.0 (TID 160, localhost, executor driver, partition 157, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159) in 30 ms on localhost (executor driver) (157/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 157.0 in stage 3.0 (TID 160)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000157_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 158.0 in stage 3.0 (TID 161, localhost, executor driver, partition 158, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 158.0 in stage 3.0 (TID 161)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160) in 18 ms on localhost (executor driver) (158/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000158_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 159.0 in stage 3.0 (TID 162, localhost, executor driver, partition 159, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 159.0 in stage 3.0 (TID 162)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161) in 18 ms on localhost (executor driver) (159/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000159_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 160.0 in stage 3.0 (TID 163, localhost, executor driver, partition 160, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 160.0 in stage 3.0 (TID 163)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162) in 16 ms on localhost (executor driver) (160/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000160_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 161.0 in stage 3.0 (TID 164, localhost, executor driver, partition 161, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 161.0 in stage 3.0 (TID 164)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163) in 15 ms on localhost (executor driver) (161/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000161_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 162.0 in stage 3.0 (TID 165, localhost, executor driver, partition 162, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 162.0 in stage 3.0 (TID 165)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164) in 14 ms on localhost (executor driver) (162/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000162_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 163.0 in stage 3.0 (TID 166, localhost, executor driver, partition 163, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 163.0 in stage 3.0 (TID 166)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165) in 13 ms on localhost (executor driver) (163/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000163_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 164.0 in stage 3.0 (TID 167, localhost, executor driver, partition 164, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 164.0 in stage 3.0 (TID 167)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166) in 12 ms on localhost (executor driver) (164/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000164_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 165.0 in stage 3.0 (TID 168, localhost, executor driver, partition 165, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167) in 13 ms on localhost (executor driver) (165/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 165.0 in stage 3.0 (TID 168)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000165_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 166.0 in stage 3.0 (TID 169, localhost, executor driver, partition 166, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 166.0 in stage 3.0 (TID 169)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168) in 15 ms on localhost (executor driver) (166/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000166_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 167.0 in stage 3.0 (TID 170, localhost, executor driver, partition 167, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169) in 15 ms on localhost (executor driver) (167/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 167.0 in stage 3.0 (TID 170)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000167_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 168.0 in stage 3.0 (TID 171, localhost, executor driver, partition 168, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 168.0 in stage 3.0 (TID 171)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170) in 15 ms on localhost (executor driver) (168/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000168_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 169.0 in stage 3.0 (TID 172, localhost, executor driver, partition 169, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171) in 17 ms on localhost (executor driver) (169/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 169.0 in stage 3.0 (TID 172)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000169_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 170.0 in stage 3.0 (TID 173, localhost, executor driver, partition 170, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 170.0 in stage 3.0 (TID 173)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172) in 17 ms on localhost (executor driver) (170/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000170_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 171.0 in stage 3.0 (TID 174, localhost, executor driver, partition 171, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173) in 15 ms on localhost (executor driver) (171/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 171.0 in stage 3.0 (TID 174)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000171_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 172.0 in stage 3.0 (TID 175, localhost, executor driver, partition 172, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174) in 19 ms on localhost (executor driver) (172/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 172.0 in stage 3.0 (TID 175)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000172_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175). 3732 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 173.0 in stage 3.0 (TID 176, localhost, executor driver, partition 173, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 173.0 in stage 3.0 (TID 176)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175) in 20 ms on localhost (executor driver) (173/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000173_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 174.0 in stage 3.0 (TID 177, localhost, executor driver, partition 174, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176) in 18 ms on localhost (executor driver) (174/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 174.0 in stage 3.0 (TID 177)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000174_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 175.0 in stage 3.0 (TID 178, localhost, executor driver, partition 175, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177) in 22 ms on localhost (executor driver) (175/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 175.0 in stage 3.0 (TID 178)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000175_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 176.0 in stage 3.0 (TID 179, localhost, executor driver, partition 176, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178) in 16 ms on localhost (executor driver) (176/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 176.0 in stage 3.0 (TID 179)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191212_0003_m_000176_0
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Starting task 177.0 in stage 3.0 (TID 180, localhost, executor driver, partition 177, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179) in 18 ms on localhost (executor driver) (177/200)
[INFO ] [2018-04-26 19:12:12] [Logging$class:logInfo:54] Running task 177.0 in stage 3.0 (TID 180)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000177_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 178.0 in stage 3.0 (TID 181, localhost, executor driver, partition 178, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 178.0 in stage 3.0 (TID 181)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180) in 27 ms on localhost (executor driver) (178/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000178_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 179.0 in stage 3.0 (TID 182, localhost, executor driver, partition 179, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181) in 33 ms on localhost (executor driver) (179/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 179.0 in stage 3.0 (TID 182)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000179_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 180.0 in stage 3.0 (TID 183, localhost, executor driver, partition 180, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182) in 53 ms on localhost (executor driver) (180/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 180.0 in stage 3.0 (TID 183)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000180_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 181.0 in stage 3.0 (TID 184, localhost, executor driver, partition 181, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 181.0 in stage 3.0 (TID 184)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183) in 22 ms on localhost (executor driver) (181/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000181_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 182.0 in stage 3.0 (TID 185, localhost, executor driver, partition 182, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184) in 49 ms on localhost (executor driver) (182/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 182.0 in stage 3.0 (TID 185)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000182_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 183.0 in stage 3.0 (TID 186, localhost, executor driver, partition 183, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 183.0 in stage 3.0 (TID 186)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185) in 15 ms on localhost (executor driver) (183/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000183_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 184.0 in stage 3.0 (TID 187, localhost, executor driver, partition 184, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186) in 13 ms on localhost (executor driver) (184/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 184.0 in stage 3.0 (TID 187)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000184_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 185.0 in stage 3.0 (TID 188, localhost, executor driver, partition 185, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 185.0 in stage 3.0 (TID 188)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187) in 12 ms on localhost (executor driver) (185/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000185_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 186.0 in stage 3.0 (TID 189, localhost, executor driver, partition 186, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 186.0 in stage 3.0 (TID 189)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188) in 12 ms on localhost (executor driver) (186/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000186_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 187.0 in stage 3.0 (TID 190, localhost, executor driver, partition 187, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 187.0 in stage 3.0 (TID 190)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189) in 13 ms on localhost (executor driver) (187/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000187_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 188.0 in stage 3.0 (TID 191, localhost, executor driver, partition 188, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190) in 16 ms on localhost (executor driver) (188/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 188.0 in stage 3.0 (TID 191)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000188_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 189.0 in stage 3.0 (TID 192, localhost, executor driver, partition 189, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 189.0 in stage 3.0 (TID 192)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191) in 16 ms on localhost (executor driver) (189/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000189_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 190.0 in stage 3.0 (TID 193, localhost, executor driver, partition 190, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 190.0 in stage 3.0 (TID 193)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192) in 12 ms on localhost (executor driver) (190/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000190_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 191.0 in stage 3.0 (TID 194, localhost, executor driver, partition 191, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 191.0 in stage 3.0 (TID 194)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193) in 13 ms on localhost (executor driver) (191/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000191_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 192.0 in stage 3.0 (TID 195, localhost, executor driver, partition 192, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 192.0 in stage 3.0 (TID 195)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194) in 18 ms on localhost (executor driver) (192/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000192_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 193.0 in stage 3.0 (TID 196, localhost, executor driver, partition 193, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 193.0 in stage 3.0 (TID 196)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195) in 24 ms on localhost (executor driver) (193/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000193_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 194.0 in stage 3.0 (TID 197, localhost, executor driver, partition 194, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196) in 23 ms on localhost (executor driver) (194/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 194.0 in stage 3.0 (TID 197)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000194_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 195.0 in stage 3.0 (TID 198, localhost, executor driver, partition 195, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 195.0 in stage 3.0 (TID 198)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197) in 27 ms on localhost (executor driver) (195/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000195_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 196.0 in stage 3.0 (TID 199, localhost, executor driver, partition 196, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 196.0 in stage 3.0 (TID 199)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198) in 16 ms on localhost (executor driver) (196/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000196_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 197.0 in stage 3.0 (TID 200, localhost, executor driver, partition 197, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 197.0 in stage 3.0 (TID 200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199) in 16 ms on localhost (executor driver) (197/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000197_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 198.0 in stage 3.0 (TID 201, localhost, executor driver, partition 198, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 198.0 in stage 3.0 (TID 201)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200) in 12 ms on localhost (executor driver) (198/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000198_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Starting task 199.0 in stage 3.0 (TID 202, localhost, executor driver, partition 199, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Running task 199.0 in stage 3.0 (TID 202)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201) in 15 ms on localhost (executor driver) (199/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191213_0003_m_000199_0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202) in 13 ms on localhost (executor driver) (200/200)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] ResultStage 3 (saveAsTable at HistoryDataToA.scala:101) finished in 4.858 s
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:101, took 5.994649 s
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 13
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 5
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 16
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 24
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Removed broadcast_8_piece0 on 192.168.0.152:63060 in memory (size: 42.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Removed broadcast_5_piece0 on 192.168.0.152:63060 in memory (size: 204.8 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned shuffle 0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 0
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 4
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 7
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 6
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 11
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 19
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned shuffle 1
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 9
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 22
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 15
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 23
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 18
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 8
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:63060 in memory (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 14
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 21
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 12
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 1
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 10
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 17
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 20
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 2
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Cleaned accumulator 3
[INFO ] [2018-04-26 19:12:13] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`original_sale_detail` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/original_sale_detail
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/original_sale_detail
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:12:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Final stage: ResultStage 4 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Submitting ResultStage 4 (MapPartitionsRDD[44] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Block broadcast_9 stored as values in memory (estimated size 80.9 KB, free 1991.2 MB)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.2 MB)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Added broadcast_9_piece0 in memory on 192.168.0.152:63060 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 4 (MapPartitionsRDD[44] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Adding task set 4.0 with 15 tasks
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Starting task 0.0 in stage 4.0 (TID 203, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Running task 0.0 in stage 4.0 (TID 203)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Code generated in 11.287035 ms
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Code generated in 6.380367 ms
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Code generated in 11.445996 ms
[ERROR] [2018-04-26 19:12:15] [Logging$class:logError:91] Exception in task 0.0 in stage 4.0 (TID 203)
java.lang.NullPointerException
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:203)
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:202)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:190)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Starting task 1.0 in stage 4.0 (TID 204, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Running task 1.0 in stage 4.0 (TID 204)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[WARN ] [2018-04-26 19:12:15] [Logging$class:logWarning:66] Lost task 0.0 in stage 4.0 (TID 203, localhost, executor driver): java.lang.NullPointerException
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:203)
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:202)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:190)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-26 19:12:15] [Logging$class:logError:70] Task 0 in stage 4.0 failed 1 times; aborting job
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Cancelling stage 4
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Stage 4 was cancelled
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] ResultStage 4 (saveAsTable at HistoryDataToA.scala:114) failed in 0.157 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 203, localhost, executor driver): java.lang.NullPointerException
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:203)
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:202)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:190)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Executor is trying to kill task 1.0 in stage 4.0 (TID 204), reason: stage cancelled
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Job 2 failed: saveAsTable at HistoryDataToA.scala:114, took 0.193493 s
[ERROR] [2018-04-26 19:12:15] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 203, localhost, executor driver): java.lang.NullPointerException
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:203)
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:202)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:190)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:394)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:114)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.NullPointerException
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:203)
	at com.dr.banner.posProductProcessor$$anonfun$13.apply(posProductProcessor.scala:202)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:190)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Executor interrupted and killed task 1.0 in stage 4.0 (TID 204), reason: stage cancelled
[WARN ] [2018-04-26 19:12:15] [Logging$class:logWarning:66] Lost task 1.0 in stage 4.0 (TID 204, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 19:12:15] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-b41b88e9-3e77-4872-bf96-6679ae71adf3
[INFO ] [2018-04-26 19:17:26] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-26 19:17:27] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-26 19:17:27] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-26 19:17:27] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-26 19:17:27] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-26 19:17:27] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-26 19:17:27] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 63174.
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-86c7b2cb-b59f-4851-9bef-0491202c30d8
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63183.
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Server created on 192.168.0.152:63183
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 63183, None)
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:63183 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 63183, None)
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 63183, None)
[INFO ] [2018-04-26 19:17:28] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 63183, None)
[INFO ] [2018-04-26 19:17:30] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-26 19:17:30] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-26 19:17:30] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:63183 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 19:17:30] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:63183 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:63183 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:17:31] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-26 19:17:32] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-26 19:17:32] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
[INFO ] [2018-04-26 19:17:32] [Logging$class:logInfo:54] Warehouse path is '/user/hive/warehouse'.
[INFO ] [2018-04-26 19:17:33] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-26 19:17:37] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 19:17:37] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
[INFO ] [2018-04-26 19:17:37] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-26 19:17:37] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:39] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:43] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-26 19:17:44] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:44] [Logging$class:logInfo:54] Code generated in 209.833573 ms
[INFO ] [2018-04-26 19:17:44] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 226.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:17:44] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:17:44] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:63183 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:17:44] [Logging$class:logInfo:54] Created broadcast 3 from 
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 11.9 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1991.1 MB)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:63183 (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Code generated in 10.892084 ms
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 73022 bytes result sent to driver
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 485 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.504 s
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.577724 s
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Code generated in 13.314646 ms
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Removed broadcast_4_piece0 on 192.168.0.152:63183 in memory (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 64.5 MB, free 1926.6 MB)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 204.8 KB, free 1926.4 MB)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:63183 (size: 204.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Created broadcast 5 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Code generated in 31.473658 ms
[INFO ] [2018-04-26 19:17:45] [Logging$class:logInfo:54] Code generated in 21.270659 ms
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:101
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Registering RDD 29 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Registering RDD 25 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:101) with 200 output partitions
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 12.3 KB, free 1926.4 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.4 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:63183 (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Block broadcast_7 stored as values in memory (estimated size 17.3 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1926.3 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Added broadcast_7_piece0 in memory on 192.168.0.152:63183 (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Code generated in 11.832262 ms
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Code generated in 21.581031 ms
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1410 bytes result sent to driver
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 294 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:101) finished in 0.297 s
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Code generated in 19.560971 ms
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Code generated in 13.169277 ms
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Removed broadcast_6_piece0 on 192.168.0.152:63183 in memory (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2). 1410 bytes result sent to driver
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2) in 350 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:101) finished in 0.595 s
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] running: Set()
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Submitting ResultStage 3 (MapPartitionsRDD[34] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Block broadcast_8 stored as values in memory (estimated size 114.6 KB, free 1926.2 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Block broadcast_8_piece0 stored as bytes in memory (estimated size 43.2 KB, free 1926.2 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Added broadcast_8_piece0 in memory on 192.168.0.152:63183 (size: 43.2 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[34] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Adding task set 3.0 with 200 tasks
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:46] [Logging$class:logInfo:54] Running task 0.0 in stage 3.0 (TID 3)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 6 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 11.843212 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 15.073797 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 7.307329 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 13.95389 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 11.577772 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 7.350373 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 7.077382 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 7.773264 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 6.279553 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 21.74037 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Code generated in 19.173573 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000000_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3). 3732 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 1.0 in stage 3.0 (TID 4)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000001_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 2.0 in stage 3.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4) in 66 ms on localhost (executor driver) (1/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3) in 375 ms on localhost (executor driver) (2/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 2.0 in stage 3.0 (TID 5)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000002_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5). 3732 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 3.0 in stage 3.0 (TID 6, localhost, executor driver, partition 3, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5) in 65 ms on localhost (executor driver) (3/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 3.0 in stage 3.0 (TID 6)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000003_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 4.0 in stage 3.0 (TID 7, localhost, executor driver, partition 4, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6) in 70 ms on localhost (executor driver) (4/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 4.0 in stage 3.0 (TID 7)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000004_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 5.0 in stage 3.0 (TID 8, localhost, executor driver, partition 5, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7) in 218 ms on localhost (executor driver) (5/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 5.0 in stage 3.0 (TID 8)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000005_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 6.0 in stage 3.0 (TID 9, localhost, executor driver, partition 6, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8) in 59 ms on localhost (executor driver) (6/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 6.0 in stage 3.0 (TID 9)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000006_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 7.0 in stage 3.0 (TID 10, localhost, executor driver, partition 7, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9) in 52 ms on localhost (executor driver) (7/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 7.0 in stage 3.0 (TID 10)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000007_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 8.0 in stage 3.0 (TID 11, localhost, executor driver, partition 8, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10) in 38 ms on localhost (executor driver) (8/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 8.0 in stage 3.0 (TID 11)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000008_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 9.0 in stage 3.0 (TID 12, localhost, executor driver, partition 9, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 9.0 in stage 3.0 (TID 12)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11) in 38 ms on localhost (executor driver) (9/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000009_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 10.0 in stage 3.0 (TID 13, localhost, executor driver, partition 10, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 10.0 in stage 3.0 (TID 13)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12) in 30 ms on localhost (executor driver) (10/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000010_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 11.0 in stage 3.0 (TID 14, localhost, executor driver, partition 11, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13) in 48 ms on localhost (executor driver) (11/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 11.0 in stage 3.0 (TID 14)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000011_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 12.0 in stage 3.0 (TID 15, localhost, executor driver, partition 12, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14) in 60 ms on localhost (executor driver) (12/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 12.0 in stage 3.0 (TID 15)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191747_0003_m_000012_0
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Starting task 13.0 in stage 3.0 (TID 16, localhost, executor driver, partition 13, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15) in 39 ms on localhost (executor driver) (13/200)
[INFO ] [2018-04-26 19:17:47] [Logging$class:logInfo:54] Running task 13.0 in stage 3.0 (TID 16)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000013_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 14.0 in stage 3.0 (TID 17, localhost, executor driver, partition 14, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 14.0 in stage 3.0 (TID 17)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16) in 54 ms on localhost (executor driver) (14/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000014_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 15.0 in stage 3.0 (TID 18, localhost, executor driver, partition 15, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17) in 36 ms on localhost (executor driver) (15/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 15.0 in stage 3.0 (TID 18)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000015_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 16.0 in stage 3.0 (TID 19, localhost, executor driver, partition 16, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 16.0 in stage 3.0 (TID 19)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18) in 33 ms on localhost (executor driver) (16/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000016_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 17.0 in stage 3.0 (TID 20, localhost, executor driver, partition 17, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19) in 25 ms on localhost (executor driver) (17/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 17.0 in stage 3.0 (TID 20)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000017_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 18.0 in stage 3.0 (TID 21, localhost, executor driver, partition 18, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20) in 31 ms on localhost (executor driver) (18/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 18.0 in stage 3.0 (TID 21)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000018_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 19.0 in stage 3.0 (TID 22, localhost, executor driver, partition 19, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 19.0 in stage 3.0 (TID 22)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21) in 22 ms on localhost (executor driver) (19/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000019_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 20.0 in stage 3.0 (TID 23, localhost, executor driver, partition 20, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22) in 24 ms on localhost (executor driver) (20/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 20.0 in stage 3.0 (TID 23)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000020_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 21.0 in stage 3.0 (TID 24, localhost, executor driver, partition 21, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 21.0 in stage 3.0 (TID 24)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23) in 27 ms on localhost (executor driver) (21/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000021_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 22.0 in stage 3.0 (TID 25, localhost, executor driver, partition 22, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24) in 28 ms on localhost (executor driver) (22/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 22.0 in stage 3.0 (TID 25)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000022_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 23.0 in stage 3.0 (TID 26, localhost, executor driver, partition 23, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25) in 30 ms on localhost (executor driver) (23/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 23.0 in stage 3.0 (TID 26)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000023_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 24.0 in stage 3.0 (TID 27, localhost, executor driver, partition 24, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 24.0 in stage 3.0 (TID 27)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26) in 45 ms on localhost (executor driver) (24/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000024_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 25.0 in stage 3.0 (TID 28, localhost, executor driver, partition 25, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 25.0 in stage 3.0 (TID 28)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27) in 25 ms on localhost (executor driver) (25/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000025_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 26.0 in stage 3.0 (TID 29, localhost, executor driver, partition 26, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28) in 31 ms on localhost (executor driver) (26/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 26.0 in stage 3.0 (TID 29)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000026_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 27.0 in stage 3.0 (TID 30, localhost, executor driver, partition 27, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29) in 33 ms on localhost (executor driver) (27/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 27.0 in stage 3.0 (TID 30)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000027_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 28.0 in stage 3.0 (TID 31, localhost, executor driver, partition 28, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 28.0 in stage 3.0 (TID 31)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30) in 27 ms on localhost (executor driver) (28/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000028_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 29.0 in stage 3.0 (TID 32, localhost, executor driver, partition 29, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31) in 22 ms on localhost (executor driver) (29/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 29.0 in stage 3.0 (TID 32)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000029_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 30.0 in stage 3.0 (TID 33, localhost, executor driver, partition 30, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32) in 25 ms on localhost (executor driver) (30/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 30.0 in stage 3.0 (TID 33)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000030_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 31.0 in stage 3.0 (TID 34, localhost, executor driver, partition 31, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 31.0 in stage 3.0 (TID 34)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33) in 22 ms on localhost (executor driver) (31/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000031_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 32.0 in stage 3.0 (TID 35, localhost, executor driver, partition 32, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 32.0 in stage 3.0 (TID 35)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34) in 22 ms on localhost (executor driver) (32/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000032_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 33.0 in stage 3.0 (TID 36, localhost, executor driver, partition 33, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 33.0 in stage 3.0 (TID 36)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35) in 42 ms on localhost (executor driver) (33/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Removed broadcast_7_piece0 on 192.168.0.152:63183 in memory (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000033_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36). 3732 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 34.0 in stage 3.0 (TID 37, localhost, executor driver, partition 34, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36) in 66 ms on localhost (executor driver) (34/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 34.0 in stage 3.0 (TID 37)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000034_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 35.0 in stage 3.0 (TID 38, localhost, executor driver, partition 35, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37) in 25 ms on localhost (executor driver) (35/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 35.0 in stage 3.0 (TID 38)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000035_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 36.0 in stage 3.0 (TID 39, localhost, executor driver, partition 36, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38) in 22 ms on localhost (executor driver) (36/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 36.0 in stage 3.0 (TID 39)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000036_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 37.0 in stage 3.0 (TID 40, localhost, executor driver, partition 37, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39) in 24 ms on localhost (executor driver) (37/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 37.0 in stage 3.0 (TID 40)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000037_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 38.0 in stage 3.0 (TID 41, localhost, executor driver, partition 38, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40) in 30 ms on localhost (executor driver) (38/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 38.0 in stage 3.0 (TID 41)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000038_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 39.0 in stage 3.0 (TID 42, localhost, executor driver, partition 39, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 39.0 in stage 3.0 (TID 42)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41) in 24 ms on localhost (executor driver) (39/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000039_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 40.0 in stage 3.0 (TID 43, localhost, executor driver, partition 40, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42) in 21 ms on localhost (executor driver) (40/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 40.0 in stage 3.0 (TID 43)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000040_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 41.0 in stage 3.0 (TID 44, localhost, executor driver, partition 41, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 41.0 in stage 3.0 (TID 44)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43) in 20 ms on localhost (executor driver) (41/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000041_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 42.0 in stage 3.0 (TID 45, localhost, executor driver, partition 42, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 42.0 in stage 3.0 (TID 45)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44) in 19 ms on localhost (executor driver) (42/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000042_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 43.0 in stage 3.0 (TID 46, localhost, executor driver, partition 43, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 43.0 in stage 3.0 (TID 46)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45) in 18 ms on localhost (executor driver) (43/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000043_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 44.0 in stage 3.0 (TID 47, localhost, executor driver, partition 44, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 44.0 in stage 3.0 (TID 47)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46) in 18 ms on localhost (executor driver) (44/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000044_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 45.0 in stage 3.0 (TID 48, localhost, executor driver, partition 45, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 45.0 in stage 3.0 (TID 48)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47) in 22 ms on localhost (executor driver) (45/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000045_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 46.0 in stage 3.0 (TID 49, localhost, executor driver, partition 46, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 46.0 in stage 3.0 (TID 49)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48) in 15 ms on localhost (executor driver) (46/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000046_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 47.0 in stage 3.0 (TID 50, localhost, executor driver, partition 47, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49) in 13 ms on localhost (executor driver) (47/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 47.0 in stage 3.0 (TID 50)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000047_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 48.0 in stage 3.0 (TID 51, localhost, executor driver, partition 48, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50) in 19 ms on localhost (executor driver) (48/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 48.0 in stage 3.0 (TID 51)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000048_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 49.0 in stage 3.0 (TID 52, localhost, executor driver, partition 49, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51) in 23 ms on localhost (executor driver) (49/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 49.0 in stage 3.0 (TID 52)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000049_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 50.0 in stage 3.0 (TID 53, localhost, executor driver, partition 50, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 50.0 in stage 3.0 (TID 53)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52) in 18 ms on localhost (executor driver) (50/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000050_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 51.0 in stage 3.0 (TID 54, localhost, executor driver, partition 51, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 51.0 in stage 3.0 (TID 54)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53) in 17 ms on localhost (executor driver) (51/200)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191748_0003_m_000051_0
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Starting task 52.0 in stage 3.0 (TID 55, localhost, executor driver, partition 52, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Running task 52.0 in stage 3.0 (TID 55)
[INFO ] [2018-04-26 19:17:48] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54) in 18 ms on localhost (executor driver) (52/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000052_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 53.0 in stage 3.0 (TID 56, localhost, executor driver, partition 53, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 53.0 in stage 3.0 (TID 56)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55) in 29 ms on localhost (executor driver) (53/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000053_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 54.0 in stage 3.0 (TID 57, localhost, executor driver, partition 54, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56) in 27 ms on localhost (executor driver) (54/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 54.0 in stage 3.0 (TID 57)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000054_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 55.0 in stage 3.0 (TID 58, localhost, executor driver, partition 55, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57) in 29 ms on localhost (executor driver) (55/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 55.0 in stage 3.0 (TID 58)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000055_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 56.0 in stage 3.0 (TID 59, localhost, executor driver, partition 56, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58) in 23 ms on localhost (executor driver) (56/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 56.0 in stage 3.0 (TID 59)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000056_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 57.0 in stage 3.0 (TID 60, localhost, executor driver, partition 57, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 57.0 in stage 3.0 (TID 60)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59) in 21 ms on localhost (executor driver) (57/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000057_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 58.0 in stage 3.0 (TID 61, localhost, executor driver, partition 58, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 58.0 in stage 3.0 (TID 61)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60) in 17 ms on localhost (executor driver) (58/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000058_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 59.0 in stage 3.0 (TID 62, localhost, executor driver, partition 59, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61) in 21 ms on localhost (executor driver) (59/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 59.0 in stage 3.0 (TID 62)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000059_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 60.0 in stage 3.0 (TID 63, localhost, executor driver, partition 60, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 60.0 in stage 3.0 (TID 63)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62) in 19 ms on localhost (executor driver) (60/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000060_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 61.0 in stage 3.0 (TID 64, localhost, executor driver, partition 61, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63) in 22 ms on localhost (executor driver) (61/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 61.0 in stage 3.0 (TID 64)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000061_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 62.0 in stage 3.0 (TID 65, localhost, executor driver, partition 62, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64) in 20 ms on localhost (executor driver) (62/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 62.0 in stage 3.0 (TID 65)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000062_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 63.0 in stage 3.0 (TID 66, localhost, executor driver, partition 63, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 63.0 in stage 3.0 (TID 66)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65) in 35 ms on localhost (executor driver) (63/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000063_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66). 3732 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 64.0 in stage 3.0 (TID 67, localhost, executor driver, partition 64, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66) in 246 ms on localhost (executor driver) (64/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 64.0 in stage 3.0 (TID 67)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000064_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 65.0 in stage 3.0 (TID 68, localhost, executor driver, partition 65, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 65.0 in stage 3.0 (TID 68)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67) in 41 ms on localhost (executor driver) (65/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000065_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 66.0 in stage 3.0 (TID 69, localhost, executor driver, partition 66, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 66.0 in stage 3.0 (TID 69)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68) in 16 ms on localhost (executor driver) (66/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000066_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 67.0 in stage 3.0 (TID 70, localhost, executor driver, partition 67, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 67.0 in stage 3.0 (TID 70)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69) in 14 ms on localhost (executor driver) (67/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000067_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 68.0 in stage 3.0 (TID 71, localhost, executor driver, partition 68, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 68.0 in stage 3.0 (TID 71)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70) in 15 ms on localhost (executor driver) (68/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000068_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 69.0 in stage 3.0 (TID 72, localhost, executor driver, partition 69, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 69.0 in stage 3.0 (TID 72)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71) in 13 ms on localhost (executor driver) (69/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000069_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 70.0 in stage 3.0 (TID 73, localhost, executor driver, partition 70, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 70.0 in stage 3.0 (TID 73)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72) in 15 ms on localhost (executor driver) (70/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000070_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 71.0 in stage 3.0 (TID 74, localhost, executor driver, partition 71, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 71.0 in stage 3.0 (TID 74)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73) in 18 ms on localhost (executor driver) (71/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000071_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 72.0 in stage 3.0 (TID 75, localhost, executor driver, partition 72, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 72.0 in stage 3.0 (TID 75)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74) in 13 ms on localhost (executor driver) (72/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000072_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 73.0 in stage 3.0 (TID 76, localhost, executor driver, partition 73, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 73.0 in stage 3.0 (TID 76)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75) in 12 ms on localhost (executor driver) (73/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000073_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 74.0 in stage 3.0 (TID 77, localhost, executor driver, partition 74, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 74.0 in stage 3.0 (TID 77)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76) in 15 ms on localhost (executor driver) (74/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000074_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 75.0 in stage 3.0 (TID 78, localhost, executor driver, partition 75, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77) in 17 ms on localhost (executor driver) (75/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 75.0 in stage 3.0 (TID 78)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000075_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 76.0 in stage 3.0 (TID 79, localhost, executor driver, partition 76, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 76.0 in stage 3.0 (TID 79)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78) in 18 ms on localhost (executor driver) (76/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000076_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 77.0 in stage 3.0 (TID 80, localhost, executor driver, partition 77, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 77.0 in stage 3.0 (TID 80)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79) in 13 ms on localhost (executor driver) (77/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000077_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 78.0 in stage 3.0 (TID 81, localhost, executor driver, partition 78, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 78.0 in stage 3.0 (TID 81)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80) in 15 ms on localhost (executor driver) (78/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000078_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 79.0 in stage 3.0 (TID 82, localhost, executor driver, partition 79, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 79.0 in stage 3.0 (TID 82)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81) in 16 ms on localhost (executor driver) (79/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000079_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 80.0 in stage 3.0 (TID 83, localhost, executor driver, partition 80, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82) in 18 ms on localhost (executor driver) (80/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 80.0 in stage 3.0 (TID 83)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000080_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 81.0 in stage 3.0 (TID 84, localhost, executor driver, partition 81, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 81.0 in stage 3.0 (TID 84)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83) in 14 ms on localhost (executor driver) (81/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000081_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 82.0 in stage 3.0 (TID 85, localhost, executor driver, partition 82, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 82.0 in stage 3.0 (TID 85)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84) in 14 ms on localhost (executor driver) (82/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000082_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 83.0 in stage 3.0 (TID 86, localhost, executor driver, partition 83, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 83.0 in stage 3.0 (TID 86)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85) in 16 ms on localhost (executor driver) (83/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000083_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 84.0 in stage 3.0 (TID 87, localhost, executor driver, partition 84, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 84.0 in stage 3.0 (TID 87)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86) in 16 ms on localhost (executor driver) (84/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000084_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 85.0 in stage 3.0 (TID 88, localhost, executor driver, partition 85, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 85.0 in stage 3.0 (TID 88)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87) in 25 ms on localhost (executor driver) (85/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000085_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 86.0 in stage 3.0 (TID 89, localhost, executor driver, partition 86, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88) in 20 ms on localhost (executor driver) (86/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 86.0 in stage 3.0 (TID 89)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000086_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 87.0 in stage 3.0 (TID 90, localhost, executor driver, partition 87, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 87.0 in stage 3.0 (TID 90)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89) in 16 ms on localhost (executor driver) (87/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000087_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 88.0 in stage 3.0 (TID 91, localhost, executor driver, partition 88, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 88.0 in stage 3.0 (TID 91)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90) in 11 ms on localhost (executor driver) (88/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000088_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 89.0 in stage 3.0 (TID 92, localhost, executor driver, partition 89, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 89.0 in stage 3.0 (TID 92)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91) in 14 ms on localhost (executor driver) (89/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000089_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 90.0 in stage 3.0 (TID 93, localhost, executor driver, partition 90, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 90.0 in stage 3.0 (TID 93)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92) in 12 ms on localhost (executor driver) (90/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000090_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 91.0 in stage 3.0 (TID 94, localhost, executor driver, partition 91, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 91.0 in stage 3.0 (TID 94)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93) in 12 ms on localhost (executor driver) (91/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000091_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 92.0 in stage 3.0 (TID 95, localhost, executor driver, partition 92, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 92.0 in stage 3.0 (TID 95)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94) in 10 ms on localhost (executor driver) (92/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000092_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 93.0 in stage 3.0 (TID 96, localhost, executor driver, partition 93, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 93.0 in stage 3.0 (TID 96)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95) in 12 ms on localhost (executor driver) (93/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000093_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 94.0 in stage 3.0 (TID 97, localhost, executor driver, partition 94, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 94.0 in stage 3.0 (TID 97)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96) in 14 ms on localhost (executor driver) (94/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000094_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 95.0 in stage 3.0 (TID 98, localhost, executor driver, partition 95, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 95.0 in stage 3.0 (TID 98)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97) in 14 ms on localhost (executor driver) (95/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000095_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 96.0 in stage 3.0 (TID 99, localhost, executor driver, partition 96, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98) in 17 ms on localhost (executor driver) (96/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 96.0 in stage 3.0 (TID 99)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191749_0003_m_000096_0
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Starting task 97.0 in stage 3.0 (TID 100, localhost, executor driver, partition 97, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Running task 97.0 in stage 3.0 (TID 100)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99) in 13 ms on localhost (executor driver) (97/200)
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000097_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 98.0 in stage 3.0 (TID 101, localhost, executor driver, partition 98, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 98.0 in stage 3.0 (TID 101)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100) in 14 ms on localhost (executor driver) (98/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000098_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 99.0 in stage 3.0 (TID 102, localhost, executor driver, partition 99, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101) in 18 ms on localhost (executor driver) (99/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 99.0 in stage 3.0 (TID 102)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000099_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 100.0 in stage 3.0 (TID 103, localhost, executor driver, partition 100, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102) in 15 ms on localhost (executor driver) (100/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 100.0 in stage 3.0 (TID 103)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000100_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 101.0 in stage 3.0 (TID 104, localhost, executor driver, partition 101, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 101.0 in stage 3.0 (TID 104)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103) in 14 ms on localhost (executor driver) (101/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000101_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 102.0 in stage 3.0 (TID 105, localhost, executor driver, partition 102, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 102.0 in stage 3.0 (TID 105)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104) in 21 ms on localhost (executor driver) (102/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000102_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 103.0 in stage 3.0 (TID 106, localhost, executor driver, partition 103, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 103.0 in stage 3.0 (TID 106)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105) in 19 ms on localhost (executor driver) (103/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000103_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 104.0 in stage 3.0 (TID 107, localhost, executor driver, partition 104, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106) in 16 ms on localhost (executor driver) (104/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 104.0 in stage 3.0 (TID 107)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000104_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 105.0 in stage 3.0 (TID 108, localhost, executor driver, partition 105, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 105.0 in stage 3.0 (TID 108)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107) in 14 ms on localhost (executor driver) (105/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000105_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 106.0 in stage 3.0 (TID 109, localhost, executor driver, partition 106, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108) in 14 ms on localhost (executor driver) (106/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 106.0 in stage 3.0 (TID 109)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000106_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 107.0 in stage 3.0 (TID 110, localhost, executor driver, partition 107, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 107.0 in stage 3.0 (TID 110)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109) in 16 ms on localhost (executor driver) (107/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000107_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 108.0 in stage 3.0 (TID 111, localhost, executor driver, partition 108, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110) in 17 ms on localhost (executor driver) (108/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 108.0 in stage 3.0 (TID 111)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000108_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 109.0 in stage 3.0 (TID 112, localhost, executor driver, partition 109, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 109.0 in stage 3.0 (TID 112)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111) in 15 ms on localhost (executor driver) (109/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000109_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 110.0 in stage 3.0 (TID 113, localhost, executor driver, partition 110, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 110.0 in stage 3.0 (TID 113)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112) in 13 ms on localhost (executor driver) (110/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000110_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 111.0 in stage 3.0 (TID 114, localhost, executor driver, partition 111, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 111.0 in stage 3.0 (TID 114)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113) in 15 ms on localhost (executor driver) (111/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000111_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 112.0 in stage 3.0 (TID 115, localhost, executor driver, partition 112, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 112.0 in stage 3.0 (TID 115)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114) in 12 ms on localhost (executor driver) (112/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000112_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 113.0 in stage 3.0 (TID 116, localhost, executor driver, partition 113, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 113.0 in stage 3.0 (TID 116)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115) in 14 ms on localhost (executor driver) (113/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000113_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 114.0 in stage 3.0 (TID 117, localhost, executor driver, partition 114, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 114.0 in stage 3.0 (TID 117)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116) in 14 ms on localhost (executor driver) (114/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000114_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 115.0 in stage 3.0 (TID 118, localhost, executor driver, partition 115, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 115.0 in stage 3.0 (TID 118)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117) in 15 ms on localhost (executor driver) (115/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000115_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 116.0 in stage 3.0 (TID 119, localhost, executor driver, partition 116, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 116.0 in stage 3.0 (TID 119)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118) in 12 ms on localhost (executor driver) (116/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000116_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 117.0 in stage 3.0 (TID 120, localhost, executor driver, partition 117, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 117.0 in stage 3.0 (TID 120)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119) in 12 ms on localhost (executor driver) (117/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000117_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 118.0 in stage 3.0 (TID 121, localhost, executor driver, partition 118, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 118.0 in stage 3.0 (TID 121)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120) in 15 ms on localhost (executor driver) (118/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000118_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 119.0 in stage 3.0 (TID 122, localhost, executor driver, partition 119, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 119.0 in stage 3.0 (TID 122)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121) in 14 ms on localhost (executor driver) (119/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000119_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 120.0 in stage 3.0 (TID 123, localhost, executor driver, partition 120, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 120.0 in stage 3.0 (TID 123)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122) in 13 ms on localhost (executor driver) (120/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000120_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 121.0 in stage 3.0 (TID 124, localhost, executor driver, partition 121, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 121.0 in stage 3.0 (TID 124)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123) in 18 ms on localhost (executor driver) (121/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000121_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 122.0 in stage 3.0 (TID 125, localhost, executor driver, partition 122, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 122.0 in stage 3.0 (TID 125)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124) in 15 ms on localhost (executor driver) (122/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000122_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 123.0 in stage 3.0 (TID 126, localhost, executor driver, partition 123, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125) in 17 ms on localhost (executor driver) (123/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 123.0 in stage 3.0 (TID 126)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000123_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 124.0 in stage 3.0 (TID 127, localhost, executor driver, partition 124, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 124.0 in stage 3.0 (TID 127)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126) in 21 ms on localhost (executor driver) (124/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000124_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 125.0 in stage 3.0 (TID 128, localhost, executor driver, partition 125, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 125.0 in stage 3.0 (TID 128)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127) in 22 ms on localhost (executor driver) (125/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000125_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 126.0 in stage 3.0 (TID 129, localhost, executor driver, partition 126, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 126.0 in stage 3.0 (TID 129)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128) in 21 ms on localhost (executor driver) (126/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000126_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 127.0 in stage 3.0 (TID 130, localhost, executor driver, partition 127, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 127.0 in stage 3.0 (TID 130)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129) in 22 ms on localhost (executor driver) (127/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000127_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 128.0 in stage 3.0 (TID 131, localhost, executor driver, partition 128, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 128.0 in stage 3.0 (TID 131)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130) in 19 ms on localhost (executor driver) (128/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000128_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 129.0 in stage 3.0 (TID 132, localhost, executor driver, partition 129, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 129.0 in stage 3.0 (TID 132)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131) in 17 ms on localhost (executor driver) (129/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000129_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 130.0 in stage 3.0 (TID 133, localhost, executor driver, partition 130, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 130.0 in stage 3.0 (TID 133)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132) in 14 ms on localhost (executor driver) (130/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000130_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 131.0 in stage 3.0 (TID 134, localhost, executor driver, partition 131, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133) in 37 ms on localhost (executor driver) (131/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 131.0 in stage 3.0 (TID 134)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000131_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 132.0 in stage 3.0 (TID 135, localhost, executor driver, partition 132, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 132.0 in stage 3.0 (TID 135)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134) in 16 ms on localhost (executor driver) (132/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000132_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 133.0 in stage 3.0 (TID 136, localhost, executor driver, partition 133, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 133.0 in stage 3.0 (TID 136)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135) in 16 ms on localhost (executor driver) (133/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000133_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 134.0 in stage 3.0 (TID 137, localhost, executor driver, partition 134, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 134.0 in stage 3.0 (TID 137)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136) in 12 ms on localhost (executor driver) (134/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000134_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 135.0 in stage 3.0 (TID 138, localhost, executor driver, partition 135, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 135.0 in stage 3.0 (TID 138)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137) in 14 ms on localhost (executor driver) (135/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000135_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 136.0 in stage 3.0 (TID 139, localhost, executor driver, partition 136, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 136.0 in stage 3.0 (TID 139)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138) in 16 ms on localhost (executor driver) (136/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000136_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 137.0 in stage 3.0 (TID 140, localhost, executor driver, partition 137, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 137.0 in stage 3.0 (TID 140)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139) in 13 ms on localhost (executor driver) (137/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000137_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 138.0 in stage 3.0 (TID 141, localhost, executor driver, partition 138, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 138.0 in stage 3.0 (TID 141)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140) in 14 ms on localhost (executor driver) (138/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000138_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 139.0 in stage 3.0 (TID 142, localhost, executor driver, partition 139, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141) in 14 ms on localhost (executor driver) (139/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 139.0 in stage 3.0 (TID 142)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000139_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 140.0 in stage 3.0 (TID 143, localhost, executor driver, partition 140, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 140.0 in stage 3.0 (TID 143)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142) in 13 ms on localhost (executor driver) (140/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000140_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 141.0 in stage 3.0 (TID 144, localhost, executor driver, partition 141, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 141.0 in stage 3.0 (TID 144)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143) in 11 ms on localhost (executor driver) (141/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000141_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 142.0 in stage 3.0 (TID 145, localhost, executor driver, partition 142, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 142.0 in stage 3.0 (TID 145)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144) in 12 ms on localhost (executor driver) (142/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000142_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 143.0 in stage 3.0 (TID 146, localhost, executor driver, partition 143, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 143.0 in stage 3.0 (TID 146)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145) in 15 ms on localhost (executor driver) (143/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000143_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 144.0 in stage 3.0 (TID 147, localhost, executor driver, partition 144, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 144.0 in stage 3.0 (TID 147)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146) in 12 ms on localhost (executor driver) (144/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000144_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 145.0 in stage 3.0 (TID 148, localhost, executor driver, partition 145, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 145.0 in stage 3.0 (TID 148)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147) in 12 ms on localhost (executor driver) (145/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000145_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 146.0 in stage 3.0 (TID 149, localhost, executor driver, partition 146, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 146.0 in stage 3.0 (TID 149)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148) in 11 ms on localhost (executor driver) (146/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000146_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 147.0 in stage 3.0 (TID 150, localhost, executor driver, partition 147, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 147.0 in stage 3.0 (TID 150)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149) in 14 ms on localhost (executor driver) (147/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000147_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 148.0 in stage 3.0 (TID 151, localhost, executor driver, partition 148, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 148.0 in stage 3.0 (TID 151)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150) in 11 ms on localhost (executor driver) (148/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000148_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 149.0 in stage 3.0 (TID 152, localhost, executor driver, partition 149, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 149.0 in stage 3.0 (TID 152)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151) in 12 ms on localhost (executor driver) (149/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000149_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 150.0 in stage 3.0 (TID 153, localhost, executor driver, partition 150, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 150.0 in stage 3.0 (TID 153)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152) in 14 ms on localhost (executor driver) (150/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000150_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 151.0 in stage 3.0 (TID 154, localhost, executor driver, partition 151, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153) in 15 ms on localhost (executor driver) (151/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 151.0 in stage 3.0 (TID 154)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000151_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 152.0 in stage 3.0 (TID 155, localhost, executor driver, partition 152, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 152.0 in stage 3.0 (TID 155)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154) in 19 ms on localhost (executor driver) (152/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000152_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 153.0 in stage 3.0 (TID 156, localhost, executor driver, partition 153, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 153.0 in stage 3.0 (TID 156)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155) in 19 ms on localhost (executor driver) (153/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000153_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 154.0 in stage 3.0 (TID 157, localhost, executor driver, partition 154, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 154.0 in stage 3.0 (TID 157)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156) in 15 ms on localhost (executor driver) (154/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000154_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 155.0 in stage 3.0 (TID 158, localhost, executor driver, partition 155, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157) in 16 ms on localhost (executor driver) (155/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 155.0 in stage 3.0 (TID 158)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000155_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 156.0 in stage 3.0 (TID 159, localhost, executor driver, partition 156, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 156.0 in stage 3.0 (TID 159)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158) in 17 ms on localhost (executor driver) (156/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000156_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 157.0 in stage 3.0 (TID 160, localhost, executor driver, partition 157, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 157.0 in stage 3.0 (TID 160)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159) in 17 ms on localhost (executor driver) (157/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000157_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 158.0 in stage 3.0 (TID 161, localhost, executor driver, partition 158, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 158.0 in stage 3.0 (TID 161)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160) in 15 ms on localhost (executor driver) (158/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000158_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 159.0 in stage 3.0 (TID 162, localhost, executor driver, partition 159, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 159.0 in stage 3.0 (TID 162)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161) in 13 ms on localhost (executor driver) (159/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000159_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 160.0 in stage 3.0 (TID 163, localhost, executor driver, partition 160, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162) in 12 ms on localhost (executor driver) (160/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 160.0 in stage 3.0 (TID 163)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000160_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 161.0 in stage 3.0 (TID 164, localhost, executor driver, partition 161, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 161.0 in stage 3.0 (TID 164)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163) in 13 ms on localhost (executor driver) (161/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000161_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 162.0 in stage 3.0 (TID 165, localhost, executor driver, partition 162, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 162.0 in stage 3.0 (TID 165)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164) in 14 ms on localhost (executor driver) (162/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000162_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 163.0 in stage 3.0 (TID 166, localhost, executor driver, partition 163, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 163.0 in stage 3.0 (TID 166)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165) in 14 ms on localhost (executor driver) (163/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000163_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 164.0 in stage 3.0 (TID 167, localhost, executor driver, partition 164, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166) in 12 ms on localhost (executor driver) (164/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 164.0 in stage 3.0 (TID 167)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000164_0
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Starting task 165.0 in stage 3.0 (TID 168, localhost, executor driver, partition 165, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Running task 165.0 in stage 3.0 (TID 168)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167) in 14 ms on localhost (executor driver) (165/200)
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:50] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191750_0003_m_000165_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 166.0 in stage 3.0 (TID 169, localhost, executor driver, partition 166, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 166.0 in stage 3.0 (TID 169)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168) in 12 ms on localhost (executor driver) (166/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000166_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 167.0 in stage 3.0 (TID 170, localhost, executor driver, partition 167, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169) in 14 ms on localhost (executor driver) (167/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 167.0 in stage 3.0 (TID 170)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000167_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 168.0 in stage 3.0 (TID 171, localhost, executor driver, partition 168, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 168.0 in stage 3.0 (TID 171)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170) in 13 ms on localhost (executor driver) (168/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000168_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 169.0 in stage 3.0 (TID 172, localhost, executor driver, partition 169, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 169.0 in stage 3.0 (TID 172)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171) in 12 ms on localhost (executor driver) (169/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000169_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 170.0 in stage 3.0 (TID 173, localhost, executor driver, partition 170, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 170.0 in stage 3.0 (TID 173)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172) in 16 ms on localhost (executor driver) (170/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000170_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 171.0 in stage 3.0 (TID 174, localhost, executor driver, partition 171, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173) in 23 ms on localhost (executor driver) (171/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 171.0 in stage 3.0 (TID 174)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000171_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 172.0 in stage 3.0 (TID 175, localhost, executor driver, partition 172, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 172.0 in stage 3.0 (TID 175)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174) in 18 ms on localhost (executor driver) (172/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000172_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 173.0 in stage 3.0 (TID 176, localhost, executor driver, partition 173, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175) in 21 ms on localhost (executor driver) (173/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 173.0 in stage 3.0 (TID 176)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000173_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 174.0 in stage 3.0 (TID 177, localhost, executor driver, partition 174, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 174.0 in stage 3.0 (TID 177)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176) in 21 ms on localhost (executor driver) (174/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000174_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 175.0 in stage 3.0 (TID 178, localhost, executor driver, partition 175, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 175.0 in stage 3.0 (TID 178)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177) in 15 ms on localhost (executor driver) (175/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000175_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 176.0 in stage 3.0 (TID 179, localhost, executor driver, partition 176, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 176.0 in stage 3.0 (TID 179)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178) in 11 ms on localhost (executor driver) (176/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000176_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 177.0 in stage 3.0 (TID 180, localhost, executor driver, partition 177, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 177.0 in stage 3.0 (TID 180)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179) in 11 ms on localhost (executor driver) (177/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000177_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 178.0 in stage 3.0 (TID 181, localhost, executor driver, partition 178, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180) in 25 ms on localhost (executor driver) (178/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 178.0 in stage 3.0 (TID 181)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000178_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 179.0 in stage 3.0 (TID 182, localhost, executor driver, partition 179, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181) in 18 ms on localhost (executor driver) (179/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 179.0 in stage 3.0 (TID 182)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000179_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 180.0 in stage 3.0 (TID 183, localhost, executor driver, partition 180, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 180.0 in stage 3.0 (TID 183)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182) in 12 ms on localhost (executor driver) (180/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000180_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 181.0 in stage 3.0 (TID 184, localhost, executor driver, partition 181, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 181.0 in stage 3.0 (TID 184)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183) in 14 ms on localhost (executor driver) (181/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000181_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 182.0 in stage 3.0 (TID 185, localhost, executor driver, partition 182, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184) in 18 ms on localhost (executor driver) (182/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 182.0 in stage 3.0 (TID 185)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000182_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 183.0 in stage 3.0 (TID 186, localhost, executor driver, partition 183, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185) in 21 ms on localhost (executor driver) (183/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 183.0 in stage 3.0 (TID 186)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000183_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 184.0 in stage 3.0 (TID 187, localhost, executor driver, partition 184, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 184.0 in stage 3.0 (TID 187)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186) in 20 ms on localhost (executor driver) (184/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000184_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 185.0 in stage 3.0 (TID 188, localhost, executor driver, partition 185, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187) in 17 ms on localhost (executor driver) (185/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 185.0 in stage 3.0 (TID 188)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000185_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 186.0 in stage 3.0 (TID 189, localhost, executor driver, partition 186, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188) in 19 ms on localhost (executor driver) (186/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 186.0 in stage 3.0 (TID 189)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000186_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 187.0 in stage 3.0 (TID 190, localhost, executor driver, partition 187, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189) in 22 ms on localhost (executor driver) (187/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 187.0 in stage 3.0 (TID 190)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000187_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 188.0 in stage 3.0 (TID 191, localhost, executor driver, partition 188, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 188.0 in stage 3.0 (TID 191)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190) in 16 ms on localhost (executor driver) (188/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000188_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 189.0 in stage 3.0 (TID 192, localhost, executor driver, partition 189, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191) in 16 ms on localhost (executor driver) (189/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 189.0 in stage 3.0 (TID 192)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000189_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 190.0 in stage 3.0 (TID 193, localhost, executor driver, partition 190, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 190.0 in stage 3.0 (TID 193)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192) in 11 ms on localhost (executor driver) (190/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000190_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 191.0 in stage 3.0 (TID 194, localhost, executor driver, partition 191, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 191.0 in stage 3.0 (TID 194)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193) in 11 ms on localhost (executor driver) (191/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000191_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 192.0 in stage 3.0 (TID 195, localhost, executor driver, partition 192, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194) in 15 ms on localhost (executor driver) (192/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 192.0 in stage 3.0 (TID 195)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000192_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 193.0 in stage 3.0 (TID 196, localhost, executor driver, partition 193, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 193.0 in stage 3.0 (TID 196)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195) in 14 ms on localhost (executor driver) (193/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000193_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 194.0 in stage 3.0 (TID 197, localhost, executor driver, partition 194, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 194.0 in stage 3.0 (TID 197)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196) in 10 ms on localhost (executor driver) (194/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000194_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 195.0 in stage 3.0 (TID 198, localhost, executor driver, partition 195, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 195.0 in stage 3.0 (TID 198)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197) in 10 ms on localhost (executor driver) (195/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000195_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 196.0 in stage 3.0 (TID 199, localhost, executor driver, partition 196, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 196.0 in stage 3.0 (TID 199)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198) in 10 ms on localhost (executor driver) (196/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000196_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199). 3689 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 197.0 in stage 3.0 (TID 200, localhost, executor driver, partition 197, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199) in 39 ms on localhost (executor driver) (197/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 197.0 in stage 3.0 (TID 200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000197_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 198.0 in stage 3.0 (TID 201, localhost, executor driver, partition 198, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200) in 27 ms on localhost (executor driver) (198/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 198.0 in stage 3.0 (TID 201)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000198_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201). 3603 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 199.0 in stage 3.0 (TID 202, localhost, executor driver, partition 199, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 199.0 in stage 3.0 (TID 202)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201) in 20 ms on localhost (executor driver) (199/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191751_0003_m_000199_0
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202). 3646 bytes result sent to driver
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202) in 17 ms on localhost (executor driver) (200/200)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] ResultStage 3 (saveAsTable at HistoryDataToA.scala:101) finished in 4.619 s
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:101, took 5.362769 s
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Final stage: ResultStage 4 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Submitting ResultStage 4 (MapPartitionsRDD[44] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Block broadcast_9 stored as values in memory (estimated size 81.0 KB, free 1926.1 MB)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1926.1 MB)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Added broadcast_9_piece0 in memory on 192.168.0.152:63183 (size: 30.9 KB, free: 1991.6 MB)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 4 (MapPartitionsRDD[44] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Adding task set 4.0 with 15 tasks
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Starting task 0.0 in stage 4.0 (TID 203, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Running task 0.0 in stage 4.0 (TID 203)
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Code generated in 11.246634 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Code generated in 7.561063 ms
[INFO ] [2018-04-26 19:17:51] [Logging$class:logInfo:54] Code generated in 10.957029 ms
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Code generated in 5.920851 ms
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Code generated in 11.83113 ms
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Code generated in 10.796557 ms
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191752_0004_m_000000_0
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Finished task 0.0 in stage 4.0 (TID 203). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Starting task 1.0 in stage 4.0 (TID 204, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Running task 1.0 in stage 4.0 (TID 204)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Finished task 0.0 in stage 4.0 (TID 203) in 108 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191752_0004_m_000001_0
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Finished task 1.0 in stage 4.0 (TID 204). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Starting task 2.0 in stage 4.0 (TID 205, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Running task 2.0 in stage 4.0 (TID 205)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Finished task 1.0 in stage 4.0 (TID 204) in 741 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191752_0004_m_000002_0
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Finished task 2.0 in stage 4.0 (TID 205). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Starting task 3.0 in stage 4.0 (TID 206, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Running task 3.0 in stage 4.0 (TID 206)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Finished task 2.0 in stage 4.0 (TID 205) in 178 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-26 19:17:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191753_0004_m_000003_0
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Finished task 3.0 in stage 4.0 (TID 206). 1796 bytes result sent to driver
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Starting task 4.0 in stage 4.0 (TID 207, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Running task 4.0 in stage 4.0 (TID 207)
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Finished task 3.0 in stage 4.0 (TID 206) in 636 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191753_0004_m_000004_0
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Finished task 4.0 in stage 4.0 (TID 207). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Starting task 5.0 in stage 4.0 (TID 208, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Running task 5.0 in stage 4.0 (TID 208)
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Finished task 4.0 in stage 4.0 (TID 207) in 165 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-26 19:17:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191754_0004_m_000005_0
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 5.0 in stage 4.0 (TID 208). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Starting task 6.0 in stage 4.0 (TID 209, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Running task 6.0 in stage 4.0 (TID 209)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 5.0 in stage 4.0 (TID 208) in 296 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191754_0004_m_000006_0
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 6.0 in stage 4.0 (TID 209). 1710 bytes result sent to driver
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Starting task 7.0 in stage 4.0 (TID 210, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Running task 7.0 in stage 4.0 (TID 210)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 6.0 in stage 4.0 (TID 209) in 40 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191754_0004_m_000007_0
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 7.0 in stage 4.0 (TID 210). 1796 bytes result sent to driver
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Starting task 8.0 in stage 4.0 (TID 211, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Running task 8.0 in stage 4.0 (TID 211)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 7.0 in stage 4.0 (TID 210) in 304 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191754_0004_m_000008_0
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 8.0 in stage 4.0 (TID 211). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Starting task 9.0 in stage 4.0 (TID 212, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Running task 9.0 in stage 4.0 (TID 212)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Finished task 8.0 in stage 4.0 (TID 211) in 297 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-26 19:17:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191755_0004_m_000009_0
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] Finished task 9.0 in stage 4.0 (TID 212). 1796 bytes result sent to driver
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] Starting task 10.0 in stage 4.0 (TID 213, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] Running task 10.0 in stage 4.0 (TID 213)
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] Finished task 9.0 in stage 4.0 (TID 212) in 884 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-26 19:17:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191756_0004_m_000010_0
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Finished task 10.0 in stage 4.0 (TID 213). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Starting task 11.0 in stage 4.0 (TID 214, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Running task 11.0 in stage 4.0 (TID 214)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Finished task 10.0 in stage 4.0 (TID 213) in 966 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191756_0004_m_000011_0
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Finished task 11.0 in stage 4.0 (TID 214). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Starting task 12.0 in stage 4.0 (TID 215, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Running task 12.0 in stage 4.0 (TID 215)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Finished task 11.0 in stage 4.0 (TID 214) in 96 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191756_0004_m_000012_0
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Finished task 12.0 in stage 4.0 (TID 215). 1753 bytes result sent to driver
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Starting task 13.0 in stage 4.0 (TID 216, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Running task 13.0 in stage 4.0 (TID 216)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Finished task 12.0 in stage 4.0 (TID 215) in 269 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-26 19:17:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191757_0004_m_000013_0
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Finished task 13.0 in stage 4.0 (TID 216). 1796 bytes result sent to driver
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Starting task 14.0 in stage 4.0 (TID 217, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Running task 14.0 in stage 4.0 (TID 217)
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Finished task 13.0 in stage 4.0 (TID 216) in 337 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180426191757_0004_m_000014_0
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Finished task 14.0 in stage 4.0 (TID 217). 1796 bytes result sent to driver
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Finished task 14.0 in stage 4.0 (TID 217) in 129 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] ResultStage 4 (saveAsTable at HistoryDataToA.scala:114) finished in 5.436 s
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Job 2 finished: saveAsTable at HistoryDataToA.scala:114, took 5.459627 s
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-26 19:17:58] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-92de6265-d023-4a33-bfae-a3945a8f99b1
