[INFO ] [2018-04-27 09:43:59] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 09:44:00] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 09:44:00] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 09:44:00] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 09:44:00] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 09:44:00] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 09:44:00] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 09:44:01] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 54017.
[INFO ] [2018-04-27 09:44:01] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 09:44:01] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 09:44:01] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 09:44:01] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 09:44:01] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-bb410b5f-1db8-489a-baee-55701f3d3252
[INFO ] [2018-04-27 09:44:01] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 09:44:02] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 09:44:02] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54028.
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Server created on 192.168.0.152:54028
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 54028, None)
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:54028 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 54028, None)
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 54028, None)
[INFO ] [2018-04-27 09:44:03] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 54028, None)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:54028 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:54028 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 214.6 KB, free 1991.3 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.3 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:54028 (size: 20.6 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 09:44:04] [Logging$class:logInfo:54] Created broadcast 2 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 09:44:06] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 09:44:06] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 09:44:06] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 09:44:06] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 09:44:12] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 09:44:13] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 09:44:13] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 09:44:14] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:16] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:17] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:21] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 09:44:22] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Code generated in 328.320563 ms
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 224.7 KB, free 1991.1 MB)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.1 MB)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:54028 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Created broadcast 3 from 
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 11.9 KB, free 1991.1 MB)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1991.1 MB)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:54028 (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 09:44:23] [Logging$class:logInfo:54] Code generated in 15.106652 ms
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 73065 bytes result sent to driver
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 650 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.668 s
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.790484 s
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Code generated in 10.398212 ms
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 64.5 MB, free 1926.6 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 204.8 KB, free 1926.4 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:54028 (size: 204.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Created broadcast 5 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Code generated in 34.044999 ms
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Code generated in 17.424624 ms
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Removed broadcast_4_piece0 on 192.168.0.152:54028 in memory (size: 6.0 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:101
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Registering RDD 25 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Registering RDD 29 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:101) with 200 output partitions
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:101)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 17.3 KB, free 1926.4 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1926.3 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:54028 (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[25] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Block broadcast_7 stored as values in memory (estimated size 12.3 KB, free 1926.3 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.3 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Added broadcast_7_piece0 in memory on 192.168.0.152:54028 (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[29] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Code generated in 17.37214 ms
[INFO ] [2018-04-27 09:44:24] [Logging$class:logInfo:54] Code generated in 14.446639 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1453 bytes result sent to driver
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 394 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:101) finished in 0.396 s
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 10.988372 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 10.729728 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2). 1367 bytes result sent to driver
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2) in 146 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:101) finished in 0.521 s
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] running: Set()
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Submitting ResultStage 3 (MapPartitionsRDD[34] at saveAsTable at HistoryDataToA.scala:101), which has no missing parents
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Block broadcast_8 stored as values in memory (estimated size 114.5 KB, free 1926.2 MB)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Block broadcast_8_piece0 stored as bytes in memory (estimated size 43.2 KB, free 1926.2 MB)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Added broadcast_8_piece0 in memory on 192.168.0.152:54028 (size: 43.2 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[34] at saveAsTable at HistoryDataToA.scala:101) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Adding task set 3.0 with 200 tasks
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Running task 0.0 in stage 3.0 (TID 3)
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Started 0 remote fetches in 6 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 24.539766 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 15.85464 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 7.987733 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 11.009139 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 29.506848 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 7.063414 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 8.697208 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 36.819844 ms
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:25] [Logging$class:logInfo:54] Code generated in 15.56277 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Code generated in 51.877787 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Code generated in 41.023079 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094425_0003_m_000000_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3). 3732 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 1.0 in stage 3.0 (TID 4)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3) in 593 ms on localhost (executor driver) (1/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000001_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 2.0 in stage 3.0 (TID 5, localhost, executor driver, partition 2, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4) in 75 ms on localhost (executor driver) (2/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 2.0 in stage 3.0 (TID 5)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 8 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 11 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000002_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 3.0 in stage 3.0 (TID 6, localhost, executor driver, partition 3, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 3.0 in stage 3.0 (TID 6)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5) in 102 ms on localhost (executor driver) (3/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000003_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 4.0 in stage 3.0 (TID 7, localhost, executor driver, partition 4, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6) in 70 ms on localhost (executor driver) (4/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 4.0 in stage 3.0 (TID 7)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000004_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 5.0 in stage 3.0 (TID 8, localhost, executor driver, partition 5, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7) in 61 ms on localhost (executor driver) (5/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 5.0 in stage 3.0 (TID 8)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000005_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 6.0 in stage 3.0 (TID 9, localhost, executor driver, partition 6, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8) in 38 ms on localhost (executor driver) (6/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 6.0 in stage 3.0 (TID 9)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000006_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 7.0 in stage 3.0 (TID 10, localhost, executor driver, partition 7, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9) in 67 ms on localhost (executor driver) (7/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 7.0 in stage 3.0 (TID 10)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000007_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 8.0 in stage 3.0 (TID 11, localhost, executor driver, partition 8, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 8.0 in stage 3.0 (TID 11)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10) in 51 ms on localhost (executor driver) (8/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000008_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 9.0 in stage 3.0 (TID 12, localhost, executor driver, partition 9, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11) in 37 ms on localhost (executor driver) (9/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 9.0 in stage 3.0 (TID 12)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000009_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 10.0 in stage 3.0 (TID 13, localhost, executor driver, partition 10, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 10.0 in stage 3.0 (TID 13)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12) in 30 ms on localhost (executor driver) (10/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Removed broadcast_7_piece0 on 192.168.0.152:54028 in memory (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000010_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13). 3732 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 11.0 in stage 3.0 (TID 14, localhost, executor driver, partition 11, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Removed broadcast_6_piece0 on 192.168.0.152:54028 in memory (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 11.0 in stage 3.0 (TID 14)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13) in 63 ms on localhost (executor driver) (11/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000011_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 12.0 in stage 3.0 (TID 15, localhost, executor driver, partition 12, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14) in 39 ms on localhost (executor driver) (12/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 12.0 in stage 3.0 (TID 15)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000012_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 13.0 in stage 3.0 (TID 16, localhost, executor driver, partition 13, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15) in 38 ms on localhost (executor driver) (13/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 13.0 in stage 3.0 (TID 16)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000013_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 14.0 in stage 3.0 (TID 17, localhost, executor driver, partition 14, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16) in 35 ms on localhost (executor driver) (14/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 14.0 in stage 3.0 (TID 17)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000014_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 15.0 in stage 3.0 (TID 18, localhost, executor driver, partition 15, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17) in 32 ms on localhost (executor driver) (15/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 15.0 in stage 3.0 (TID 18)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000015_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 16.0 in stage 3.0 (TID 19, localhost, executor driver, partition 16, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 16.0 in stage 3.0 (TID 19)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18) in 36 ms on localhost (executor driver) (16/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000016_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 17.0 in stage 3.0 (TID 20, localhost, executor driver, partition 17, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19) in 39 ms on localhost (executor driver) (17/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 17.0 in stage 3.0 (TID 20)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000017_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 18.0 in stage 3.0 (TID 21, localhost, executor driver, partition 18, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20) in 34 ms on localhost (executor driver) (18/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 18.0 in stage 3.0 (TID 21)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000018_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 19.0 in stage 3.0 (TID 22, localhost, executor driver, partition 19, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 19.0 in stage 3.0 (TID 22)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21) in 30 ms on localhost (executor driver) (19/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094426_0003_m_000019_0
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Starting task 20.0 in stage 3.0 (TID 23, localhost, executor driver, partition 20, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22) in 25 ms on localhost (executor driver) (20/200)
[INFO ] [2018-04-27 09:44:26] [Logging$class:logInfo:54] Running task 20.0 in stage 3.0 (TID 23)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000020_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 21.0 in stage 3.0 (TID 24, localhost, executor driver, partition 21, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23) in 33 ms on localhost (executor driver) (21/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 21.0 in stage 3.0 (TID 24)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000021_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 22.0 in stage 3.0 (TID 25, localhost, executor driver, partition 22, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 22.0 in stage 3.0 (TID 25)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24) in 30 ms on localhost (executor driver) (22/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000022_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 23.0 in stage 3.0 (TID 26, localhost, executor driver, partition 23, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 23.0 in stage 3.0 (TID 26)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25) in 22 ms on localhost (executor driver) (23/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000023_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 24.0 in stage 3.0 (TID 27, localhost, executor driver, partition 24, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 24.0 in stage 3.0 (TID 27)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26) in 21 ms on localhost (executor driver) (24/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000024_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 25.0 in stage 3.0 (TID 28, localhost, executor driver, partition 25, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27) in 22 ms on localhost (executor driver) (25/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 25.0 in stage 3.0 (TID 28)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000025_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 26.0 in stage 3.0 (TID 29, localhost, executor driver, partition 26, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28) in 55 ms on localhost (executor driver) (26/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 26.0 in stage 3.0 (TID 29)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000026_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 27.0 in stage 3.0 (TID 30, localhost, executor driver, partition 27, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29) in 23 ms on localhost (executor driver) (27/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 27.0 in stage 3.0 (TID 30)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000027_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 28.0 in stage 3.0 (TID 31, localhost, executor driver, partition 28, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 28.0 in stage 3.0 (TID 31)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30) in 26 ms on localhost (executor driver) (28/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000028_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 29.0 in stage 3.0 (TID 32, localhost, executor driver, partition 29, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 29.0 in stage 3.0 (TID 32)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31) in 29 ms on localhost (executor driver) (29/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000029_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 30.0 in stage 3.0 (TID 33, localhost, executor driver, partition 30, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 30.0 in stage 3.0 (TID 33)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32) in 20 ms on localhost (executor driver) (30/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000030_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 31.0 in stage 3.0 (TID 34, localhost, executor driver, partition 31, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33) in 21 ms on localhost (executor driver) (31/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 31.0 in stage 3.0 (TID 34)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000031_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 32.0 in stage 3.0 (TID 35, localhost, executor driver, partition 32, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34) in 24 ms on localhost (executor driver) (32/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 32.0 in stage 3.0 (TID 35)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000032_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 33.0 in stage 3.0 (TID 36, localhost, executor driver, partition 33, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35) in 26 ms on localhost (executor driver) (33/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 33.0 in stage 3.0 (TID 36)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000033_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 34.0 in stage 3.0 (TID 37, localhost, executor driver, partition 34, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 34.0 in stage 3.0 (TID 37)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36) in 49 ms on localhost (executor driver) (34/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000034_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 35.0 in stage 3.0 (TID 38, localhost, executor driver, partition 35, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37) in 23 ms on localhost (executor driver) (35/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 35.0 in stage 3.0 (TID 38)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000035_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 36.0 in stage 3.0 (TID 39, localhost, executor driver, partition 36, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 36.0 in stage 3.0 (TID 39)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38) in 19 ms on localhost (executor driver) (36/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000036_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 37.0 in stage 3.0 (TID 40, localhost, executor driver, partition 37, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39) in 21 ms on localhost (executor driver) (37/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 37.0 in stage 3.0 (TID 40)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000037_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 38.0 in stage 3.0 (TID 41, localhost, executor driver, partition 38, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40) in 21 ms on localhost (executor driver) (38/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 38.0 in stage 3.0 (TID 41)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000038_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 39.0 in stage 3.0 (TID 42, localhost, executor driver, partition 39, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 39.0 in stage 3.0 (TID 42)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41) in 22 ms on localhost (executor driver) (39/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000039_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 40.0 in stage 3.0 (TID 43, localhost, executor driver, partition 40, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 40.0 in stage 3.0 (TID 43)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42) in 21 ms on localhost (executor driver) (40/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000040_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 41.0 in stage 3.0 (TID 44, localhost, executor driver, partition 41, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43) in 20 ms on localhost (executor driver) (41/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 41.0 in stage 3.0 (TID 44)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000041_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 42.0 in stage 3.0 (TID 45, localhost, executor driver, partition 42, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44) in 204 ms on localhost (executor driver) (42/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 42.0 in stage 3.0 (TID 45)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000042_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 43.0 in stage 3.0 (TID 46, localhost, executor driver, partition 43, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45) in 36 ms on localhost (executor driver) (43/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 43.0 in stage 3.0 (TID 46)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000043_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 44.0 in stage 3.0 (TID 47, localhost, executor driver, partition 44, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 44.0 in stage 3.0 (TID 47)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46) in 21 ms on localhost (executor driver) (44/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 7 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 56 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000044_0
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47). 3732 bytes result sent to driver
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Starting task 45.0 in stage 3.0 (TID 48, localhost, executor driver, partition 45, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47) in 198 ms on localhost (executor driver) (45/200)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Running task 45.0 in stage 3.0 (TID 48)
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094427_0003_m_000045_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 46.0 in stage 3.0 (TID 49, localhost, executor driver, partition 46, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48) in 66 ms on localhost (executor driver) (46/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 46.0 in stage 3.0 (TID 49)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000046_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 47.0 in stage 3.0 (TID 50, localhost, executor driver, partition 47, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49) in 43 ms on localhost (executor driver) (47/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 47.0 in stage 3.0 (TID 50)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000047_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 48.0 in stage 3.0 (TID 51, localhost, executor driver, partition 48, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50) in 116 ms on localhost (executor driver) (48/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 48.0 in stage 3.0 (TID 51)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000048_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 49.0 in stage 3.0 (TID 52, localhost, executor driver, partition 49, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51) in 194 ms on localhost (executor driver) (49/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 49.0 in stage 3.0 (TID 52)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000049_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 50.0 in stage 3.0 (TID 53, localhost, executor driver, partition 50, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52) in 24 ms on localhost (executor driver) (50/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 50.0 in stage 3.0 (TID 53)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000050_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 51.0 in stage 3.0 (TID 54, localhost, executor driver, partition 51, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53) in 42 ms on localhost (executor driver) (51/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 51.0 in stage 3.0 (TID 54)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000051_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 52.0 in stage 3.0 (TID 55, localhost, executor driver, partition 52, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54) in 61 ms on localhost (executor driver) (52/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 52.0 in stage 3.0 (TID 55)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000052_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 53.0 in stage 3.0 (TID 56, localhost, executor driver, partition 53, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55) in 23 ms on localhost (executor driver) (53/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 53.0 in stage 3.0 (TID 56)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000053_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 54.0 in stage 3.0 (TID 57, localhost, executor driver, partition 54, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 54.0 in stage 3.0 (TID 57)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56) in 106 ms on localhost (executor driver) (54/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000054_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 55.0 in stage 3.0 (TID 58, localhost, executor driver, partition 55, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 55.0 in stage 3.0 (TID 58)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57) in 38 ms on localhost (executor driver) (55/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000055_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 56.0 in stage 3.0 (TID 59, localhost, executor driver, partition 56, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58) in 37 ms on localhost (executor driver) (56/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 56.0 in stage 3.0 (TID 59)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000056_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59). 3732 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 57.0 in stage 3.0 (TID 60, localhost, executor driver, partition 57, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59) in 65 ms on localhost (executor driver) (57/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 57.0 in stage 3.0 (TID 60)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000057_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 58.0 in stage 3.0 (TID 61, localhost, executor driver, partition 58, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 58.0 in stage 3.0 (TID 61)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60) in 16 ms on localhost (executor driver) (58/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 3 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000058_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 59.0 in stage 3.0 (TID 62, localhost, executor driver, partition 59, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61) in 27 ms on localhost (executor driver) (59/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 59.0 in stage 3.0 (TID 62)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000059_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 60.0 in stage 3.0 (TID 63, localhost, executor driver, partition 60, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62) in 64 ms on localhost (executor driver) (60/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 60.0 in stage 3.0 (TID 63)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000060_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 61.0 in stage 3.0 (TID 64, localhost, executor driver, partition 61, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63) in 62 ms on localhost (executor driver) (61/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 61.0 in stage 3.0 (TID 64)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000061_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 62.0 in stage 3.0 (TID 65, localhost, executor driver, partition 62, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64) in 26 ms on localhost (executor driver) (62/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 62.0 in stage 3.0 (TID 65)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000062_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Starting task 63.0 in stage 3.0 (TID 66, localhost, executor driver, partition 63, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Running task 63.0 in stage 3.0 (TID 66)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65) in 17 ms on localhost (executor driver) (63/200)
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094428_0003_m_000063_0
[INFO ] [2018-04-27 09:44:28] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 64.0 in stage 3.0 (TID 67, localhost, executor driver, partition 64, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66) in 19 ms on localhost (executor driver) (64/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 64.0 in stage 3.0 (TID 67)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 165 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000064_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 65.0 in stage 3.0 (TID 68, localhost, executor driver, partition 65, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67) in 184 ms on localhost (executor driver) (65/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 65.0 in stage 3.0 (TID 68)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000065_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 66.0 in stage 3.0 (TID 69, localhost, executor driver, partition 66, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68) in 15 ms on localhost (executor driver) (66/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 66.0 in stage 3.0 (TID 69)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000066_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 67.0 in stage 3.0 (TID 70, localhost, executor driver, partition 67, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 67.0 in stage 3.0 (TID 70)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69) in 16 ms on localhost (executor driver) (67/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000067_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 68.0 in stage 3.0 (TID 71, localhost, executor driver, partition 68, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 68.0 in stage 3.0 (TID 71)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70) in 12 ms on localhost (executor driver) (68/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000068_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 69.0 in stage 3.0 (TID 72, localhost, executor driver, partition 69, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 69.0 in stage 3.0 (TID 72)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71) in 13 ms on localhost (executor driver) (69/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000069_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 70.0 in stage 3.0 (TID 73, localhost, executor driver, partition 70, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72) in 16 ms on localhost (executor driver) (70/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 70.0 in stage 3.0 (TID 73)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000070_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 71.0 in stage 3.0 (TID 74, localhost, executor driver, partition 71, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73) in 16 ms on localhost (executor driver) (71/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 71.0 in stage 3.0 (TID 74)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000071_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 72.0 in stage 3.0 (TID 75, localhost, executor driver, partition 72, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 72.0 in stage 3.0 (TID 75)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74) in 17 ms on localhost (executor driver) (72/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000072_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 73.0 in stage 3.0 (TID 76, localhost, executor driver, partition 73, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75) in 16 ms on localhost (executor driver) (73/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 73.0 in stage 3.0 (TID 76)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000073_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 74.0 in stage 3.0 (TID 77, localhost, executor driver, partition 74, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 74.0 in stage 3.0 (TID 77)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76) in 19 ms on localhost (executor driver) (74/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000074_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 75.0 in stage 3.0 (TID 78, localhost, executor driver, partition 75, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77) in 27 ms on localhost (executor driver) (75/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 75.0 in stage 3.0 (TID 78)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000075_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 76.0 in stage 3.0 (TID 79, localhost, executor driver, partition 76, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 76.0 in stage 3.0 (TID 79)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78) in 19 ms on localhost (executor driver) (76/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000076_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 77.0 in stage 3.0 (TID 80, localhost, executor driver, partition 77, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79) in 15 ms on localhost (executor driver) (77/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 77.0 in stage 3.0 (TID 80)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000077_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 78.0 in stage 3.0 (TID 81, localhost, executor driver, partition 78, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80) in 33 ms on localhost (executor driver) (78/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 78.0 in stage 3.0 (TID 81)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000078_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 79.0 in stage 3.0 (TID 82, localhost, executor driver, partition 79, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81) in 34 ms on localhost (executor driver) (79/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 79.0 in stage 3.0 (TID 82)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000079_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 80.0 in stage 3.0 (TID 83, localhost, executor driver, partition 80, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82) in 23 ms on localhost (executor driver) (80/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 80.0 in stage 3.0 (TID 83)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000080_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 81.0 in stage 3.0 (TID 84, localhost, executor driver, partition 81, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 81.0 in stage 3.0 (TID 84)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83) in 15 ms on localhost (executor driver) (81/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000081_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 82.0 in stage 3.0 (TID 85, localhost, executor driver, partition 82, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 82.0 in stage 3.0 (TID 85)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84) in 14 ms on localhost (executor driver) (82/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000082_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 83.0 in stage 3.0 (TID 86, localhost, executor driver, partition 83, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 83.0 in stage 3.0 (TID 86)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85) in 15 ms on localhost (executor driver) (83/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000083_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 84.0 in stage 3.0 (TID 87, localhost, executor driver, partition 84, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86) in 52 ms on localhost (executor driver) (84/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 84.0 in stage 3.0 (TID 87)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000084_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 85.0 in stage 3.0 (TID 88, localhost, executor driver, partition 85, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87) in 26 ms on localhost (executor driver) (85/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 85.0 in stage 3.0 (TID 88)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 5 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000085_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 86.0 in stage 3.0 (TID 89, localhost, executor driver, partition 86, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88) in 44 ms on localhost (executor driver) (86/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 86.0 in stage 3.0 (TID 89)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000086_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 87.0 in stage 3.0 (TID 90, localhost, executor driver, partition 87, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89) in 16 ms on localhost (executor driver) (87/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 87.0 in stage 3.0 (TID 90)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000087_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 88.0 in stage 3.0 (TID 91, localhost, executor driver, partition 88, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 88.0 in stage 3.0 (TID 91)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90) in 17 ms on localhost (executor driver) (88/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000088_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 89.0 in stage 3.0 (TID 92, localhost, executor driver, partition 89, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91) in 16 ms on localhost (executor driver) (89/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 89.0 in stage 3.0 (TID 92)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000089_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 90.0 in stage 3.0 (TID 93, localhost, executor driver, partition 90, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 90.0 in stage 3.0 (TID 93)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92) in 16 ms on localhost (executor driver) (90/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000090_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 91.0 in stage 3.0 (TID 94, localhost, executor driver, partition 91, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 91.0 in stage 3.0 (TID 94)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93) in 16 ms on localhost (executor driver) (91/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000091_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 92.0 in stage 3.0 (TID 95, localhost, executor driver, partition 92, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 92.0 in stage 3.0 (TID 95)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94) in 25 ms on localhost (executor driver) (92/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000092_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 93.0 in stage 3.0 (TID 96, localhost, executor driver, partition 93, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 93.0 in stage 3.0 (TID 96)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95) in 11 ms on localhost (executor driver) (93/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000093_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 94.0 in stage 3.0 (TID 97, localhost, executor driver, partition 94, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 94.0 in stage 3.0 (TID 97)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96) in 12 ms on localhost (executor driver) (94/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000094_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 95.0 in stage 3.0 (TID 98, localhost, executor driver, partition 95, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 95.0 in stage 3.0 (TID 98)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97) in 11 ms on localhost (executor driver) (95/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000095_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 96.0 in stage 3.0 (TID 99, localhost, executor driver, partition 96, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 96.0 in stage 3.0 (TID 99)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98) in 14 ms on localhost (executor driver) (96/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000096_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 97.0 in stage 3.0 (TID 100, localhost, executor driver, partition 97, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99) in 15 ms on localhost (executor driver) (97/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 97.0 in stage 3.0 (TID 100)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000097_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 98.0 in stage 3.0 (TID 101, localhost, executor driver, partition 98, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100) in 20 ms on localhost (executor driver) (98/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 98.0 in stage 3.0 (TID 101)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000098_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 99.0 in stage 3.0 (TID 102, localhost, executor driver, partition 99, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 99.0 in stage 3.0 (TID 102)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101) in 19 ms on localhost (executor driver) (99/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000099_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 100.0 in stage 3.0 (TID 103, localhost, executor driver, partition 100, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 100.0 in stage 3.0 (TID 103)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102) in 16 ms on localhost (executor driver) (100/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000100_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 101.0 in stage 3.0 (TID 104, localhost, executor driver, partition 101, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 101.0 in stage 3.0 (TID 104)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103) in 12 ms on localhost (executor driver) (101/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000101_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 102.0 in stage 3.0 (TID 105, localhost, executor driver, partition 102, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 102.0 in stage 3.0 (TID 105)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104) in 12 ms on localhost (executor driver) (102/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000102_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 103.0 in stage 3.0 (TID 106, localhost, executor driver, partition 103, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105) in 17 ms on localhost (executor driver) (103/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 103.0 in stage 3.0 (TID 106)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000103_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 104.0 in stage 3.0 (TID 107, localhost, executor driver, partition 104, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 104.0 in stage 3.0 (TID 107)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106) in 14 ms on localhost (executor driver) (104/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000104_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 105.0 in stage 3.0 (TID 108, localhost, executor driver, partition 105, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 105.0 in stage 3.0 (TID 108)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107) in 16 ms on localhost (executor driver) (105/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000105_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 106.0 in stage 3.0 (TID 109, localhost, executor driver, partition 106, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 106.0 in stage 3.0 (TID 109)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108) in 13 ms on localhost (executor driver) (106/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000106_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 107.0 in stage 3.0 (TID 110, localhost, executor driver, partition 107, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109) in 14 ms on localhost (executor driver) (107/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 107.0 in stage 3.0 (TID 110)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000107_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 108.0 in stage 3.0 (TID 111, localhost, executor driver, partition 108, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 108.0 in stage 3.0 (TID 111)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110) in 15 ms on localhost (executor driver) (108/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000108_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 109.0 in stage 3.0 (TID 112, localhost, executor driver, partition 109, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 109.0 in stage 3.0 (TID 112)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111) in 11 ms on localhost (executor driver) (109/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000109_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 110.0 in stage 3.0 (TID 113, localhost, executor driver, partition 110, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 110.0 in stage 3.0 (TID 113)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112) in 15 ms on localhost (executor driver) (110/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000110_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 111.0 in stage 3.0 (TID 114, localhost, executor driver, partition 111, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 111.0 in stage 3.0 (TID 114)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113) in 12 ms on localhost (executor driver) (111/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094429_0003_m_000111_0
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Starting task 112.0 in stage 3.0 (TID 115, localhost, executor driver, partition 112, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114) in 13 ms on localhost (executor driver) (112/200)
[INFO ] [2018-04-27 09:44:29] [Logging$class:logInfo:54] Running task 112.0 in stage 3.0 (TID 115)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000112_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 113.0 in stage 3.0 (TID 116, localhost, executor driver, partition 113, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 113.0 in stage 3.0 (TID 116)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115) in 16 ms on localhost (executor driver) (113/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000113_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 114.0 in stage 3.0 (TID 117, localhost, executor driver, partition 114, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 114.0 in stage 3.0 (TID 117)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116) in 129 ms on localhost (executor driver) (114/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000114_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 115.0 in stage 3.0 (TID 118, localhost, executor driver, partition 115, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 115.0 in stage 3.0 (TID 118)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117) in 20 ms on localhost (executor driver) (115/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000115_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 116.0 in stage 3.0 (TID 119, localhost, executor driver, partition 116, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 116.0 in stage 3.0 (TID 119)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118) in 18 ms on localhost (executor driver) (116/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000116_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119) in 18 ms on localhost (executor driver) (117/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 117.0 in stage 3.0 (TID 120, localhost, executor driver, partition 117, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 117.0 in stage 3.0 (TID 120)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000117_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 118.0 in stage 3.0 (TID 121, localhost, executor driver, partition 118, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 118.0 in stage 3.0 (TID 121)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120) in 21 ms on localhost (executor driver) (118/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000118_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 119.0 in stage 3.0 (TID 122, localhost, executor driver, partition 119, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 119.0 in stage 3.0 (TID 122)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121) in 19 ms on localhost (executor driver) (119/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000119_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 120.0 in stage 3.0 (TID 123, localhost, executor driver, partition 120, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 120.0 in stage 3.0 (TID 123)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122) in 13 ms on localhost (executor driver) (120/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000120_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 121.0 in stage 3.0 (TID 124, localhost, executor driver, partition 121, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 121.0 in stage 3.0 (TID 124)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123) in 13 ms on localhost (executor driver) (121/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000121_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 122.0 in stage 3.0 (TID 125, localhost, executor driver, partition 122, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124) in 17 ms on localhost (executor driver) (122/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 122.0 in stage 3.0 (TID 125)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000122_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 123.0 in stage 3.0 (TID 126, localhost, executor driver, partition 123, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125) in 54 ms on localhost (executor driver) (123/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 123.0 in stage 3.0 (TID 126)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000123_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 124.0 in stage 3.0 (TID 127, localhost, executor driver, partition 124, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126) in 19 ms on localhost (executor driver) (124/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 124.0 in stage 3.0 (TID 127)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000124_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 125.0 in stage 3.0 (TID 128, localhost, executor driver, partition 125, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 125.0 in stage 3.0 (TID 128)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127) in 13 ms on localhost (executor driver) (125/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000125_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 126.0 in stage 3.0 (TID 129, localhost, executor driver, partition 126, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128) in 14 ms on localhost (executor driver) (126/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 126.0 in stage 3.0 (TID 129)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000126_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 127.0 in stage 3.0 (TID 130, localhost, executor driver, partition 127, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 127.0 in stage 3.0 (TID 130)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129) in 15 ms on localhost (executor driver) (127/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000127_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 128.0 in stage 3.0 (TID 131, localhost, executor driver, partition 128, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 128.0 in stage 3.0 (TID 131)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130) in 15 ms on localhost (executor driver) (128/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000128_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 129.0 in stage 3.0 (TID 132, localhost, executor driver, partition 129, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 129.0 in stage 3.0 (TID 132)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131) in 15 ms on localhost (executor driver) (129/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000129_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 130.0 in stage 3.0 (TID 133, localhost, executor driver, partition 130, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 130.0 in stage 3.0 (TID 133)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132) in 15 ms on localhost (executor driver) (130/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000130_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 131.0 in stage 3.0 (TID 134, localhost, executor driver, partition 131, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133) in 14 ms on localhost (executor driver) (131/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 131.0 in stage 3.0 (TID 134)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000131_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 132.0 in stage 3.0 (TID 135, localhost, executor driver, partition 132, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 132.0 in stage 3.0 (TID 135)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134) in 14 ms on localhost (executor driver) (132/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000132_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 133.0 in stage 3.0 (TID 136, localhost, executor driver, partition 133, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 133.0 in stage 3.0 (TID 136)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135) in 14 ms on localhost (executor driver) (133/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000133_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 134.0 in stage 3.0 (TID 137, localhost, executor driver, partition 134, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136) in 12 ms on localhost (executor driver) (134/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 134.0 in stage 3.0 (TID 137)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000134_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 135.0 in stage 3.0 (TID 138, localhost, executor driver, partition 135, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 135.0 in stage 3.0 (TID 138)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137) in 12 ms on localhost (executor driver) (135/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000135_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 136.0 in stage 3.0 (TID 139, localhost, executor driver, partition 136, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 136.0 in stage 3.0 (TID 139)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138) in 11 ms on localhost (executor driver) (136/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000136_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 137.0 in stage 3.0 (TID 140, localhost, executor driver, partition 137, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 137.0 in stage 3.0 (TID 140)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139) in 11 ms on localhost (executor driver) (137/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000137_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 138.0 in stage 3.0 (TID 141, localhost, executor driver, partition 138, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 138.0 in stage 3.0 (TID 141)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140) in 11 ms on localhost (executor driver) (138/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000138_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 139.0 in stage 3.0 (TID 142, localhost, executor driver, partition 139, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 139.0 in stage 3.0 (TID 142)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141) in 11 ms on localhost (executor driver) (139/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000139_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 140.0 in stage 3.0 (TID 143, localhost, executor driver, partition 140, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 140.0 in stage 3.0 (TID 143)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142) in 10 ms on localhost (executor driver) (140/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000140_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 141.0 in stage 3.0 (TID 144, localhost, executor driver, partition 141, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 141.0 in stage 3.0 (TID 144)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143) in 11 ms on localhost (executor driver) (141/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000141_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 142.0 in stage 3.0 (TID 145, localhost, executor driver, partition 142, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 142.0 in stage 3.0 (TID 145)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144) in 14 ms on localhost (executor driver) (142/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000142_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 143.0 in stage 3.0 (TID 146, localhost, executor driver, partition 143, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145) in 14 ms on localhost (executor driver) (143/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 143.0 in stage 3.0 (TID 146)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000143_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 144.0 in stage 3.0 (TID 147, localhost, executor driver, partition 144, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 144.0 in stage 3.0 (TID 147)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146) in 16 ms on localhost (executor driver) (144/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000144_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 145.0 in stage 3.0 (TID 148, localhost, executor driver, partition 145, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 145.0 in stage 3.0 (TID 148)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147) in 12 ms on localhost (executor driver) (145/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000145_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 146.0 in stage 3.0 (TID 149, localhost, executor driver, partition 146, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 146.0 in stage 3.0 (TID 149)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148) in 14 ms on localhost (executor driver) (146/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000146_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 147.0 in stage 3.0 (TID 150, localhost, executor driver, partition 147, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 147.0 in stage 3.0 (TID 150)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149) in 14 ms on localhost (executor driver) (147/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000147_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 148.0 in stage 3.0 (TID 151, localhost, executor driver, partition 148, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 148.0 in stage 3.0 (TID 151)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150) in 12 ms on localhost (executor driver) (148/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000148_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 149.0 in stage 3.0 (TID 152, localhost, executor driver, partition 149, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 149.0 in stage 3.0 (TID 152)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151) in 13 ms on localhost (executor driver) (149/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000149_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 150.0 in stage 3.0 (TID 153, localhost, executor driver, partition 150, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 150.0 in stage 3.0 (TID 153)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152) in 18 ms on localhost (executor driver) (150/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000150_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 151.0 in stage 3.0 (TID 154, localhost, executor driver, partition 151, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 151.0 in stage 3.0 (TID 154)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153) in 15 ms on localhost (executor driver) (151/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000151_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 152.0 in stage 3.0 (TID 155, localhost, executor driver, partition 152, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 152.0 in stage 3.0 (TID 155)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154) in 13 ms on localhost (executor driver) (152/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000152_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 153.0 in stage 3.0 (TID 156, localhost, executor driver, partition 153, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155) in 16 ms on localhost (executor driver) (153/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 153.0 in stage 3.0 (TID 156)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000153_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 154.0 in stage 3.0 (TID 157, localhost, executor driver, partition 154, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156) in 17 ms on localhost (executor driver) (154/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 154.0 in stage 3.0 (TID 157)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000154_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 155.0 in stage 3.0 (TID 158, localhost, executor driver, partition 155, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 155.0 in stage 3.0 (TID 158)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157) in 14 ms on localhost (executor driver) (155/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000155_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 156.0 in stage 3.0 (TID 159, localhost, executor driver, partition 156, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 156.0 in stage 3.0 (TID 159)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158) in 12 ms on localhost (executor driver) (156/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000156_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 157.0 in stage 3.0 (TID 160, localhost, executor driver, partition 157, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 157.0 in stage 3.0 (TID 160)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159) in 14 ms on localhost (executor driver) (157/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000157_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 158.0 in stage 3.0 (TID 161, localhost, executor driver, partition 158, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 158.0 in stage 3.0 (TID 161)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160) in 13 ms on localhost (executor driver) (158/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000158_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 159.0 in stage 3.0 (TID 162, localhost, executor driver, partition 159, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 159.0 in stage 3.0 (TID 162)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161) in 14 ms on localhost (executor driver) (159/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000159_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 160.0 in stage 3.0 (TID 163, localhost, executor driver, partition 160, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 160.0 in stage 3.0 (TID 163)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162) in 11 ms on localhost (executor driver) (160/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000160_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 161.0 in stage 3.0 (TID 164, localhost, executor driver, partition 161, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163) in 11 ms on localhost (executor driver) (161/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 161.0 in stage 3.0 (TID 164)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 5 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000161_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 162.0 in stage 3.0 (TID 165, localhost, executor driver, partition 162, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164) in 28 ms on localhost (executor driver) (162/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 162.0 in stage 3.0 (TID 165)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000162_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 163.0 in stage 3.0 (TID 166, localhost, executor driver, partition 163, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 163.0 in stage 3.0 (TID 166)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165) in 19 ms on localhost (executor driver) (163/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000163_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 164.0 in stage 3.0 (TID 167, localhost, executor driver, partition 164, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 164.0 in stage 3.0 (TID 167)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166) in 13 ms on localhost (executor driver) (164/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000164_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 165.0 in stage 3.0 (TID 168, localhost, executor driver, partition 165, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 165.0 in stage 3.0 (TID 168)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167) in 14 ms on localhost (executor driver) (165/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000165_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 166.0 in stage 3.0 (TID 169, localhost, executor driver, partition 166, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 166.0 in stage 3.0 (TID 169)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168) in 10 ms on localhost (executor driver) (166/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000166_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 167.0 in stage 3.0 (TID 170, localhost, executor driver, partition 167, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 167.0 in stage 3.0 (TID 170)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169) in 15 ms on localhost (executor driver) (167/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000167_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 168.0 in stage 3.0 (TID 171, localhost, executor driver, partition 168, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 168.0 in stage 3.0 (TID 171)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170) in 14 ms on localhost (executor driver) (168/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000168_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 169.0 in stage 3.0 (TID 172, localhost, executor driver, partition 169, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171) in 16 ms on localhost (executor driver) (169/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 169.0 in stage 3.0 (TID 172)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000169_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 170.0 in stage 3.0 (TID 173, localhost, executor driver, partition 170, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172) in 18 ms on localhost (executor driver) (170/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 170.0 in stage 3.0 (TID 173)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094430_0003_m_000170_0
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Starting task 171.0 in stage 3.0 (TID 174, localhost, executor driver, partition 171, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173) in 35 ms on localhost (executor driver) (171/200)
[INFO ] [2018-04-27 09:44:30] [Logging$class:logInfo:54] Running task 171.0 in stage 3.0 (TID 174)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000171_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 172.0 in stage 3.0 (TID 175, localhost, executor driver, partition 172, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174) in 24 ms on localhost (executor driver) (172/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 172.0 in stage 3.0 (TID 175)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000172_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 173.0 in stage 3.0 (TID 176, localhost, executor driver, partition 173, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175) in 54 ms on localhost (executor driver) (173/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 173.0 in stage 3.0 (TID 176)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000173_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 174.0 in stage 3.0 (TID 177, localhost, executor driver, partition 174, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 174.0 in stage 3.0 (TID 177)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176) in 22 ms on localhost (executor driver) (174/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000174_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 175.0 in stage 3.0 (TID 178, localhost, executor driver, partition 175, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 175.0 in stage 3.0 (TID 178)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177) in 14 ms on localhost (executor driver) (175/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000175_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 176.0 in stage 3.0 (TID 179, localhost, executor driver, partition 176, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 176.0 in stage 3.0 (TID 179)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178) in 12 ms on localhost (executor driver) (176/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000176_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 177.0 in stage 3.0 (TID 180, localhost, executor driver, partition 177, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 177.0 in stage 3.0 (TID 180)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179) in 11 ms on localhost (executor driver) (177/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000177_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 178.0 in stage 3.0 (TID 181, localhost, executor driver, partition 178, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180) in 16 ms on localhost (executor driver) (178/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 178.0 in stage 3.0 (TID 181)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000178_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181). 3603 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 179.0 in stage 3.0 (TID 182, localhost, executor driver, partition 179, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181) in 28 ms on localhost (executor driver) (179/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 179.0 in stage 3.0 (TID 182)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000179_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 180.0 in stage 3.0 (TID 183, localhost, executor driver, partition 180, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182) in 20 ms on localhost (executor driver) (180/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 180.0 in stage 3.0 (TID 183)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000180_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 181.0 in stage 3.0 (TID 184, localhost, executor driver, partition 181, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 181.0 in stage 3.0 (TID 184)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183) in 16 ms on localhost (executor driver) (181/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000181_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 182.0 in stage 3.0 (TID 185, localhost, executor driver, partition 182, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 182.0 in stage 3.0 (TID 185)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184) in 34 ms on localhost (executor driver) (182/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000182_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 183.0 in stage 3.0 (TID 186, localhost, executor driver, partition 183, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 183.0 in stage 3.0 (TID 186)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185) in 18 ms on localhost (executor driver) (183/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000183_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 184.0 in stage 3.0 (TID 187, localhost, executor driver, partition 184, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 184.0 in stage 3.0 (TID 187)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186) in 25 ms on localhost (executor driver) (184/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000184_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 185.0 in stage 3.0 (TID 188, localhost, executor driver, partition 185, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 185.0 in stage 3.0 (TID 188)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187) in 22 ms on localhost (executor driver) (185/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000185_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 186.0 in stage 3.0 (TID 189, localhost, executor driver, partition 186, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188) in 23 ms on localhost (executor driver) (186/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 186.0 in stage 3.0 (TID 189)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000186_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 187.0 in stage 3.0 (TID 190, localhost, executor driver, partition 187, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189) in 35 ms on localhost (executor driver) (187/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 187.0 in stage 3.0 (TID 190)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000187_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 188.0 in stage 3.0 (TID 191, localhost, executor driver, partition 188, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190) in 33 ms on localhost (executor driver) (188/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 188.0 in stage 3.0 (TID 191)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000188_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 189.0 in stage 3.0 (TID 192, localhost, executor driver, partition 189, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191) in 20 ms on localhost (executor driver) (189/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 189.0 in stage 3.0 (TID 192)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000189_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 190.0 in stage 3.0 (TID 193, localhost, executor driver, partition 190, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 190.0 in stage 3.0 (TID 193)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192) in 15 ms on localhost (executor driver) (190/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000190_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 191.0 in stage 3.0 (TID 194, localhost, executor driver, partition 191, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193) in 19 ms on localhost (executor driver) (191/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 191.0 in stage 3.0 (TID 194)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000191_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 192.0 in stage 3.0 (TID 195, localhost, executor driver, partition 192, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 192.0 in stage 3.0 (TID 195)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194) in 19 ms on localhost (executor driver) (192/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000192_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 193.0 in stage 3.0 (TID 196, localhost, executor driver, partition 193, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195) in 15 ms on localhost (executor driver) (193/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 193.0 in stage 3.0 (TID 196)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000193_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 194.0 in stage 3.0 (TID 197, localhost, executor driver, partition 194, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196) in 19 ms on localhost (executor driver) (194/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 194.0 in stage 3.0 (TID 197)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000194_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 195.0 in stage 3.0 (TID 198, localhost, executor driver, partition 195, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 195.0 in stage 3.0 (TID 198)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197) in 15 ms on localhost (executor driver) (195/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000195_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 196.0 in stage 3.0 (TID 199, localhost, executor driver, partition 196, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 196.0 in stage 3.0 (TID 199)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198) in 16 ms on localhost (executor driver) (196/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000196_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 197.0 in stage 3.0 (TID 200, localhost, executor driver, partition 197, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 197.0 in stage 3.0 (TID 200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199) in 14 ms on localhost (executor driver) (197/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000197_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200). 3689 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 198.0 in stage 3.0 (TID 201, localhost, executor driver, partition 198, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 198.0 in stage 3.0 (TID 201)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200) in 51 ms on localhost (executor driver) (198/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000198_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Starting task 199.0 in stage 3.0 (TID 202, localhost, executor driver, partition 199, PROCESS_LOCAL, 5054 bytes)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Running task 199.0 in stage 3.0 (TID 202)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201) in 13 ms on localhost (executor driver) (199/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Getting 0 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094431_0003_m_000199_0
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202). 3646 bytes result sent to driver
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202) in 14 ms on localhost (executor driver) (200/200)
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] ResultStage 3 (saveAsTable at HistoryDataToA.scala:101) finished in 6.030 s
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:101, took 6.728619 s
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:31] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Final stage: ResultStage 4 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Submitting ResultStage 4 (MapPartitionsRDD[44] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Block broadcast_9 stored as values in memory (estimated size 80.9 KB, free 1926.1 MB)
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1926.1 MB)
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Added broadcast_9_piece0 in memory on 192.168.0.152:54028 (size: 30.9 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 4 (MapPartitionsRDD[44] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Adding task set 4.0 with 15 tasks
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Starting task 0.0 in stage 4.0 (TID 203, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Running task 0.0 in stage 4.0 (TID 203)
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Code generated in 20.814926 ms
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Code generated in 10.803735 ms
[INFO ] [2018-04-27 09:44:33] [Logging$class:logInfo:54] Code generated in 13.570651 ms
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Code generated in 5.082622 ms
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Code generated in 12.478686 ms
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Code generated in 12.806803 ms
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094434_0004_m_000000_0
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Finished task 0.0 in stage 4.0 (TID 203). 1796 bytes result sent to driver
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Starting task 1.0 in stage 4.0 (TID 204, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Running task 1.0 in stage 4.0 (TID 204)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Finished task 0.0 in stage 4.0 (TID 203) in 168 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094434_0004_m_000001_0
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Finished task 1.0 in stage 4.0 (TID 204). 1796 bytes result sent to driver
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Starting task 2.0 in stage 4.0 (TID 205, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Running task 2.0 in stage 4.0 (TID 205)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Finished task 1.0 in stage 4.0 (TID 204) in 652 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094434_0004_m_000002_0
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Finished task 2.0 in stage 4.0 (TID 205). 1753 bytes result sent to driver
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Starting task 3.0 in stage 4.0 (TID 206, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Running task 3.0 in stage 4.0 (TID 206)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Finished task 2.0 in stage 4.0 (TID 205) in 155 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 09:44:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094435_0004_m_000003_0
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 3.0 in stage 4.0 (TID 206). 1796 bytes result sent to driver
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Starting task 4.0 in stage 4.0 (TID 207, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Running task 4.0 in stage 4.0 (TID 207)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 3.0 in stage 4.0 (TID 206) in 578 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094435_0004_m_000004_0
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 4.0 in stage 4.0 (TID 207). 1796 bytes result sent to driver
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Starting task 5.0 in stage 4.0 (TID 208, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Running task 5.0 in stage 4.0 (TID 208)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 4.0 in stage 4.0 (TID 207) in 137 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094435_0004_m_000005_0
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 5.0 in stage 4.0 (TID 208). 1753 bytes result sent to driver
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Starting task 6.0 in stage 4.0 (TID 209, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Running task 6.0 in stage 4.0 (TID 209)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 5.0 in stage 4.0 (TID 208) in 218 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094435_0004_m_000006_0
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 6.0 in stage 4.0 (TID 209). 1710 bytes result sent to driver
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Starting task 7.0 in stage 4.0 (TID 210, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Running task 7.0 in stage 4.0 (TID 210)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Finished task 6.0 in stage 4.0 (TID 209) in 61 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 09:44:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094436_0004_m_000007_0
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Finished task 7.0 in stage 4.0 (TID 210). 1796 bytes result sent to driver
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Starting task 8.0 in stage 4.0 (TID 211, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Finished task 7.0 in stage 4.0 (TID 210) in 210 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Running task 8.0 in stage 4.0 (TID 211)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094436_0004_m_000008_0
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Finished task 8.0 in stage 4.0 (TID 211). 1753 bytes result sent to driver
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Starting task 9.0 in stage 4.0 (TID 212, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Running task 9.0 in stage 4.0 (TID 212)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Finished task 8.0 in stage 4.0 (TID 211) in 254 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094436_0004_m_000009_0
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Finished task 9.0 in stage 4.0 (TID 212). 1796 bytes result sent to driver
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Starting task 10.0 in stage 4.0 (TID 213, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Running task 10.0 in stage 4.0 (TID 213)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Finished task 9.0 in stage 4.0 (TID 212) in 630 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 09:44:36] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094437_0004_m_000010_0
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Finished task 10.0 in stage 4.0 (TID 213). 1796 bytes result sent to driver
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Starting task 11.0 in stage 4.0 (TID 214, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Running task 11.0 in stage 4.0 (TID 214)
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Finished task 10.0 in stage 4.0 (TID 213) in 836 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094437_0004_m_000011_0
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Finished task 11.0 in stage 4.0 (TID 214). 1753 bytes result sent to driver
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Starting task 12.0 in stage 4.0 (TID 215, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Running task 12.0 in stage 4.0 (TID 215)
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Finished task 11.0 in stage 4.0 (TID 214) in 83 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 09:44:37] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094438_0004_m_000012_0
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Finished task 12.0 in stage 4.0 (TID 215). 1753 bytes result sent to driver
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Starting task 13.0 in stage 4.0 (TID 216, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Running task 13.0 in stage 4.0 (TID 216)
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Finished task 12.0 in stage 4.0 (TID 215) in 176 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094438_0004_m_000013_0
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Finished task 13.0 in stage 4.0 (TID 216). 1839 bytes result sent to driver
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Starting task 14.0 in stage 4.0 (TID 217, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Running task 14.0 in stage 4.0 (TID 217)
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Finished task 13.0 in stage 4.0 (TID 216) in 306 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427094438_0004_m_000014_0
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Finished task 14.0 in stage 4.0 (TID 217). 1753 bytes result sent to driver
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Finished task 14.0 in stage 4.0 (TID 217) in 121 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] ResultStage 4 (saveAsTable at HistoryDataToA.scala:114) finished in 4.579 s
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Job 2 finished: saveAsTable at HistoryDataToA.scala:114, took 4.614386 s
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:38] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 09:44:39] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-2a405ee7-55e5-4171-9744-9583a8c0d5f8
[INFO ] [2018-04-27 10:10:51] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 54527.
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-0a15112a-7ae1-41f4-8141-7e79d8366361
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 10:10:52] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54536.
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Server created on 192.168.0.152:54536
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 54536, None)
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:54536 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 54536, None)
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 54536, None)
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 54536, None)
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:54536 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 10:10:53] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 10:10:54] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[ERROR] [2018-04-27 10:10:57] [ObjectStore:checkSchema:6684] Version information found in metastore differs 2.1.0 from expected schema version 1.2.0. Schema verififcation is disabled hive.metastore.schema.verification so setting version.
[INFO ] [2018-04-27 10:10:58] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:10:59] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 10:11:00] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 10:11:00] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 10:11:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 10:11:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Got job 0 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Final stage: ResultStage 0 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:54536 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 10:11:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Code generated in 248.504777 ms
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Code generated in 16.010582 ms
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Code generated in 9.984383 ms
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Code generated in 12.673517 ms
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Code generated in 18.453155 ms
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Code generated in 14.502521 ms
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101107_0000_m_000000_0
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1882 bytes result sent to driver
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 849 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 10:11:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101108_0000_m_000001_0
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1882 bytes result sent to driver
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 727 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101108_0000_m_000002_0
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1796 bytes result sent to driver
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 112 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101108_0000_m_000003_0
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1796 bytes result sent to driver
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 535 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101108_0000_m_000004_0
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1796 bytes result sent to driver
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 118 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 10:11:08] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101109_0000_m_000005_0
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1839 bytes result sent to driver
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 181 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101109_0000_m_000006_0
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1796 bytes result sent to driver
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 35 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101109_0000_m_000007_0
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1796 bytes result sent to driver
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 256 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101109_0000_m_000008_0
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1753 bytes result sent to driver
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 294 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 10:11:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101110_0000_m_000009_0
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1839 bytes result sent to driver
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 682 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 10:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101111_0000_m_000010_0
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1839 bytes result sent to driver
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 752 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101111_0000_m_000011_0
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1753 bytes result sent to driver
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 48 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101111_0000_m_000012_0
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1796 bytes result sent to driver
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 182 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101111_0000_m_000013_0
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1839 bytes result sent to driver
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 198 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427101111_0000_m_000014_0
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1796 bytes result sent to driver
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 130 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] ResultStage 0 (saveAsTable at HistoryDataToA.scala:114) finished in 5.075 s
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Job 0 finished: saveAsTable at HistoryDataToA.scala:114, took 5.204302 s
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 10:11:11] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 10:11:12] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-b08f8749-58b7-4fd9-9865-ecb6ec501538
[INFO ] [2018-04-27 11:14:05] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:14:06] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:14:06] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:14:06] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:14:06] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:14:06] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:14:06] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56238.
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-f07a0b23-7d37-493a-91d7-3859f7fb38b3
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56247.
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Server created on 192.168.0.152:56247
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56247, None)
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56247 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56247, None)
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56247, None)
[INFO ] [2018-04-27 11:14:07] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56247, None)
[INFO ] [2018-04-27 11:14:08] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:14:08] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:14:08] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:56247 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:14:08] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:14:09] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:14:09] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:14:09] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:14:09] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:14:14] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:14:14] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:14:14] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:14:14] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:15] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:14:17] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Got job 0 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Final stage: ResultStage 0 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:56247 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:14:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Code generated in 295.722834 ms
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Code generated in 19.763363 ms
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Code generated in 16.205036 ms
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Code generated in 18.828471 ms
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Code generated in 27.532476 ms
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Code generated in 18.328931 ms
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111424_0000_m_000000_0
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 1061 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:14:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111425_0000_m_000001_0
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 938 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111425_0000_m_000002_0
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 135 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:14:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111426_0000_m_000003_0
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 655 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111426_0000_m_000004_0
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 128 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111426_0000_m_000005_0
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 199 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111426_0000_m_000006_0
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 37 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:14:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111427_0000_m_000007_0
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 327 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111427_0000_m_000008_0
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 363 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:14:27] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111428_0000_m_000009_0
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 842 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:14:28] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111429_0000_m_000010_0
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 946 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111429_0000_m_000011_0
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 60 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111429_0000_m_000012_0
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 240 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111429_0000_m_000013_0
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 223 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427111429_0000_m_000014_0
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 138 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] ResultStage 0 (saveAsTable at HistoryDataToA.scala:114) finished in 6.236 s
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Job 0 finished: saveAsTable at HistoryDataToA.scala:114, took 6.417583 s
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:14:29] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:14:30] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-24742488-558c-46e2-8330-11c62050f095
[INFO ] [2018-04-27 11:20:48] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56370.
[INFO ] [2018-04-27 11:20:49] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-d82c9229-803a-4d0e-bd03-76dfcabe409c
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56379.
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Server created on 192.168.0.152:56379
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56379, None)
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56379 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56379, None)
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56379, None)
[INFO ] [2018-04-27 11:20:50] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56379, None)
[INFO ] [2018-04-27 11:20:51] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:20:51] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:20:51] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:56379 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:20:51] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:20:51] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:20:51] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:20:51] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:20:52] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:20:56] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:20:56] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:20:56] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:20:56] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:57] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:20:58] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Got job 0 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Final stage: ResultStage 0 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:56379 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:21:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Code generated in 277.827745 ms
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Code generated in 13.879135 ms
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Code generated in 11.678591 ms
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Code generated in 11.211145 ms
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Code generated in 18.424082 ms
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Code generated in 11.54908 ms
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112105_0000_m_000000_0
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1882 bytes result sent to driver
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 852 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:21:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112106_0000_m_000001_0
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 759 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112106_0000_m_000002_0
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 128 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112106_0000_m_000003_0
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 562 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112106_0000_m_000004_0
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 107 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:21:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112107_0000_m_000005_0
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 193 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112107_0000_m_000006_0
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1710 bytes result sent to driver
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 34 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112107_0000_m_000007_0
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 311 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112107_0000_m_000008_0
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 210 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:21:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112108_0000_m_000009_0
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 639 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:21:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112109_0000_m_000010_0
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 732 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112109_0000_m_000011_0
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 53 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112109_0000_m_000012_0
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 160 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112109_0000_m_000013_0
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 196 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112109_0000_m_000014_0
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 149 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] ResultStage 0 (saveAsTable at HistoryDataToA.scala:114) finished in 5.047 s
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Job 0 finished: saveAsTable at HistoryDataToA.scala:114, took 5.187133 s
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:21:09] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:21:10] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-028f0f41-6774-492b-8bb6-ba26ce9ad206
[INFO ] [2018-04-27 11:24:07] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56449.
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-e3b717c0-f080-4bc6-9187-bc67090838c0
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:24:08] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56458.
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Server created on 192.168.0.152:56458
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56458, None)
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56458 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56458, None)
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56458, None)
[INFO ] [2018-04-27 11:24:09] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56458, None)
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:56458 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:24:10] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:24:14] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:15] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:24:16] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:24:17] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:24:22] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Got job 0 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Final stage: ResultStage 0 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:56458 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Code generated in 241.201978 ms
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Code generated in 16.00605 ms
[INFO ] [2018-04-27 11:24:23] [Logging$class:logInfo:54] Code generated in 13.299925 ms
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Code generated in 20.072601 ms
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Code generated in 29.982224 ms
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Code generated in 15.462711 ms
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112424_0000_m_000000_0
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 927 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:24:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112426_0000_m_000001_0
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 2088 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112426_0000_m_000002_0
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:24:26] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 362 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112427_0000_m_000003_0
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 1275 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:24:27] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112428_0000_m_000004_0
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 308 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112428_0000_m_000005_0
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 364 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112428_0000_m_000006_0
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 46 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:24:28] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112429_0000_m_000007_0
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 726 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112429_0000_m_000008_0
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 538 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:24:29] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112431_0000_m_000009_0
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 1866 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:24:31] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112433_0000_m_000010_0
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 2209 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:24:33] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112434_0000_m_000011_0
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 106 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112434_0000_m_000012_0
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 451 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112434_0000_m_000013_0
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:24:34] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 455 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112435_0000_m_000014_0
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 339 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] ResultStage 0 (saveAsTable at HistoryDataToA.scala:114) finished in 11.931 s
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Job 0 finished: saveAsTable at HistoryDataToA.scala:114, took 12.087122 s
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:24:35] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:24:36] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-24fe4584-7440-40bf-81aa-f560e4d88aaf
[INFO ] [2018-04-27 11:26:15] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56513.
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:26:16] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-f7db8d6e-8b1a-4b94-aa12-178c7f253de5
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56522.
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Server created on 192.168.0.152:56522
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56522, None)
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56522 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56522, None)
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56522, None)
[INFO ] [2018-04-27 11:26:17] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56522, None)
[INFO ] [2018-04-27 11:26:18] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:26:18] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:26:18] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:56522 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:26:18] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:26:18] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:26:18] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:26:18] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:26:19] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:23] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:26:25] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:26:30] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:30] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Got job 0 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Final stage: ResultStage 0 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:56522 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Code generated in 264.720009 ms
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Code generated in 15.335088 ms
[INFO ] [2018-04-27 11:26:31] [Logging$class:logInfo:54] Code generated in 12.021812 ms
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Code generated in 11.922886 ms
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Code generated in 21.692803 ms
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Code generated in 14.419454 ms
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112632_0000_m_000000_0
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1882 bytes result sent to driver
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 990 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:26:32] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112635_0000_m_000001_0
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 2965 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112635_0000_m_000002_0
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:26:35] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 378 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112637_0000_m_000003_0
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 2030 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112637_0000_m_000004_0
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 385 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:26:37] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112638_0000_m_000005_0
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 637 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112638_0000_m_000006_0
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 151 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:26:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112639_0000_m_000007_0
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:26:39] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 916 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112640_0000_m_000008_0
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:26:40] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 887 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112642_0000_m_000009_0
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 2419 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:26:42] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112645_0000_m_000010_0
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 2789 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112645_0000_m_000011_0
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 127 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:26:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112646_0000_m_000012_0
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 652 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112646_0000_m_000013_0
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:26:46] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 702 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112647_0000_m_000014_0
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 351 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] ResultStage 0 (saveAsTable at HistoryDataToA.scala:114) finished in 16.154 s
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Job 0 finished: saveAsTable at HistoryDataToA.scala:114, took 16.309224 s
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:26:47] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:26:48] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-479d36f8-095f-45f9-9262-561cb754ae7d
[INFO ] [2018-04-27 11:27:46] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:27:46] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:27:46] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:27:46] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:27:46] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:27:46] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:27:46] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 56578.
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-e0b0fe57-bfae-4cba-a6cb-6cc6c59640b9
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56587.
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Server created on 192.168.0.152:56587
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 56587, None)
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:56587 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 56587, None)
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 56587, None)
[INFO ] [2018-04-27 11:27:47] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 56587, None)
[INFO ] [2018-04-27 11:27:48] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:27:49] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:27:49] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:56587 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:27:49] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:27:49] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:27:49] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:27:49] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:27:49] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:27:53] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:54] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:27:56] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:28:01] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:114
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Got job 0 (saveAsTable at HistoryDataToA.scala:114) with 15 output partitions
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Final stage: ResultStage 0 (saveAsTable at HistoryDataToA.scala:114)
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114), which has no missing parents
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:56587 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:114) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Code generated in 236.549042 ms
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Code generated in 16.497661 ms
[INFO ] [2018-04-27 11:28:02] [Logging$class:logInfo:54] Code generated in 11.454308 ms
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Code generated in 12.44357 ms
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Code generated in 23.438361 ms
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Code generated in 12.157741 ms
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112803_0000_m_000000_0
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1882 bytes result sent to driver
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 840 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:28:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112804_0000_m_000001_0
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 1629 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112804_0000_m_000002_0
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 230 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:28:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:28:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112805_0000_m_000003_0
[INFO ] [2018-04-27 11:28:05] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:05] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:28:05] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 1050 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112806_0000_m_000004_0
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 255 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112806_0000_m_000005_0
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 238 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112806_0000_m_000006_0
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1710 bytes result sent to driver
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:28:06] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 82 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112807_0000_m_000007_0
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 613 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112807_0000_m_000008_0
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 532 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:28:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112809_0000_m_000009_0
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 1357 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:28:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112810_0000_m_000010_0
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 1614 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112810_0000_m_000011_0
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 93 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:28:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112811_0000_m_000012_0
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 338 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112811_0000_m_000013_0
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 372 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427112811_0000_m_000014_0
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 288 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] ResultStage 0 (saveAsTable at HistoryDataToA.scala:114) finished in 9.425 s
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Job 0 finished: saveAsTable at HistoryDataToA.scala:114, took 9.555581 s
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:28:11] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:28:12] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-f46d7908-2524-48cd-822c-6defd645fbbe
[INFO ] [2018-04-27 11:45:37] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 57304.
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-19bb3b36-1ca9-4814-8d0c-37275e489ea6
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:45:38] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57313.
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Server created on 192.168.0.152:57313
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 57313, None)
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:57313 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 57313, None)
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 57313, None)
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 57313, None)
[INFO ] [2018-04-27 11:45:39] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:45:40] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:45:40] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:57313 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:45:40] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Starting job: collect at LoadDataUtil.scala:40
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Got job 0 (collect at LoadDataUtil.scala:40) with 15 output partitions
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at LoadDataUtil.scala:40)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Submitting ResultStage 0 (HadoopMapPartitionsWithSplitRDD[1] at mapPartitionsWithInputSplit at LoadDataUtil.scala:29), which has no missing parents
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 1962.0 B, free 1991.8 MB)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:57313 (size: 1962.0 B, free: 1992.0 MB)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (HadoopMapPartitionsWithSplitRDD[1] at mapPartitionsWithInputSplit at LoadDataUtil.scala:29) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 554407 bytes result sent to driver
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:45:45] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 548 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Block taskresult_1 stored as bytes in memory (estimated size 32.0 MB, free 1959.8 MB)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Added taskresult_1 in memory on 192.168.0.152:57313 (size: 32.0 MB, free: 1960.0 MB)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 33567694 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Block taskresult_2 stored as bytes in memory (estimated size 4.6 MB, free 1955.2 MB)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Added taskresult_2 in memory on 192.168.0.152:57313 (size: 4.6 MB, free: 1955.4 MB)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 4801230 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:45:47] [TransportClientFactory:createClient:254] Successfully created connection to /192.168.0.152:57313 after 64 ms (0 ms spent in bootstraps)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 863 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:45:47] [Logging$class:logInfo:54] Removed taskresult_2 on 192.168.0.152:57313 in memory (size: 4.6 MB, free: 1960.0 MB)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Block taskresult_3 stored as bytes in memory (estimated size 24.8 MB, free 1934.9 MB)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Added taskresult_3 in memory on 192.168.0.152:57313 (size: 24.8 MB, free: 1935.2 MB)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 26010445 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Block taskresult_4 stored as bytes in memory (estimated size 4.5 MB, free 1930.4 MB)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Added taskresult_4 in memory on 192.168.0.152:57313 (size: 4.5 MB, free: 1930.7 MB)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 4721932 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Block taskresult_5 stored as bytes in memory (estimated size 6.3 MB, free 1924.1 MB)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Added taskresult_5 in memory on 192.168.0.152:57313 (size: 6.3 MB, free: 1924.3 MB)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 6617379 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 18284 bytes result sent to driver
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:45:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 3161 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 486 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Removed taskresult_1 on 192.168.0.152:57313 in memory (size: 32.0 MB, free: 1956.4 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Removed taskresult_4 on 192.168.0.152:57313 in memory (size: 4.5 MB, free: 1960.9 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 1094 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 867 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Removed taskresult_5 on 192.168.0.152:57313 in memory (size: 6.3 MB, free: 1967.2 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Block taskresult_7 stored as bytes in memory (estimated size 10.5 MB, free 1956.4 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Added taskresult_7 in memory on 192.168.0.152:57313 (size: 10.5 MB, free: 1956.6 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 11059704 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 2239 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Removed taskresult_3 on 192.168.0.152:57313 in memory (size: 24.8 MB, free: 1981.4 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 994 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Removed taskresult_7 on 192.168.0.152:57313 in memory (size: 10.5 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Block taskresult_8 stored as bytes in memory (estimated size 10.2 MB, free 1981.6 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Added taskresult_8 in memory on 192.168.0.152:57313 (size: 10.2 MB, free: 1981.8 MB)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 10709879 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 676 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:45:49] [Logging$class:logInfo:54] Removed taskresult_8 on 192.168.0.152:57313 in memory (size: 10.2 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:45:50] [Logging$class:logInfo:54] Block taskresult_9 stored as bytes in memory (estimated size 31.5 MB, free 1960.3 MB)
[INFO ] [2018-04-27 11:45:50] [Logging$class:logInfo:54] Added taskresult_9 in memory on 192.168.0.152:57313 (size: 31.5 MB, free: 1960.5 MB)
[INFO ] [2018-04-27 11:45:50] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 33018054 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:50] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:45:50] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:45:50] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 1376 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Removed taskresult_9 on 192.168.0.152:57313 in memory (size: 31.5 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Block taskresult_10 stored as bytes in memory (estimated size 38.6 MB, free 1953.2 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Added taskresult_10 in memory on 192.168.0.152:57313 (size: 38.6 MB, free: 1953.4 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 40489767 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Block taskresult_11 stored as bytes in memory (estimated size 1344.4 KB, free 1951.8 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Added taskresult_11 in memory on 192.168.0.152:57313 (size: 1344.4 KB, free: 1952.1 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1376630 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Block taskresult_12 stored as bytes in memory (estimated size 6.8 MB, free 1945.0 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Added taskresult_12 in memory on 192.168.0.152:57313 (size: 6.8 MB, free: 1945.2 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 7137742 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 710 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Removed taskresult_11 on 192.168.0.152:57313 in memory (size: 1344.4 KB, free: 1946.6 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Block taskresult_13 stored as bytes in memory (estimated size 7.9 MB, free 1938.5 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Added taskresult_13 in memory on 192.168.0.152:57313 (size: 7.9 MB, free: 1938.7 MB)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 8251949 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:45:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 569 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Removed taskresult_12 on 192.168.0.152:57313 in memory (size: 6.8 MB, free: 1945.5 MB)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Block taskresult_14 stored as bytes in memory (estimated size 3.8 MB, free 1941.5 MB)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Added taskresult_14 in memory on 192.168.0.152:57313 (size: 3.8 MB, free: 1941.7 MB)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 3973694 bytes result sent via BlockManager)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 402 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Removed taskresult_13 on 192.168.0.152:57313 in memory (size: 7.9 MB, free: 1949.6 MB)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 316 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Removed taskresult_14 on 192.168.0.152:57313 in memory (size: 3.8 MB, free: 1953.4 MB)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 2015 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Removed taskresult_10 on 192.168.0.152:57313 in memory (size: 38.6 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] ResultStage 0 (collect at LoadDataUtil.scala:40) finished in 6.894 s
[INFO ] [2018-04-27 11:45:52] [Logging$class:logInfo:54] Job 0 finished: collect at LoadDataUtil.scala:40, took 7.011304 s
[INFO ] [2018-04-27 11:45:59] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:45:59] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:45:59] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:46:00] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:46:03] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:57313 in memory (size: 1962.0 B, free: 1992.0 MB)
[INFO ] [2018-04-27 11:46:03] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:46:04] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:116
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:116) with 15 output partitions
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Final stage: ResultStage 1 (saveAsTable at HistoryDataToA.scala:116)
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:116), which has no missing parents
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:57313 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at saveAsTable at HistoryDataToA.scala:116) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 11:46:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Code generated in 247.345981 ms
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Code generated in 14.287678 ms
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Code generated in 14.409636 ms
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Code generated in 10.290602 ms
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Code generated in 29.734153 ms
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Code generated in 13.980327 ms
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114606_0001_m_000000_0
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 594 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:46:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114607_0001_m_000001_0
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 762 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114607_0001_m_000002_0
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 116 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114607_0001_m_000003_0
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 510 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114607_0001_m_000004_0
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 119 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:46:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114608_0001_m_000005_0
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 153 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114608_0001_m_000006_0
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1710 bytes result sent to driver
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 27 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114608_0001_m_000007_0
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 257 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114608_0001_m_000008_0
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 215 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:46:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114609_0001_m_000009_0
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 663 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:46:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114610_0001_m_000010_0
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 759 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114610_0001_m_000011_0
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 63 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114610_0001_m_000012_0
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 175 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114610_0001_m_000013_0
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 187 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114610_0001_m_000014_0
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 119 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] ResultStage 1 (saveAsTable at HistoryDataToA.scala:116) finished in 4.704 s
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:116, took 4.742079 s
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:46:10] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:46:11] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-f96b1098-b4cd-4947-8137-920ab11d4652
[INFO ] [2018-04-27 11:47:39] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:47:39] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:47:39] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:47:39] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:47:39] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:47:39] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:47:39] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 57381.
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-f600d086-e5e5-42eb-a854-97537721417a
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57390.
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Server created on 192.168.0.152:57390
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 57390, None)
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:57390 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 57390, None)
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 57390, None)
[INFO ] [2018-04-27 11:47:40] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 57390, None)
[INFO ] [2018-04-27 11:47:41] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:47:41] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:47:41] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:57390 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:47:41] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:47:43] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:47:43] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:47:43] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:47:43] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:47:48] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:47:48] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:47:48] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:47:49] [Logging$class:logInfo:54] Code generated in 242.346049 ms
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Got job 0 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 13.4 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:57390 (size: 6.2 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:47:54] [Logging$class:logInfo:54] Code generated in 20.589511 ms
[INFO ] [2018-04-27 11:47:55] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
[INFO ] [2018-04-27 11:47:55] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:47:55] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:47:55] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 797 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:47:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 1124 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 104 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1152 bytes result sent to driver
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 472 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:47:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 93 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 138 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1023 bytes result sent to driver
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 13 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 269 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 237 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:47:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:47:58] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1152 bytes result sent to driver
[INFO ] [2018-04-27 11:47:58] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:47:58] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:47:58] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 609 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:47:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 731 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 40 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1152 bytes result sent to driver
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 162 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 173 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 99 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] ResultStage 0 (collect at HistoryDataToA.scala:111) finished in 5.059 s
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Job 0 finished: collect at HistoryDataToA.scala:111, took 5.206076 s
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:47:59] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Final stage: ResultStage 1 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.6 MB)
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:57390 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Code generated in 16.386275 ms
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Code generated in 12.566285 ms
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Code generated in 6.844039 ms
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Code generated in 20.795292 ms
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Code generated in 14.90389 ms
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114800_0001_m_000000_0
[INFO ] [2018-04-27 11:48:00] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 207 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:57390 in memory (size: 6.2 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114801_0001_m_000001_0
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 947 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 11:48:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114802_0001_m_000002_0
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 144 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114802_0001_m_000003_0
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 593 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114802_0001_m_000004_0
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 129 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114802_0001_m_000005_0
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 162 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114802_0001_m_000006_0
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1710 bytes result sent to driver
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 34 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114803_0001_m_000007_0
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 318 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114803_0001_m_000008_0
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 229 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 11:48:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114804_0001_m_000009_0
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 704 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:48:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114805_0001_m_000010_0
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 902 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114805_0001_m_000011_0
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 42 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114805_0001_m_000012_0
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 166 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114805_0001_m_000013_0
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 195 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427114805_0001_m_000014_0
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 100 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] ResultStage 1 (saveAsTable at HistoryDataToA.scala:117) finished in 4.858 s
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:117, took 4.900047 s
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:48:05] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:48:06] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-2a323fdb-ac55-4e66-af55-d844d6851dc0
[INFO ] [2018-04-27 11:57:00] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 11:57:01] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 11:57:01] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 11:57:01] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 11:57:01] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 11:57:01] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 11:57:01] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 57545.
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-491c33c9-a060-4193-9671-d54cd462c8d3
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57556.
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Server created on 192.168.0.152:57556
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 57556, None)
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:57556 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 57556, None)
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 57556, None)
[INFO ] [2018-04-27 11:57:02] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 57556, None)
[INFO ] [2018-04-27 11:57:03] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:57:03] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:57:03] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:57556 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:57:03] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 11:57:05] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 11:57:05] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 11:57:05] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 11:57:05] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 11:57:09] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:57:09] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 11:57:09] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 11:57:10] [Logging$class:logInfo:54] Code generated in 215.342187 ms
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Got job 0 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 13.4 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:57556 (size: 6.2 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Code generated in 27.524924 ms
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 313 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:57:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 517 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 71 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 355 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 80 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 92 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 9 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 191 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:57:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 181 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 398 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1152 bytes result sent to driver
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 459 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 11:57:17] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 30 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 106 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1109 bytes result sent to driver
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 116 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1066 bytes result sent to driver
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 74 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] ResultStage 0 (collect at HistoryDataToA.scala:111) finished in 2.981 s
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Job 0 finished: collect at HistoryDataToA.scala:111, took 3.076649 s
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:57:18] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Final stage: ResultStage 1 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.6 MB)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:57556 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Code generated in 13.666179 ms
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Code generated in 10.008548 ms
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Code generated in 9.193728 ms
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Code generated in 24.927542 ms
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Code generated in 18.110311 ms
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115719_0001_m_000000_0
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 194 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 11:57:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:57556 in memory (size: 6.2 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115720_0001_m_000001_0
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 669 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115720_0001_m_000002_0
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 77 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115720_0001_m_000003_0
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 354 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115720_0001_m_000004_0
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 82 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:20] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115720_0001_m_000005_0
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 115 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115721_0001_m_000006_0
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 39 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115721_0001_m_000007_0
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 237 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115721_0001_m_000008_0
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 181 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115721_0001_m_000009_0
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 473 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 11:57:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115722_0001_m_000010_0
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1839 bytes result sent to driver
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 551 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115722_0001_m_000011_0
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 33 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115722_0001_m_000012_0
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 109 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115722_0001_m_000013_0
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1753 bytes result sent to driver
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 123 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427115722_0001_m_000014_0
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1796 bytes result sent to driver
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 80 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] ResultStage 1 (saveAsTable at HistoryDataToA.scala:117) finished in 3.307 s
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:117, took 3.343306 s
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 11:57:22] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 11:57:23] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-2ba0309e-e4fc-46e2-9141-c45c3f2d7606
[INFO ] [2018-04-27 13:45:03] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 59499.
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-d575710c-287d-4ef7-b1cc-e3a3c8064c8a
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 13:45:04] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59508.
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Server created on 192.168.0.152:59508
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 59508, None)
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:59508 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 59508, None)
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 59508, None)
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 59508, None)
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:59508 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 13:45:05] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 13:45:07] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 13:45:07] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 13:45:07] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 13:45:07] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 13:45:11] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 13:45:12] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 13:45:12] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 13:45:12] [Logging$class:logInfo:54] Code generated in 243.92245 ms
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Got job 0 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 13.4 KB, free 1991.8 MB)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:59508 (size: 6.2 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Code generated in 15.132705 ms
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 288 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 13:45:17] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 517 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 79 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1109 bytes result sent to driver
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 335 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 65 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1109 bytes result sent to driver
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 95 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 16 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 13:45:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 153 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 165 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1109 bytes result sent to driver
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 465 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 13:45:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1109 bytes result sent to driver
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 471 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 29 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 97 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 116 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1066 bytes result sent to driver
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 78 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] ResultStage 0 (collect at HistoryDataToA.scala:111) finished in 2.961 s
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Job 0 finished: collect at HistoryDataToA.scala:111, took 3.043997 s
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 13:45:20] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Final stage: ResultStage 1 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 80.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.9 KB, free 1991.6 MB)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:59508 (size: 30.9 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Code generated in 15.862569 ms
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Code generated in 12.104125 ms
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Code generated in 7.744948 ms
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Code generated in 21.402443 ms
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Code generated in 17.344954 ms
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134521_0001_m_000000_0
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1839 bytes result sent to driver
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 13:45:21] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 192 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:59508 in memory (size: 6.2 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134522_0001_m_000001_0
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1839 bytes result sent to driver
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 549 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134522_0001_m_000002_0
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1796 bytes result sent to driver
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 81 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 13:45:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134523_0001_m_000003_0
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1796 bytes result sent to driver
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 590 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134523_0001_m_000004_0
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1839 bytes result sent to driver
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 128 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134523_0001_m_000005_0
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1796 bytes result sent to driver
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 143 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134523_0001_m_000006_0
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1796 bytes result sent to driver
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 30 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134523_0001_m_000007_0
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1796 bytes result sent to driver
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 262 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134523_0001_m_000008_0
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1753 bytes result sent to driver
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 205 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 13:45:23] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134524_0001_m_000009_0
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1753 bytes result sent to driver
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 448 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 13:45:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134525_0001_m_000010_0
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1839 bytes result sent to driver
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 874 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134525_0001_m_000011_0
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1796 bytes result sent to driver
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 41 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134525_0001_m_000012_0
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1753 bytes result sent to driver
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 123 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134525_0001_m_000013_0
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1796 bytes result sent to driver
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 134 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427134525_0001_m_000014_0
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1753 bytes result sent to driver
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 82 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] ResultStage 1 (saveAsTable at HistoryDataToA.scala:117) finished in 3.865 s
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:117, took 3.902575 s
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 13:45:25] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Cleaned accumulator 49
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Cleaned accumulator 51
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Cleaned accumulator 52
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Removed broadcast_2_piece0 on 192.168.0.152:59508 in memory (size: 30.9 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Cleaned accumulator 50
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 13:45:26] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-98159d76-e35b-4043-9595-10c2f18fa7c0
[INFO ] [2018-04-27 15:30:38] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 15:30:39] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 15:30:39] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 15:30:39] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 15:30:39] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 15:30:39] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 15:30:39] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 61820.
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-2ca99062-3caa-438d-ab61-d8d39be78b85
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61829.
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Server created on 192.168.0.152:61829
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 61829, None)
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:61829 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 61829, None)
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 61829, None)
[INFO ] [2018-04-27 15:30:40] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 61829, None)
[INFO ] [2018-04-27 15:30:41] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 15:30:41] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 15:30:41] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:61829 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 15:30:41] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 15:30:42] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 15:30:42] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 15:30:42] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 15:30:43] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[ERROR] [2018-04-27 15:30:46] [ObjectStore:checkSchema:6684] Version information found in metastore differs 2.1.0 from expected schema version 1.2.0. Schema verififcation is disabled hive.metastore.schema.verification so setting version.
[INFO ] [2018-04-27 15:30:47] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 15:30:47] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 15:30:47] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 15:30:48] [Logging$class:logInfo:54] Code generated in 207.174347 ms
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Got job 0 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:61829 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 15:30:52] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Code generated in 19.35784 ms
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 303 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 539 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 73 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 15:30:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1109 bytes result sent to driver
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 372 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 66 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 90 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 11 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 215 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1109 bytes result sent to driver
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 165 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 15:30:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1109 bytes result sent to driver
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 400 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1152 bytes result sent to driver
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 481 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 31 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 93 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 112 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 15:30:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1066 bytes result sent to driver
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 75 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] ResultStage 0 (collect at HistoryDataToA.scala:111) finished in 3.015 s
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Job 0 finished: collect at HistoryDataToA.scala:111, took 3.103920 s
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 15:30:56] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Code generated in 20.796047 ms
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Final stage: ResultStage 1 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[12] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 86.1 KB, free 1991.7 MB)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.7 KB, free 1991.6 MB)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:61829 (size: 32.7 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Code generated in 19.226441 ms
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Code generated in 10.728596 ms
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Code generated in 7.024146 ms
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Code generated in 24.246763 ms
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Code generated in 15.260705 ms
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153057_0001_m_000000_0
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1929 bytes result sent to driver
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 209 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 15:30:57] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:61829 in memory (size: 6.3 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153058_0001_m_000001_0
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1886 bytes result sent to driver
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 522 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153058_0001_m_000002_0
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 109 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153058_0001_m_000003_0
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 536 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153058_0001_m_000004_0
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1929 bytes result sent to driver
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 124 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153058_0001_m_000005_0
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 127 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153058_0001_m_000006_0
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 34 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 15:30:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153059_0001_m_000007_0
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1886 bytes result sent to driver
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 215 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153059_0001_m_000008_0
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 159 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153059_0001_m_000009_0
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 447 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 15:30:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153100_0001_m_000010_0
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1929 bytes result sent to driver
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 518 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153100_0001_m_000011_0
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 41 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153100_0001_m_000012_0
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 105 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153100_0001_m_000013_0
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 131 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427153100_0001_m_000014_0
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1843 bytes result sent to driver
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 66 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] ResultStage 1 (saveAsTable at HistoryDataToA.scala:117) finished in 3.327 s
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:117, took 3.370817 s
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 15:31:00] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 15:31:01] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-193e9d6a-6083-476b-91e9-6d4c76602224
[INFO ] [2018-04-27 16:45:41] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 52083.
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-ccd9f6df-ea0a-4944-b65d-e8d12568a999
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 16:45:42] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52093.
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Server created on 192.168.0.152:52093
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 52093, None)
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:52093 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 52093, None)
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 52093, None)
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 52093, None)
[INFO ] [2018-04-27 16:45:43] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 16:45:44] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 16:45:44] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:52093 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 16:45:44] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 16:45:45] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 16:45:45] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 16:45:45] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 16:45:46] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[ERROR] [2018-04-27 16:45:49] [ObjectStore:checkSchema:6684] Version information found in metastore differs 2.1.0 from expected schema version 1.2.0. Schema verififcation is disabled hive.metastore.schema.verification so setting version.
[INFO ] [2018-04-27 16:45:50] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 16:45:50] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 16:45:50] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 16:45:51] [Logging$class:logInfo:54] Code generated in 233.819218 ms
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Got job 0 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:52093 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 16:45:56] [Logging$class:logInfo:54] Code generated in 55.119722 ms
[INFO ] [2018-04-27 16:45:57] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
[INFO ] [2018-04-27 16:45:57] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 16:45:57] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 16:45:57] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 647 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 16:45:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 1144 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1066 bytes result sent to driver
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 130 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1109 bytes result sent to driver
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 412 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 16:45:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1066 bytes result sent to driver
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 133 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1109 bytes result sent to driver
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 160 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1023 bytes result sent to driver
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 36 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1066 bytes result sent to driver
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 214 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1066 bytes result sent to driver
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 231 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 16:45:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1109 bytes result sent to driver
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 426 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1109 bytes result sent to driver
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 592 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1066 bytes result sent to driver
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 96 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1066 bytes result sent to driver
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 139 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 16:46:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1109 bytes result sent to driver
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 187 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1066 bytes result sent to driver
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 126 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] ResultStage 0 (collect at HistoryDataToA.scala:111) finished in 4.661 s
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Job 0 finished: collect at HistoryDataToA.scala:111, took 4.791786 s
[INFO ] [2018-04-27 16:46:01] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:02] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Code generated in 21.897837 ms
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Final stage: ResultStage 1 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[12] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 86.1 KB, free 1991.7 MB)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.7 KB, free 1991.6 MB)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:52093 (size: 32.7 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Code generated in 16.775945 ms
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Code generated in 11.634418 ms
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Code generated in 9.438404 ms
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Code generated in 34.271183 ms
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Code generated in 15.810469 ms
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164603_0001_m_000000_0
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1929 bytes result sent to driver
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 234 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 16:46:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:52093 in memory (size: 6.3 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164604_0001_m_000001_0
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 514 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164604_0001_m_000002_0
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 86 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164604_0001_m_000003_0
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1843 bytes result sent to driver
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 368 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164604_0001_m_000004_0
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1929 bytes result sent to driver
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 83 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164605_0001_m_000005_0
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 116 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164605_0001_m_000006_0
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 51 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164605_0001_m_000007_0
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 293 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164605_0001_m_000008_0
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 234 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 16:46:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164606_0001_m_000009_0
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1843 bytes result sent to driver
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 486 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164606_0001_m_000010_0
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 487 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164606_0001_m_000011_0
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1843 bytes result sent to driver
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 50 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164606_0001_m_000012_0
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1886 bytes result sent to driver
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 111 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164606_0001_m_000013_0
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1843 bytes result sent to driver
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 137 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 16:46:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427164607_0001_m_000014_0
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1843 bytes result sent to driver
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 69 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] ResultStage 1 (saveAsTable at HistoryDataToA.scala:117) finished in 3.304 s
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:117, took 3.349165 s
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 16:46:07] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-e661a12a-11bc-4e8a-ad88-439c6dab909e
[INFO ] [2018-04-27 17:00:36] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:00:36] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:00:36] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:00:36] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:00:36] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:00:36] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:00:36] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 52271.
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-7ab3aa37-593b-41f3-b91d-096f79947482
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52280.
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Server created on 192.168.0.152:52280
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 52280, None)
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:52280 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 52280, None)
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 52280, None)
[INFO ] [2018-04-27 17:00:37] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 52280, None)
[INFO ] [2018-04-27 17:00:38] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:00:38] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:00:38] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:52280 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:00:38] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:00:39] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 17:00:39] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 17:00:39] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 17:00:40] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 17:00:44] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:00:44] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:00:44] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 17:00:45] [Logging$class:logInfo:54] Code generated in 252.094916 ms
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Got job 0 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:52280 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:00:49] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Code generated in 21.252928 ms
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 299 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 482 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 62 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:00:50] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 305 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 61 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 81 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 14 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 185 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 152 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1152 bytes result sent to driver
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 369 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 474 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 32 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 88 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 103 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 77 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] ResultStage 0 (collect at HistoryDataToA.scala:111) finished in 2.779 s
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Job 0 finished: collect at HistoryDataToA.scala:111, took 2.865225 s
[INFO ] [2018-04-27 17:00:52] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Final stage: ResultStage 1 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 81.5 KB, free 1991.7 MB)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.0 KB, free 1991.6 MB)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:52280 (size: 31.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Code generated in 16.329644 ms
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Code generated in 14.471943 ms
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Code generated in 7.091735 ms
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Code generated in 19.36049 ms
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Code generated in 16.500311 ms
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170054_0001_m_000000_0
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 202 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:52280 in memory (size: 6.3 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170054_0001_m_000001_0
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 493 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170054_0001_m_000002_0
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 110 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 17:00:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170055_0001_m_000003_0
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 462 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170055_0001_m_000004_0
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 86 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170055_0001_m_000005_0
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 143 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170055_0001_m_000006_0
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1710 bytes result sent to driver
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 41 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170055_0001_m_000007_0
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 223 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 17:00:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170056_0001_m_000008_0
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 221 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170056_0001_m_000009_0
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 494 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:00:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170057_0001_m_000010_0
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 506 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170057_0001_m_000011_0
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 44 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170057_0001_m_000012_0
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 109 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170057_0001_m_000013_0
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 114 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170057_0001_m_000014_0
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 82 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] ResultStage 1 (saveAsTable at HistoryDataToA.scala:117) finished in 3.308 s
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:117, took 3.342056 s
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Cleaned accumulator 52
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Cleaned accumulator 49
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Removed broadcast_2_piece0 on 192.168.0.152:52280 in memory (size: 31.0 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Cleaned accumulator 51
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Cleaned accumulator 50
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:00:57] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:00:58] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-44454c15-3a93-4919-8724-b3fa0ab7ba35
[INFO ] [2018-04-27 17:07:37] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 52395.
[INFO ] [2018-04-27 17:07:38] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-1d9aac8a-a6bf-4667-950d-f901989a39f3
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52404.
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Server created on 192.168.0.152:52404
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 52404, None)
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:52404 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 52404, None)
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 52404, None)
[INFO ] [2018-04-27 17:07:39] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 52404, None)
[INFO ] [2018-04-27 17:07:40] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:07:40] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:07:40] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:52404 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:07:40] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:07:42] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 17:07:42] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 17:07:42] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 17:07:42] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 17:07:46] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:07:46] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:07:47] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 17:07:47] [Logging$class:logInfo:54] Code generated in 256.779947 ms
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:217
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:217) with 15 output partitions
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:217)
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[9] at collect at posProductProcessor.scala:217), which has no missing parents
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:52404 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at collect at posProductProcessor.scala:217) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:07:52] [Logging$class:logInfo:54] Code generated in 24.090831 ms
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 298 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 471 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 66 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 1195 bytes result sent to driver
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 365 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 67 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:07:53] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 85 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 12 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 197 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 163 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 357 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:07:54] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 1152 bytes result sent to driver
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 414 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 27 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 88 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 111 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 65 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:217) finished in 2.781 s
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Job 0 finished: collect at posProductProcessor.scala:217, took 2.877318 s
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Got job 1 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Final stage: ResultStage 1 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[11] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 13.9 KB, free 1991.7 MB)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.7 MB)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:52404 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 26 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:07:55] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:52404 in memory (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 495 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 65 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 333 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 69 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 91 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 16 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 133 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 120 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:07:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 371 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 426 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 28 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 105 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 127 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 66 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] ResultStage 1 (collect at HistoryDataToA.scala:111) finished in 2.452 s
[INFO ] [2018-04-27 17:07:57] [Logging$class:logInfo:54] Job 1 finished: collect at HistoryDataToA.scala:111, took 2.468043 s
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 17:07:58] [Logging$class:logInfo:54] Removed broadcast_2_piece0 on 192.168.0.152:52404 in memory (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Final stage: ResultStage 2 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Submitting ResultStage 2 (MapPartitionsRDD[13] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 81.5 KB, free 1991.7 MB)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 31.0 KB, free 1991.7 MB)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:52404 (size: 31.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Adding task set 2.0 with 15 tasks
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 30, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 30)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Code generated in 16.638883 ms
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Code generated in 11.61214 ms
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Code generated in 9.436894 ms
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Code generated in 21.97411 ms
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Code generated in 19.373328 ms
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170759_0002_m_000000_0
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Starting task 1.0 in stage 2.0 (TID 31, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30) in 184 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Running task 1.0 in stage 2.0 (TID 31)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170759_0002_m_000001_0
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Starting task 2.0 in stage 2.0 (TID 32, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Running task 2.0 in stage 2.0 (TID 32)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31) in 462 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170759_0002_m_000002_0
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Starting task 3.0 in stage 2.0 (TID 33, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Running task 3.0 in stage 2.0 (TID 33)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32) in 68 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:07:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170800_0002_m_000003_0
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Starting task 4.0 in stage 2.0 (TID 34, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Running task 4.0 in stage 2.0 (TID 34)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33) in 315 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170800_0002_m_000004_0
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Starting task 5.0 in stage 2.0 (TID 35, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Running task 5.0 in stage 2.0 (TID 35)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34) in 66 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170800_0002_m_000005_0
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Starting task 6.0 in stage 2.0 (TID 36, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35) in 111 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Running task 6.0 in stage 2.0 (TID 36)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170800_0002_m_000006_0
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36). 1710 bytes result sent to driver
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Starting task 7.0 in stage 2.0 (TID 37, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Running task 7.0 in stage 2.0 (TID 37)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36) in 23 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170800_0002_m_000007_0
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Starting task 8.0 in stage 2.0 (TID 38, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Running task 8.0 in stage 2.0 (TID 38)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37) in 174 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170800_0002_m_000008_0
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Starting task 9.0 in stage 2.0 (TID 39, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Running task 9.0 in stage 2.0 (TID 39)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38) in 151 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:08:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170801_0002_m_000009_0
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Starting task 10.0 in stage 2.0 (TID 40, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Running task 10.0 in stage 2.0 (TID 40)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39) in 344 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170801_0002_m_000010_0
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Starting task 11.0 in stage 2.0 (TID 41, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Running task 11.0 in stage 2.0 (TID 41)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40) in 413 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170801_0002_m_000011_0
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Starting task 12.0 in stage 2.0 (TID 42, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Running task 12.0 in stage 2.0 (TID 42)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41) in 33 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170801_0002_m_000012_0
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Starting task 13.0 in stage 2.0 (TID 43, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Running task 13.0 in stage 2.0 (TID 43)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42) in 100 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170801_0002_m_000013_0
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Starting task 14.0 in stage 2.0 (TID 44, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Running task 14.0 in stage 2.0 (TID 44)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43) in 109 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427170801_0002_m_000014_0
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44) in 64 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] ResultStage 2 (saveAsTable at HistoryDataToA.scala:117) finished in 2.608 s
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Job 2 finished: saveAsTable at HistoryDataToA.scala:117, took 2.636835 s
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 17:08:01] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:08:02] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-cb2ca939-b16e-44d5-8986-4aead3f48acb
[INFO ] [2018-04-27 17:10:52] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:10:53] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:10:53] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:10:53] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:10:53] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:10:53] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:10:53] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 52471.
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-94c7c0d2-d6a8-4efc-a522-06acda7b3c83
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52480.
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Server created on 192.168.0.152:52480
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 52480, None)
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:52480 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 52480, None)
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 52480, None)
[INFO ] [2018-04-27 17:10:54] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 52480, None)
[INFO ] [2018-04-27 17:10:55] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:10:55] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:10:55] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:52480 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:10:55] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:216
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:216) with 15 output partitions
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:216)
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211), which has no missing parents
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:52480 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:11:00] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 400 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:11:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 640 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 78 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 280 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 76 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 82 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 716 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 17 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 144 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 117 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:11:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 332 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 436 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 22 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 75 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 88 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 52 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:216) finished in 2.836 s
[INFO ] [2018-04-27 17:11:03] [Logging$class:logInfo:54] Job 0 finished: collect at posProductProcessor.scala:216, took 2.964652 s
[INFO ] [2018-04-27 17:11:04] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:52480 in memory (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:11:05] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 17:11:05] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 17:11:05] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 17:11:05] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 17:11:08] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:11:08] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Code generated in 217.507321 ms
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:111
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Got job 1 (collect at HistoryDataToA.scala:111) with 15 output partitions
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Final stage: ResultStage 1 (collect at HistoryDataToA.scala:111)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111), which has no missing parents
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:52480 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Code generated in 16.80011 ms
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 59 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:11:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 422 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 52 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 259 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 58 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 105 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1023 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 16 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 154 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 151 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 17:11:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 502 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 396 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 32 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:11:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 99 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 104 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 51 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] ResultStage 1 (collect at HistoryDataToA.scala:111) finished in 2.445 s
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Job 1 finished: collect at HistoryDataToA.scala:111, took 2.466033 s
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:12] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:117
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:117) with 15 output partitions
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Final stage: ResultStage 2 (saveAsTable at HistoryDataToA.scala:117)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Submitting ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117), which has no missing parents
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 81.5 KB, free 1991.7 MB)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 31.0 KB, free 1991.6 MB)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:52480 (size: 31.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:117) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Adding task set 2.0 with 15 tasks
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 30, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 30)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Code generated in 14.484403 ms
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Code generated in 11.551727 ms
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Code generated in 9.718947 ms
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Code generated in 20.845896 ms
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Code generated in 13.867058 ms
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171113_0002_m_000000_0
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Starting task 1.0 in stage 2.0 (TID 31, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Running task 1.0 in stage 2.0 (TID 31)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30) in 181 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Removed broadcast_2_piece0 on 192.168.0.152:52480 in memory (size: 6.3 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171113_0002_m_000001_0
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Starting task 2.0 in stage 2.0 (TID 32, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Running task 2.0 in stage 2.0 (TID 32)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31) in 377 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171113_0002_m_000002_0
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Starting task 3.0 in stage 2.0 (TID 33, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Running task 3.0 in stage 2.0 (TID 33)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32) in 77 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171114_0002_m_000003_0
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Starting task 4.0 in stage 2.0 (TID 34, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Running task 4.0 in stage 2.0 (TID 34)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33) in 290 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171114_0002_m_000004_0
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Starting task 5.0 in stage 2.0 (TID 35, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34) in 69 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Running task 5.0 in stage 2.0 (TID 35)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171114_0002_m_000005_0
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Starting task 6.0 in stage 2.0 (TID 36, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35) in 107 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Running task 6.0 in stage 2.0 (TID 36)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171114_0002_m_000006_0
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Starting task 7.0 in stage 2.0 (TID 37, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Running task 7.0 in stage 2.0 (TID 37)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36) in 31 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171114_0002_m_000007_0
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Starting task 8.0 in stage 2.0 (TID 38, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Running task 8.0 in stage 2.0 (TID 38)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37) in 164 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171114_0002_m_000008_0
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Starting task 9.0 in stage 2.0 (TID 39, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Running task 9.0 in stage 2.0 (TID 39)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38) in 157 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:11:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171115_0002_m_000009_0
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Starting task 10.0 in stage 2.0 (TID 40, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Running task 10.0 in stage 2.0 (TID 40)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39) in 344 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171115_0002_m_000010_0
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Starting task 11.0 in stage 2.0 (TID 41, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40) in 384 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Running task 11.0 in stage 2.0 (TID 41)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171115_0002_m_000011_0
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Starting task 12.0 in stage 2.0 (TID 42, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Running task 12.0 in stage 2.0 (TID 42)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41) in 33 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171115_0002_m_000012_0
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Starting task 13.0 in stage 2.0 (TID 43, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42) in 96 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Running task 13.0 in stage 2.0 (TID 43)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171115_0002_m_000013_0
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Starting task 14.0 in stage 2.0 (TID 44, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Running task 14.0 in stage 2.0 (TID 44)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43) in 94 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427171115_0002_m_000014_0
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44) in 60 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] ResultStage 2 (saveAsTable at HistoryDataToA.scala:117) finished in 2.448 s
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Job 2 finished: saveAsTable at HistoryDataToA.scala:117, took 2.475897 s
[INFO ] [2018-04-27 17:11:15] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Cleaned accumulator 75
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Cleaned accumulator 76
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Cleaned accumulator 73
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:52480 in memory (size: 31.0 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Cleaned accumulator 74
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:11:16] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-d1147fce-7061-4997-8078-db235d9609e6
[INFO ] [2018-04-27 17:31:52] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53131.
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-c3f4ada0-4d53-465d-a74c-456c24ffcabb
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53140.
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Server created on 192.168.0.152:53140
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53140, None)
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53140 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53140, None)
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53140, None)
[INFO ] [2018-04-27 17:31:53] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53140, None)
[INFO ] [2018-04-27 17:31:54] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:31:54] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:31:54] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53140 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:31:54] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:216
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:216) with 15 output partitions
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:216)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211), which has no missing parents
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53140 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 343 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:32:00] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 454 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 42 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 197 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 45 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 78 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 17 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 97 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 93 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 286 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:32:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 267 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 716 bytes result sent to driver
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 19 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 54 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 64 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 36 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:216) finished in 2.087 s
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Job 0 finished: collect at posProductProcessor.scala:216, took 2.229812 s
[INFO ] [2018-04-27 17:32:02] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:53140 in memory (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:32:03] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 17:32:03] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 17:32:03] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 17:32:03] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 17:32:07] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:32:07] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:32:07] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Code generated in 202.809961 ms
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:113
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Got job 1 (collect at HistoryDataToA.scala:113) with 15 output partitions
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Final stage: ResultStage 1 (collect at HistoryDataToA.scala:113)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113), which has no missing parents
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:53140 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Code generated in 17.792017 ms
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 59 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 318 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 44 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 203 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 57 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 93 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 17 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:32:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 186 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 116 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 287 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1152 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 307 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 20 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 61 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 79 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1066 bytes result sent to driver
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 40 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] ResultStage 1 (collect at HistoryDataToA.scala:113) finished in 1.876 s
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Job 1 finished: collect at HistoryDataToA.scala:113, took 1.896617 s
[INFO ] [2018-04-27 17:32:09] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:10] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:119
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:119) with 15 output partitions
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Final stage: ResultStage 2 (saveAsTable at HistoryDataToA.scala:119)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Submitting ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:119), which has no missing parents
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 81.5 KB, free 1991.7 MB)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 31.0 KB, free 1991.6 MB)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:53140 (size: 31.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Adding task set 2.0 with 15 tasks
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 30, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 30)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Code generated in 21.417554 ms
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Code generated in 32.00305 ms
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Code generated in 11.358406 ms
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Code generated in 23.411562 ms
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Code generated in 16.140853 ms
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173211_0002_m_000000_0
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30). 1839 bytes result sent to driver
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Starting task 1.0 in stage 2.0 (TID 31, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30) in 252 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Running task 1.0 in stage 2.0 (TID 31)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173211_0002_m_000001_0
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Starting task 2.0 in stage 2.0 (TID 32, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Running task 2.0 in stage 2.0 (TID 32)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31) in 309 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173211_0002_m_000002_0
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Starting task 3.0 in stage 2.0 (TID 33, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Running task 3.0 in stage 2.0 (TID 33)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32) in 61 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:32:11] [Logging$class:logInfo:54] Removed broadcast_2_piece0 on 192.168.0.152:53140 in memory (size: 6.3 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173212_0002_m_000003_0
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33). 1882 bytes result sent to driver
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Starting task 4.0 in stage 2.0 (TID 34, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Running task 4.0 in stage 2.0 (TID 34)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33) in 230 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173212_0002_m_000004_0
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Starting task 5.0 in stage 2.0 (TID 35, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Running task 5.0 in stage 2.0 (TID 35)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34) in 58 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173212_0002_m_000005_0
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Starting task 6.0 in stage 2.0 (TID 36, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35) in 86 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Running task 6.0 in stage 2.0 (TID 36)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173212_0002_m_000006_0
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Starting task 7.0 in stage 2.0 (TID 37, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Running task 7.0 in stage 2.0 (TID 37)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36) in 23 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173212_0002_m_000007_0
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Starting task 8.0 in stage 2.0 (TID 38, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Running task 8.0 in stage 2.0 (TID 38)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37) in 138 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173212_0002_m_000008_0
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Starting task 9.0 in stage 2.0 (TID 39, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38) in 96 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Running task 9.0 in stage 2.0 (TID 39)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173212_0002_m_000009_0
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Starting task 10.0 in stage 2.0 (TID 40, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39) in 279 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Running task 10.0 in stage 2.0 (TID 40)
[INFO ] [2018-04-27 17:32:12] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173213_0002_m_000010_0
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Starting task 11.0 in stage 2.0 (TID 41, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Running task 11.0 in stage 2.0 (TID 41)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40) in 371 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173213_0002_m_000011_0
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Starting task 12.0 in stage 2.0 (TID 42, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Running task 12.0 in stage 2.0 (TID 42)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41) in 31 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173213_0002_m_000012_0
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Starting task 13.0 in stage 2.0 (TID 43, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Running task 13.0 in stage 2.0 (TID 43)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42) in 78 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173213_0002_m_000013_0
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43). 1796 bytes result sent to driver
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Starting task 14.0 in stage 2.0 (TID 44, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Running task 14.0 in stage 2.0 (TID 44)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43) in 74 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427173213_0002_m_000014_0
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44). 1753 bytes result sent to driver
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44) in 48 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] ResultStage 2 (saveAsTable at HistoryDataToA.scala:119) finished in 2.122 s
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Job 2 finished: saveAsTable at HistoryDataToA.scala:119, took 2.152879 s
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Found 0 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Finished to gather the fast stats for all 0 partitions.
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Recovered all partitions (0).
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:32:13] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-0fd63273-6179-4a52-8d36-3590d919fc83
[INFO ] [2018-04-27 17:35:10] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:35:10] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:35:10] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:35:10] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:35:10] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:35:10] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:35:10] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53204.
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-ed046f3f-7ea5-4068-95dd-1be3a1e65e5b
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53213.
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Server created on 192.168.0.152:53213
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53213, None)
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53213 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53213, None)
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53213, None)
[INFO ] [2018-04-27 17:35:11] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53213, None)
[INFO ] [2018-04-27 17:35:12] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:35:12] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:35:12] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53213 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:35:12] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:216
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:216) with 15 output partitions
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:216)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211), which has no missing parents
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53213 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 372 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:35:18] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:35:21] [Logging$class:logInfo:54] Block taskresult_1 stored as bytes in memory (estimated size 9.5 MB, free 1982.2 MB)
[INFO ] [2018-04-27 17:35:21] [Logging$class:logInfo:54] Added taskresult_1 in memory on 192.168.0.152:53213 (size: 9.5 MB, free: 1982.4 MB)
[INFO ] [2018-04-27 17:35:21] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 9991361 bytes result sent via BlockManager)
[INFO ] [2018-04-27 17:35:21] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:35:21] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:35:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:35:22] [TransportClientFactory:createClient:254] Successfully created connection to /192.168.0.152:53213 after 39 ms (0 ms spent in bootstraps)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 3511 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Removed taskresult_1 on 192.168.0.152:53213 in memory (size: 9.5 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Block taskresult_2 stored as bytes in memory (estimated size 1347.8 KB, free 1990.4 MB)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Added taskresult_2 in memory on 192.168.0.152:53213 (size: 1347.8 KB, free: 1990.7 MB)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1380171 bytes result sent via BlockManager)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Removed taskresult_2 on 192.168.0.152:53213 in memory (size: 1347.8 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:35:22] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 764 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Block taskresult_3 stored as bytes in memory (estimated size 7.7 MB, free 1984.1 MB)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Added taskresult_3 in memory on 192.168.0.152:53213 (size: 7.7 MB, free: 1984.3 MB)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 8071224 bytes result sent via BlockManager)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 109 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 2450 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Removed taskresult_3 on 192.168.0.152:53213 in memory (size: 7.7 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 222 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 716 bytes result sent to driver
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 10 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 17:35:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 716 bytes result sent to driver
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 183 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 417 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 17:35:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 558 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 23 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 106 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 121 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 716 bytes result sent to driver
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 65 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:216) finished in 8.032 s
[INFO ] [2018-04-27 17:35:26] [Logging$class:logInfo:54] Job 0 finished: collect at posProductProcessor.scala:216, took 8.165698 s
[INFO ] [2018-04-27 17:35:29] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 17:35:29] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 17:35:29] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 17:35:30] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 17:35:33] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:53213 in memory (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:35:34] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:35:34] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 17:35:34] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Code generated in 244.957116 ms
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:113
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Got job 1 (collect at HistoryDataToA.scala:113) with 15 output partitions
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Final stage: ResultStage 1 (collect at HistoryDataToA.scala:113)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113), which has no missing parents
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:53213 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Code generated in 16.415732 ms
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1109 bytes result sent to driver
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 61 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Code generated in 115.676239 ms
[ERROR] [2018-04-27 17:35:35] [Logging$class:logError:91] Exception in task 1.0 in stage 1.0 (TID 16)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of bigint
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, promotion_code), StringType), true) AS promotion_code#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, promotion_good_id), StringType), true) AS promotion_good_id#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, promotion_shop_id), StringType), true) AS promotion_shop_id#2
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, promotion_type), StringType), true) AS promotion_type#3
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, promotion_start), LongType) AS promotion_start#4L
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, promotion_end), LongType) AS promotion_end#5L
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, promotion_banner), StringType), true) AS promotion_banner#6
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 7, promotion_banner_code), StringType), true) AS promotion_banner_code#7
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of bigint
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr4$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[WARN ] [2018-04-27 17:35:35] [Logging$class:logWarning:66] Lost task 1.0 in stage 1.0 (TID 16, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of bigint
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, promotion_code), StringType), true) AS promotion_code#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, promotion_good_id), StringType), true) AS promotion_good_id#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, promotion_shop_id), StringType), true) AS promotion_shop_id#2
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, promotion_type), StringType), true) AS promotion_type#3
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, promotion_start), LongType) AS promotion_start#4L
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, promotion_end), LongType) AS promotion_end#5L
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, promotion_banner), StringType), true) AS promotion_banner#6
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 7, promotion_banner_code), StringType), true) AS promotion_banner_code#7
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of bigint
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr4$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[ERROR] [2018-04-27 17:35:35] [Logging$class:logError:70] Task 1 in stage 1.0 failed 1 times; aborting job
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Cancelling stage 1
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Executor is trying to kill task 2.0 in stage 1.0 (TID 17), reason: stage cancelled
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Stage 1 was cancelled
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] ResultStage 1 (collect at HistoryDataToA.scala:113) failed in 0.349 s due to Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 16, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of bigint
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, promotion_code), StringType), true) AS promotion_code#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, promotion_good_id), StringType), true) AS promotion_good_id#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, promotion_shop_id), StringType), true) AS promotion_shop_id#2
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, promotion_type), StringType), true) AS promotion_type#3
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, promotion_start), LongType) AS promotion_start#4L
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, promotion_end), LongType) AS promotion_end#5L
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, promotion_banner), StringType), true) AS promotion_banner#6
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 7, promotion_banner_code), StringType), true) AS promotion_banner_code#7
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of bigint
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr4$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_1$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Executor interrupted and killed task 2.0 in stage 1.0 (TID 17), reason: stage cancelled
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Job 1 failed: collect at HistoryDataToA.scala:113, took 0.388767 s
[WARN ] [2018-04-27 17:35:35] [Logging$class:logWarning:66] Lost task 2.0 in stage 1.0 (TID 17, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:35:35] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-218ef9f2-9e28-4c01-ad71-46662c8a5288
[INFO ] [2018-04-27 17:39:38] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53289.
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-057375ae-e70b-4050-b132-5756e0bb7c42
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:39:39] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53298.
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Server created on 192.168.0.152:53298
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53298, None)
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53298 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53298, None)
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53298, None)
[INFO ] [2018-04-27 17:39:40] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53298, None)
[INFO ] [2018-04-27 17:39:41] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:39:41] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:39:41] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53298 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:39:41] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:216
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:216) with 15 output partitions
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:216)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211), which has no missing parents
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53298 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 434 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[ERROR] [2018-04-27 17:39:47] [Logging$class:logError:91] Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NumberFormatException: For input string: "1514822340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[ERROR] [2018-04-27 17:39:47] [Logging$class:logError:91] Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.NumberFormatException: For input string: "1524585540000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[WARN ] [2018-04-27 17:39:47] [Logging$class:logWarning:66] Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "1514822340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 17:39:47] [Logging$class:logError:91] Exception in task 3.0 in stage 0.0 (TID 3)
java.lang.NumberFormatException: For input string: "1459094340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] [2018-04-27 17:39:47] [Logging$class:logError:70] Task 1 in stage 0.0 failed 1 times; aborting job
[WARN ] [2018-04-27 17:39:47] [Logging$class:logWarning:66] Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NumberFormatException: For input string: "1524585540000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[WARN ] [2018-04-27 17:39:47] [Logging$class:logWarning:66] Lost task 3.0 in stage 0.0 (TID 3, localhost, executor driver): java.lang.NumberFormatException: For input string: "1459094340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Cancelling stage 0
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:216) failed in 0.609 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "1514822340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Job 0 failed: collect at posProductProcessor.scala:216, took 0.753629 s
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:39:47] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-30c31f11-1429-429c-99a7-09bd62a5e0f0
[INFO ] [2018-04-27 17:42:30] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53352.
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-648a2bbe-9021-4096-bedd-3488a1d597c2
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:42:31] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53361.
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Server created on 192.168.0.152:53361
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53361, None)
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53361 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53361, None)
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53361, None)
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53361, None)
[INFO ] [2018-04-27 17:42:32] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:42:33] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:42:33] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53361 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:42:33] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:216
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:216) with 15 output partitions
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:216)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211), which has no missing parents
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53361 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 350 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[ERROR] [2018-04-27 17:42:38] [Logging$class:logError:91] Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NumberFormatException: For input string: "1514822340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:42:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[WARN ] [2018-04-27 17:42:39] [Logging$class:logWarning:66] Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "1514822340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 17:42:39] [Logging$class:logError:70] Task 1 in stage 0.0 failed 1 times; aborting job
[ERROR] [2018-04-27 17:42:39] [Logging$class:logError:91] Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.NumberFormatException: For input string: "1524585540000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Cancelling stage 0
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Stage 0 was cancelled
[WARN ] [2018-04-27 17:42:39] [Logging$class:logWarning:66] Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NumberFormatException: For input string: "1524585540000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:216) failed in 0.492 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "1514822340000"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Job 0 failed: collect at posProductProcessor.scala:216, took 0.599749 s
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:42:39] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-7e21d51d-fefc-44af-9aec-5e66c4b55283
[INFO ] [2018-04-27 17:47:59] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:47:59] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:47:59] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:47:59] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:47:59] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:47:59] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:47:59] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53561.
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-4007c522-3acc-4c65-aa81-d7be33dab499
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53570.
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Server created on 192.168.0.152:53570
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53570, None)
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53570 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53570, None)
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53570, None)
[INFO ] [2018-04-27 17:48:00] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53570, None)
[INFO ] [2018-04-27 17:48:01] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:48:01] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:48:01] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53570 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:48:01] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:216
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:216) with 15 output partitions
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:216)
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211), which has no missing parents
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53570 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:48:06] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 802 bytes result sent to driver
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 388 ms on localhost (executor driver) (1/15)
[ERROR] [2018-04-27 17:48:07] [Logging$class:logError:91] Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NumberFormatException: For input string: "R10003"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[WARN ] [2018-04-27 17:48:07] [Logging$class:logWarning:66] Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "R10003"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 17:48:07] [Logging$class:logError:70] Task 1 in stage 0.0 failed 1 times; aborting job
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Cancelling stage 0
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Executor is trying to kill task 2.0 in stage 0.0 (TID 2), reason: stage cancelled
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Stage 0 was cancelled
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Executor killed task 2.0 in stage 0.0 (TID 2), reason: stage cancelled
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:216) failed in 0.491 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "R10003"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[WARN ] [2018-04-27 17:48:07] [Logging$class:logWarning:66] Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Job 0 failed: collect at posProductProcessor.scala:216, took 0.618849 s
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:48:07] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-339b6e18-3ed6-4f72-8bf3-ef702251f818
[INFO ] [2018-04-27 17:49:05] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53624.
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:49:06] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-192c3f87-44c1-4d4a-b43f-f39fff58c393
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53633.
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Server created on 192.168.0.152:53633
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53633, None)
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53633 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53633, None)
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53633, None)
[INFO ] [2018-04-27 17:49:07] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53633, None)
[INFO ] [2018-04-27 17:49:08] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:49:08] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:49:08] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53633 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:49:08] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:216
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:216) with 15 output partitions
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:216)
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211), which has no missing parents
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53633 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:211) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:49:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 759 bytes result sent to driver
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 341 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[ERROR] [2018-04-27 17:49:14] [Logging$class:logError:91] Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NumberFormatException: For input string: "1513872000000  "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[WARN ] [2018-04-27 17:49:14] [Logging$class:logWarning:66] Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "1513872000000  "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 17:49:14] [Logging$class:logError:70] Task 1 in stage 0.0 failed 1 times; aborting job
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Cancelling stage 0
[ERROR] [2018-04-27 17:49:14] [Logging$class:logError:91] Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.NumberFormatException: For input string: "1524326400000  "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Stage 0 was cancelled
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:216) failed in 0.465 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NumberFormatException: For input string: "1513872000000  "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[WARN ] [2018-04-27 17:49:14] [Logging$class:logWarning:66] Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): java.lang.NumberFormatException: For input string: "1524326400000  "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:214)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:211)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Job 0 failed: collect at posProductProcessor.scala:216, took 0.582872 s
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:49:14] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-267dc758-e315-43d7-9c98-7224c52d96f2
[INFO ] [2018-04-27 17:56:25] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 17:56:25] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 17:56:25] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 17:56:25] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 17:56:25] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 17:56:25] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 17:56:25] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53738.
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-48de7f52-788c-4ce1-8a85-83097075d959
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53747.
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Server created on 192.168.0.152:53747
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53747, None)
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53747 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53747, None)
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53747, None)
[INFO ] [2018-04-27 17:56:26] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53747, None)
[INFO ] [2018-04-27 17:56:27] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:56:27] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:56:27] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53747 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:56:27] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:225
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:225) with 15 output partitions
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:225)
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:220), which has no missing parents
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53747 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:220) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 17:56:32] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[ERROR] [2018-04-27 17:56:33] [Logging$class:logError:91] Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 19
	at com.dr.banner.posProductProcessor$$anonfun$12$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:157)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:155)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:152)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[WARN ] [2018-04-27 17:56:33] [Logging$class:logWarning:66] Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 19
	at com.dr.banner.posProductProcessor$$anonfun$12$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:157)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:155)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:152)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 17:56:33] [Logging$class:logError:70] Task 0 in stage 0.0 failed 1 times; aborting job
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[ERROR] [2018-04-27 17:56:33] [Logging$class:logError:91] Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.ArrayIndexOutOfBoundsException: 48
	at com.dr.banner.posProductProcessor$$anonfun$12$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:157)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:155)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:152)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Cancelling stage 0
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Stage 0 was cancelled
[WARN ] [2018-04-27 17:56:33] [Logging$class:logWarning:66] Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 48
	at com.dr.banner.posProductProcessor$$anonfun$12$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:157)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:155)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:152)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:225) failed in 0.378 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 19
	at com.dr.banner.posProductProcessor$$anonfun$12$$anonfun$apply$1.apply$mcVI$sp(posProductProcessor.scala:157)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:155)
	at com.dr.banner.posProductProcessor$$anonfun$12.apply(posProductProcessor.scala:152)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Job 0 failed: collect at posProductProcessor.scala:225, took 0.481434 s
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 17:56:33] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-aa3c6082-bddc-44cc-9158-7b01c6abb56e
[INFO ] [2018-04-27 18:00:20] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 18:00:21] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 18:00:21] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 18:00:21] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 18:00:21] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 18:00:21] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 18:00:21] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53816.
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-1b22e247-3aaa-4142-8cf5-a7df2a97f2fc
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53825.
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Server created on 192.168.0.152:53825
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53825, None)
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53825 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53825, None)
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53825, None)
[INFO ] [2018-04-27 18:00:22] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53825, None)
[INFO ] [2018-04-27 18:00:23] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:00:23] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:00:23] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53825 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:23] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:225
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:225) with 15 output partitions
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:225)
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:220), which has no missing parents
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53825 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:220) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 18:00:28] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 18:00:29] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 845 bytes result sent to driver
[INFO ] [2018-04-27 18:00:29] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:00:29] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 18:00:29] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 364 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 18:00:29] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 18:00:32] [Logging$class:logInfo:54] Block taskresult_1 stored as bytes in memory (estimated size 9.0 MB, free 1982.7 MB)
[INFO ] [2018-04-27 18:00:32] [Logging$class:logInfo:54] Added taskresult_1 in memory on 192.168.0.152:53825 (size: 9.0 MB, free: 1983.0 MB)
[INFO ] [2018-04-27 18:00:32] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 9456761 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:00:32] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:00:32] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 18:00:32] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 18:00:32] [TransportClientFactory:createClient:254] Successfully created connection to /192.168.0.152:53825 after 54 ms (0 ms spent in bootstraps)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 3793 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Removed taskresult_1 on 192.168.0.152:53825 in memory (size: 9.0 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Block taskresult_2 stored as bytes in memory (estimated size 1275.7 KB, free 1990.5 MB)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Added taskresult_2 in memory on 192.168.0.152:53825 (size: 1275.7 KB, free: 1990.7 MB)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1306351 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 1080 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 18:00:33] [Logging$class:logInfo:54] Removed taskresult_2 on 192.168.0.152:53825 in memory (size: 1275.7 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Block taskresult_3 stored as bytes in memory (estimated size 7.3 MB, free 1984.5 MB)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Added taskresult_3 in memory on 192.168.0.152:53825 (size: 7.3 MB, free: 1984.7 MB)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 7639303 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 802 bytes result sent to driver
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 93 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 18:00:34] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 1991 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Removed taskresult_3 on 192.168.0.152:53825 in memory (size: 7.3 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 802 bytes result sent to driver
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 167 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 15 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 802 bytes result sent to driver
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 211 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 802 bytes result sent to driver
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 141 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 404 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 18:00:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 498 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 802 bytes result sent to driver
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 21 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 95 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 106 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 61 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:225) finished in 7.772 s
[INFO ] [2018-04-27 18:00:36] [Logging$class:logInfo:54] Job 0 finished: collect at posProductProcessor.scala:225, took 7.894441 s
[INFO ] [2018-04-27 18:00:39] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 18:00:39] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 18:00:39] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 18:00:39] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 18:00:42] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:53825 in memory (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:42] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Code generated in 198.004104 ms
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:113
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Got job 1 (collect at HistoryDataToA.scala:113) with 15 output partitions
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Final stage: ResultStage 1 (collect at HistoryDataToA.scala:113)
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113), which has no missing parents
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:53825 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Code generated in 19.190579 ms
[INFO ] [2018-04-27 18:00:43] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:44] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:00:44] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 18:00:44] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 18:00:44] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 69 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 18:00:44] [Logging$class:logInfo:54] Code generated in 91.238033 ms
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Block taskresult_16 stored as bytes in memory (estimated size 1092.6 KB, free 1990.7 MB)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Added taskresult_16 in memory on 192.168.0.152:53825 (size: 1092.6 KB, free: 1990.9 MB)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1118840 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 2121 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Removed taskresult_16 on 192.168.0.152:53825 in memory (size: 1092.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 84008 bytes result sent to driver
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 253 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 18:00:46] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 879230 bytes result sent to driver
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 1470 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 61 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 18:00:47] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 109 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1023 bytes result sent to driver
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 11 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 158 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 164 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 1109 bytes result sent to driver
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 470 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 18:00:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 458 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 1109 bytes result sent to driver
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 27 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 95 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 114 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 59 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] ResultStage 1 (collect at HistoryDataToA.scala:113) finished in 5.615 s
[INFO ] [2018-04-27 18:00:49] [Logging$class:logInfo:54] Job 1 finished: collect at HistoryDataToA.scala:113, took 5.630217 s
[INFO ] [2018-04-27 18:00:50] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:119
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:119) with 15 output partitions
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Final stage: ResultStage 2 (saveAsTable at HistoryDataToA.scala:119)
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Submitting ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:119), which has no missing parents
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 81.5 KB, free 1991.7 MB)
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 31.0 KB, free 1991.6 MB)
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:53825 (size: 31.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Adding task set 2.0 with 15 tasks
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 30, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 30)
[INFO ] [2018-04-27 18:00:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Code generated in 16.262435 ms
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Code generated in 12.450749 ms
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Code generated in 16.025691 ms
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Code generated in 21.705649 ms
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Code generated in 18.980266 ms
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180052_0002_m_000000_0
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30). 1839 bytes result sent to driver
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Starting task 1.0 in stage 2.0 (TID 31, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30) in 207 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Running task 1.0 in stage 2.0 (TID 31)
[INFO ] [2018-04-27 18:00:52] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 18:00:54] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] attempt_20180427180054_0002_m_000001_0: Committed
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31). 1878 bytes result sent to driver
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Starting task 2.0 in stage 2.0 (TID 32, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Running task 2.0 in stage 2.0 (TID 32)
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31) in 3389 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Removed broadcast_2_piece0 on 192.168.0.152:53825 in memory (size: 6.3 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:00:56] [Logging$class:logInfo:54] attempt_20180427180055_0002_m_000002_0: Committed
[INFO ] [2018-04-27 18:00:56] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32). 1835 bytes result sent to driver
[INFO ] [2018-04-27 18:00:56] [Logging$class:logInfo:54] Starting task 3.0 in stage 2.0 (TID 33, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 18:00:56] [Logging$class:logInfo:54] Running task 3.0 in stage 2.0 (TID 33)
[INFO ] [2018-04-27 18:00:56] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32) in 516 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 18:00:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] attempt_20180427180057_0002_m_000003_0: Committed
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33). 1835 bytes result sent to driver
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Starting task 4.0 in stage 2.0 (TID 34, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Running task 4.0 in stage 2.0 (TID 34)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33) in 1639 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180057_0002_m_000004_0
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Starting task 5.0 in stage 2.0 (TID 35, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34) in 101 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Running task 5.0 in stage 2.0 (TID 35)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180057_0002_m_000005_0
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35). 1839 bytes result sent to driver
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Starting task 6.0 in stage 2.0 (TID 36, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Running task 6.0 in stage 2.0 (TID 36)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35) in 154 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180057_0002_m_000006_0
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36). 1710 bytes result sent to driver
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Starting task 7.0 in stage 2.0 (TID 37, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Running task 7.0 in stage 2.0 (TID 37)
[INFO ] [2018-04-27 18:00:57] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36) in 26 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180058_0002_m_000007_0
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37). 1796 bytes result sent to driver
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Starting task 8.0 in stage 2.0 (TID 38, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Running task 8.0 in stage 2.0 (TID 38)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37) in 169 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180058_0002_m_000008_0
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Starting task 9.0 in stage 2.0 (TID 39, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Running task 9.0 in stage 2.0 (TID 39)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38) in 155 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180058_0002_m_000009_0
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Starting task 10.0 in stage 2.0 (TID 40, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Running task 10.0 in stage 2.0 (TID 40)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39) in 437 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 18:00:58] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180059_0002_m_000010_0
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40). 1796 bytes result sent to driver
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Starting task 11.0 in stage 2.0 (TID 41, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40) in 476 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Running task 11.0 in stage 2.0 (TID 41)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180059_0002_m_000011_0
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41). 1796 bytes result sent to driver
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Starting task 12.0 in stage 2.0 (TID 42, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Running task 12.0 in stage 2.0 (TID 42)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41) in 33 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180059_0002_m_000012_0
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42). 1796 bytes result sent to driver
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Starting task 13.0 in stage 2.0 (TID 43, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Running task 13.0 in stage 2.0 (TID 43)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42) in 105 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180059_0002_m_000013_0
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Starting task 14.0 in stage 2.0 (TID 44, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Running task 14.0 in stage 2.0 (TID 44)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43) in 122 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180059_0002_m_000014_0
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44) in 73 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] ResultStage 2 (saveAsTable at HistoryDataToA.scala:119) finished in 7.588 s
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Job 2 finished: saveAsTable at HistoryDataToA.scala:119, took 7.613480 s
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 18:00:59] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Found 1 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Finished to gather the fast stats for all 1 partitions.
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Recovered all partitions (1).
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 18:01:00] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-e28866b0-a61a-4bae-9ce8-b9243f6fe8e5
[INFO ] [2018-04-27 18:06:24] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 18:06:24] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 18:06:24] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 18:06:24] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 18:06:24] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 18:06:24] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 18:06:24] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53925.
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-3d5e9b3f-7972-4365-bb1f-4bcb190da614
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53934.
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Server created on 192.168.0.152:53934
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53934, None)
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53934 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53934, None)
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53934, None)
[INFO ] [2018-04-27 18:06:25] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53934, None)
[INFO ] [2018-04-27 18:06:26] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:06:26] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:06:26] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53934 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:06:26] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:223
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:223) with 15 output partitions
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:223)
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:218), which has no missing parents
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53934 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:218) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 18:06:31] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 18:06:32] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 18:06:32] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 212294 bytes result sent to driver
[INFO ] [2018-04-27 18:06:32] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:06:32] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 18:06:32] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 18:06:32] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 702 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Block taskresult_1 stored as bytes in memory (estimated size 9.0 MB, free 1982.7 MB)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Added taskresult_1 in memory on 192.168.0.152:53934 (size: 9.0 MB, free: 1983.0 MB)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 9456718 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 18:06:35] [TransportClientFactory:createClient:254] Successfully created connection to /192.168.0.152:53934 after 28 ms (0 ms spent in bootstraps)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 3171 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Removed taskresult_1 on 192.168.0.152:53934 in memory (size: 9.0 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Block taskresult_2 stored as bytes in memory (estimated size 1275.7 KB, free 1990.5 MB)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Added taskresult_2 in memory on 192.168.0.152:53934 (size: 1275.7 KB, free: 1990.7 MB)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1306351 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 686 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 18:06:35] [Logging$class:logInfo:54] Removed taskresult_2 on 192.168.0.152:53934 in memory (size: 1275.7 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:06:37] [Logging$class:logInfo:54] Block taskresult_3 stored as bytes in memory (estimated size 7.3 MB, free 1984.5 MB)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Added taskresult_3 in memory on 192.168.0.152:53934 (size: 7.3 MB, free: 1984.7 MB)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 7639260 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 845 bytes result sent to driver
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 185 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 2533 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Removed taskresult_3 on 192.168.0.152:53934 in memory (size: 7.3 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 164 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 11 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 172 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 18:06:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Block taskresult_8 stored as bytes in memory (estimated size 3.5 MB, free 1988.3 MB)
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Added taskresult_8 in memory on 192.168.0.152:53934 (size: 3.5 MB, free: 1988.5 MB)
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 3623692 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 865 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 18:06:39] [Logging$class:logInfo:54] Removed taskresult_8 on 192.168.0.152:53934 in memory (size: 3.5 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Block taskresult_9 stored as bytes in memory (estimated size 10.0 MB, free 1981.7 MB)
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Added taskresult_9 in memory on 192.168.0.152:53934 (size: 10.0 MB, free: 1982.0 MB)
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 10512126 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 2431 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 18:06:41] [Logging$class:logInfo:54] Removed taskresult_9 on 192.168.0.152:53934 in memory (size: 10.0 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:06:44] [Logging$class:logInfo:54] Block taskresult_10 stored as bytes in memory (estimated size 12.6 MB, free 1979.1 MB)
[INFO ] [2018-04-27 18:06:44] [Logging$class:logInfo:54] Added taskresult_10 in memory on 192.168.0.152:53934 (size: 12.6 MB, free: 1979.3 MB)
[INFO ] [2018-04-27 18:06:44] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 13243222 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:06:44] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:06:44] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 18:06:44] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 426799 bytes result sent to driver
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 133 ms on localhost (executor driver) (11/15)
[ERROR] [2018-04-27 18:06:45] [Logging$class:logError:91] Exception in task 12.0 in stage 0.0 (TID 12)
java.lang.NumberFormatException: For input string: "1443283140000 "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:221)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:218)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[WARN ] [2018-04-27 18:06:45] [Logging$class:logWarning:66] Lost task 12.0 in stage 0.0 (TID 12, localhost, executor driver): java.lang.NumberFormatException: For input string: "1443283140000 "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:221)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:218)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 18:06:45] [Logging$class:logError:70] Task 12 in stage 0.0 failed 1 times; aborting job
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Cancelling stage 0
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Stage 0 was cancelled
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:223) failed in 13.208 s due to Job aborted due to stage failure: Task 12 in stage 0.0 failed 1 times, most recent failure: Lost task 12.0 in stage 0.0 (TID 12, localhost, executor driver): java.lang.NumberFormatException: For input string: "1443283140000 "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:276)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:29)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:221)
	at com.dr.banner.posProductProcessor$$anonfun$15.apply(posProductProcessor.scala:218)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Executor is trying to kill task 13.0 in stage 0.0 (TID 13), reason: stage cancelled
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Executor killed task 13.0 in stage 0.0 (TID 13), reason: stage cancelled
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Job 0 failed: collect at posProductProcessor.scala:223, took 13.472157 s
[WARN ] [2018-04-27 18:06:45] [Logging$class:logWarning:66] Lost task 13.0 in stage 0.0 (TID 13, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-c8c39890-0ac2-4e76-8844-0188a37ca4a5
[ERROR] [2018-04-27 18:06:45] [Logging$class:logError:91] Exception while getting task result
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.removeBlock(BlockManagerMaster.scala:115)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:95)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 18:06:45] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:08:30] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 53983.
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-654c7774-963c-4fbf-9239-a3dd161cb5f9
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 18:08:31] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53992.
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Server created on 192.168.0.152:53992
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 53992, None)
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:53992 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 53992, None)
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 53992, None)
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 53992, None)
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:53992 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:32] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Starting job: collect at posProductProcessor.scala:223
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Got job 0 (collect at posProductProcessor.scala:223) with 15 output partitions
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Final stage: ResultStage 0 (collect at posProductProcessor.scala:223)
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:218), which has no missing parents
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:53992 (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at posProductProcessor.scala:218) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Adding task set 0.0 with 15 tasks
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 18:08:38] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 18:08:39] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 212294 bytes result sent to driver
[INFO ] [2018-04-27 18:08:39] [Logging$class:logInfo:54] Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:08:39] [Logging$class:logInfo:54] Running task 1.0 in stage 0.0 (TID 1)
[INFO ] [2018-04-27 18:08:39] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 18:08:39] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 787 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 18:08:41] [Logging$class:logInfo:54] Block taskresult_1 stored as bytes in memory (estimated size 9.0 MB, free 1982.7 MB)
[INFO ] [2018-04-27 18:08:41] [Logging$class:logInfo:54] Added taskresult_1 in memory on 192.168.0.152:53992 (size: 9.0 MB, free: 1983.0 MB)
[INFO ] [2018-04-27 18:08:41] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1). 9456718 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:08:41] [Logging$class:logInfo:54] Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:08:41] [Logging$class:logInfo:54] Running task 2.0 in stage 0.0 (TID 2)
[INFO ] [2018-04-27 18:08:41] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 18:08:41] [TransportClientFactory:createClient:254] Successfully created connection to /192.168.0.152:53992 after 32 ms (0 ms spent in bootstraps)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Finished task 1.0 in stage 0.0 (TID 1) in 3388 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Removed taskresult_1 on 192.168.0.152:53992 in memory (size: 9.0 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Block taskresult_2 stored as bytes in memory (estimated size 1275.7 KB, free 1990.5 MB)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Added taskresult_2 in memory on 192.168.0.152:53992 (size: 1275.7 KB, free: 1990.7 MB)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2). 1306351 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Running task 3.0 in stage 0.0 (TID 3)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Finished task 2.0 in stage 0.0 (TID 2) in 873 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 18:08:42] [Logging$class:logInfo:54] Removed taskresult_2 on 192.168.0.152:53992 in memory (size: 1275.7 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Block taskresult_3 stored as bytes in memory (estimated size 7.3 MB, free 1984.5 MB)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Added taskresult_3 in memory on 192.168.0.152:53992 (size: 7.3 MB, free: 1984.7 MB)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3). 7639260 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Running task 4.0 in stage 0.0 (TID 4)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Running task 5.0 in stage 0.0 (TID 5)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 4.0 in stage 0.0 (TID 4) in 99 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 3.0 in stage 0.0 (TID 3) in 2024 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Removed taskresult_3 on 192.168.0.152:53992 in memory (size: 7.3 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5). 845 bytes result sent to driver
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 5.0 in stage 0.0 (TID 5) in 262 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Running task 6.0 in stage 0.0 (TID 6)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Running task 7.0 in stage 0.0 (TID 7)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 6.0 in stage 0.0 (TID 6) in 14 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Running task 8.0 in stage 0.0 (TID 8)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Finished task 7.0 in stage 0.0 (TID 7) in 150 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 18:08:44] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Block taskresult_8 stored as bytes in memory (estimated size 3.5 MB, free 1988.3 MB)
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Added taskresult_8 in memory on 192.168.0.152:53992 (size: 3.5 MB, free: 1988.5 MB)
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8). 3623735 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Running task 9.0 in stage 0.0 (TID 9)
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Finished task 8.0 in stage 0.0 (TID 8) in 877 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 18:08:45] [Logging$class:logInfo:54] Removed taskresult_8 on 192.168.0.152:53992 in memory (size: 3.5 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:47] [Logging$class:logInfo:54] Block taskresult_9 stored as bytes in memory (estimated size 10.0 MB, free 1981.7 MB)
[INFO ] [2018-04-27 18:08:47] [Logging$class:logInfo:54] Added taskresult_9 in memory on 192.168.0.152:53992 (size: 10.0 MB, free: 1982.0 MB)
[INFO ] [2018-04-27 18:08:47] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9). 10512126 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:08:47] [Logging$class:logInfo:54] Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:08:47] [Logging$class:logInfo:54] Running task 10.0 in stage 0.0 (TID 10)
[INFO ] [2018-04-27 18:08:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 18:08:48] [Logging$class:logInfo:54] Finished task 9.0 in stage 0.0 (TID 9) in 2446 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 18:08:48] [Logging$class:logInfo:54] Removed taskresult_9 on 192.168.0.152:53992 in memory (size: 10.0 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Block taskresult_10 stored as bytes in memory (estimated size 12.6 MB, free 1979.1 MB)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Added taskresult_10 in memory on 192.168.0.152:53992 (size: 12.6 MB, free: 1979.3 MB)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10). 13243222 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Running task 11.0 in stage 0.0 (TID 11)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11). 426799 bytes result sent to driver
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Running task 12.0 in stage 0.0 (TID 12)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 11.0 in stage 0.0 (TID 11) in 150 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Removed taskresult_10 on 192.168.0.152:53992 in memory (size: 12.6 MB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 10.0 in stage 0.0 (TID 10) in 3787 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Block taskresult_12 stored as bytes in memory (estimated size 1747.6 KB, free 1990.1 MB)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Added taskresult_12 in memory on 192.168.0.152:53992 (size: 1747.6 KB, free: 1990.3 MB)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12). 1789493 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Running task 13.0 in stage 0.0 (TID 13)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 12.0 in stage 0.0 (TID 12) in 497 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Removed taskresult_12 on 192.168.0.152:53992 in memory (size: 1747.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 13.0 in stage 0.0 (TID 13) in 124 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Running task 14.0 in stage 0.0 (TID 14)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14). 759 bytes result sent to driver
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Finished task 14.0 in stage 0.0 (TID 14) in 55 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] ResultStage 0 (collect at posProductProcessor.scala:223) finished in 13.569 s
[INFO ] [2018-04-27 18:08:51] [Logging$class:logInfo:54] Job 0 finished: collect at posProductProcessor.scala:223, took 13.680221 s
[INFO ] [2018-04-27 18:08:56] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 18:08:56] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 18:08:56] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 18:08:56] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 18:08:59] [Logging$class:logInfo:54] Removed broadcast_1_piece0 on 192.168.0.152:53992 in memory (size: 2.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:09:00] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:09:00] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:09:00] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Code generated in 229.255391 ms
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Starting job: collect at HistoryDataToA.scala:113
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Got job 1 (collect at HistoryDataToA.scala:113) with 15 output partitions
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Final stage: ResultStage 1 (collect at HistoryDataToA.scala:113)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Submitting ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113), which has no missing parents
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 13.9 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:53992 (size: 6.3 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at collect at HistoryDataToA.scala:113) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Adding task set 1.0 with 15 tasks
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 15, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 15)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Code generated in 22.185555 ms
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Code generated in 131.066083 ms
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15). 14054 bytes result sent to driver
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Starting task 1.0 in stage 1.0 (TID 16, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Running task 1.0 in stage 1.0 (TID 16)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 15) in 385 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 18:09:01] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Block taskresult_16 stored as bytes in memory (estimated size 1092.6 KB, free 1990.7 MB)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Added taskresult_16 in memory on 192.168.0.152:53992 (size: 1092.6 KB, free: 1990.9 MB)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16). 1118840 bytes result sent via BlockManager)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Starting task 2.0 in stage 1.0 (TID 17, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Running task 2.0 in stage 1.0 (TID 17)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Finished task 1.0 in stage 1.0 (TID 16) in 1909 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Removed taskresult_16 on 192.168.0.152:53992 in memory (size: 1092.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17). 84051 bytes result sent to driver
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Starting task 3.0 in stage 1.0 (TID 18, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Running task 3.0 in stage 1.0 (TID 18)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Finished task 2.0 in stage 1.0 (TID 17) in 275 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 18:09:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18). 879187 bytes result sent to driver
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Starting task 4.0 in stage 1.0 (TID 19, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Running task 4.0 in stage 1.0 (TID 19)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 3.0 in stage 1.0 (TID 18) in 1494 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Starting task 5.0 in stage 1.0 (TID 20, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Running task 5.0 in stage 1.0 (TID 20)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 4.0 in stage 1.0 (TID 19) in 62 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Starting task 6.0 in stage 1.0 (TID 21, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Running task 6.0 in stage 1.0 (TID 21)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 5.0 in stage 1.0 (TID 20) in 111 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21). 1023 bytes result sent to driver
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Starting task 7.0 in stage 1.0 (TID 22, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Running task 7.0 in stage 1.0 (TID 22)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 6.0 in stage 1.0 (TID 21) in 10 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22). 1066 bytes result sent to driver
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Starting task 8.0 in stage 1.0 (TID 23, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Finished task 7.0 in stage 1.0 (TID 22) in 181 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Running task 8.0 in stage 1.0 (TID 23)
[INFO ] [2018-04-27 18:09:05] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 18:09:06] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23). 227052 bytes result sent to driver
[INFO ] [2018-04-27 18:09:06] [Logging$class:logInfo:54] Starting task 9.0 in stage 1.0 (TID 24, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:09:06] [Logging$class:logInfo:54] Running task 9.0 in stage 1.0 (TID 24)
[INFO ] [2018-04-27 18:09:06] [Logging$class:logInfo:54] Finished task 8.0 in stage 1.0 (TID 23) in 675 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 18:09:06] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 18:09:08] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24). 698842 bytes result sent to driver
[INFO ] [2018-04-27 18:09:08] [Logging$class:logInfo:54] Starting task 10.0 in stage 1.0 (TID 25, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:09:08] [Logging$class:logInfo:54] Running task 10.0 in stage 1.0 (TID 25)
[INFO ] [2018-04-27 18:09:08] [Logging$class:logInfo:54] Finished task 9.0 in stage 1.0 (TID 24) in 1810 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 18:09:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25). 897082 bytes result sent to driver
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Starting task 11.0 in stage 1.0 (TID 26, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Running task 11.0 in stage 1.0 (TID 26)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 10.0 in stage 1.0 (TID 25) in 2197 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26). 31644 bytes result sent to driver
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Starting task 12.0 in stage 1.0 (TID 27, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 11.0 in stage 1.0 (TID 26) in 81 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Running task 12.0 in stage 1.0 (TID 27)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27). 129817 bytes result sent to driver
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Starting task 13.0 in stage 1.0 (TID 28, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Running task 13.0 in stage 1.0 (TID 28)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 12.0 in stage 1.0 (TID 27) in 336 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28). 1109 bytes result sent to driver
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Starting task 14.0 in stage 1.0 (TID 29, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Running task 14.0 in stage 1.0 (TID 29)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 13.0 in stage 1.0 (TID 28) in 112 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29). 1109 bytes result sent to driver
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Finished task 14.0 in stage 1.0 (TID 29) in 53 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] ResultStage 1 (collect at HistoryDataToA.scala:113) finished in 9.667 s
[INFO ] [2018-04-27 18:09:10] [Logging$class:logInfo:54] Job 1 finished: collect at HistoryDataToA.scala:113, took 9.684783 s
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: ba_model.banner_promotion
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:14] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:119
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Got job 2 (saveAsTable at HistoryDataToA.scala:119) with 15 output partitions
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Final stage: ResultStage 2 (saveAsTable at HistoryDataToA.scala:119)
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Submitting ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:119), which has no missing parents
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 81.5 KB, free 1991.7 MB)
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 31.0 KB, free 1991.6 MB)
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:53992 (size: 31.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Submitting 15 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at HistoryDataToA.scala:119) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Adding task set 2.0 with 15 tasks
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 30, localhost, executor driver, partition 0, ANY, 4928 bytes)
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 30)
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/全场总额优惠活动促销商品详细信息.txt:0+380868
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Code generated in 15.691909 ms
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Code generated in 25.208095 ms
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Code generated in 11.973863 ms
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Code generated in 30.766094 ms
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Code generated in 10.456741 ms
[INFO ] [2018-04-27 18:09:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:16] [Logging$class:logInfo:54] attempt_20180427180915_0002_m_000000_0: Committed
[INFO ] [2018-04-27 18:09:16] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30). 1878 bytes result sent to driver
[INFO ] [2018-04-27 18:09:16] [Logging$class:logInfo:54] Starting task 1.0 in stage 2.0 (TID 31, localhost, executor driver, partition 1, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:09:16] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 30) in 1253 ms on localhost (executor driver) (1/15)
[INFO ] [2018-04-27 18:09:16] [Logging$class:logInfo:54] Running task 1.0 in stage 2.0 (TID 31)
[INFO ] [2018-04-27 18:09:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20170101—20171231.txt:0+24859329
[INFO ] [2018-04-27 18:09:18] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:18] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:18] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] attempt_20180427180918_0002_m_000001_0: Committed
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31). 1878 bytes result sent to driver
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Starting task 2.0 in stage 2.0 (TID 32, localhost, executor driver, partition 2, ANY, 4935 bytes)
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Finished task 1.0 in stage 2.0 (TID 31) in 2428 ms on localhost (executor driver) (2/15)
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Running task 2.0 in stage 2.0 (TID 32)
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表20180101—20180424.txt:0+3542719
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] attempt_20180427180919_0002_m_000002_0: Committed
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32). 1792 bytes result sent to driver
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Starting task 3.0 in stage 2.0 (TID 33, localhost, executor driver, partition 3, ANY, 4925 bytes)
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Running task 3.0 in stage 2.0 (TID 33)
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Finished task 2.0 in stage 2.0 (TID 32) in 422 ms on localhost (executor driver) (3/15)
[INFO ] [2018-04-27 18:09:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/售价、进价促销商品明细表_20161231.txt:0+19752159
[INFO ] [2018-04-27 18:09:20] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:20] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:20] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] attempt_20180427180920_0002_m_000003_0: Committed
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33). 1835 bytes result sent to driver
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Starting task 4.0 in stage 2.0 (TID 34, localhost, executor driver, partition 4, ANY, 4922 bytes)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Running task 4.0 in stage 2.0 (TID 34)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 3.0 in stage 2.0 (TID 33) in 1552 ms on localhost (executor driver) (4/15)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品总额优惠促销商品详细信息.txt:0+3385742
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180921_0002_m_000004_0
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Starting task 5.0 in stage 2.0 (TID 35, localhost, executor driver, partition 5, ANY, 4895 bytes)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 4.0 in stage 2.0 (TID 34) in 76 ms on localhost (executor driver) (5/15)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Running task 5.0 in stage 2.0 (TID 35)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/商品档案表.txt:0+5619732
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180921_0002_m_000005_0
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35). 1796 bytes result sent to driver
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Starting task 6.0 in stage 2.0 (TID 36, localhost, executor driver, partition 6, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Running task 6.0 in stage 2.0 (TID 36)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 5.0 in stage 2.0 (TID 35) in 164 ms on localhost (executor driver) (6/15)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/安达便利门店信息表.txt:0+9224
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180921_0002_m_000006_0
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Starting task 7.0 in stage 2.0 (TID 37, localhost, executor driver, partition 7, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 6.0 in stage 2.0 (TID 36) in 16 ms on localhost (executor driver) (7/15)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Running task 7.0 in stage 2.0 (TID 37)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180921_0002_m_000007_0
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37). 1796 bytes result sent to driver
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Starting task 8.0 in stage 2.0 (TID 38, localhost, executor driver, partition 8, ANY, 4915 bytes)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Running task 8.0 in stage 2.0 (TID 38)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Finished task 7.0 in stage 2.0 (TID 37) in 191 ms on localhost (executor driver) (8/15)
[INFO ] [2018-04-27 18:09:21] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20151231.txt:0+8176619
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] attempt_20180427180922_0002_m_000008_0: Committed
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38). 1835 bytes result sent to driver
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Starting task 9.0 in stage 2.0 (TID 39, localhost, executor driver, partition 9, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Running task 9.0 in stage 2.0 (TID 39)
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Finished task 8.0 in stage 2.0 (TID 38) in 789 ms on localhost (executor driver) (9/15)
[INFO ] [2018-04-27 18:09:22] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20160701—20161231.txt:0+24288418
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] attempt_20180427180924_0002_m_000009_0: Committed
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39). 1835 bytes result sent to driver
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Starting task 10.0 in stage 2.0 (TID 40, localhost, executor driver, partition 10, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Running task 10.0 in stage 2.0 (TID 40)
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Finished task 9.0 in stage 2.0 (TID 39) in 1941 ms on localhost (executor driver) (10/15)
[INFO ] [2018-04-27 18:09:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20170101—20171231.txt:0+29747358
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] attempt_20180427180926_0002_m_000010_0: Committed
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40). 1835 bytes result sent to driver
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Starting task 11.0 in stage 2.0 (TID 41, localhost, executor driver, partition 11, ANY, 4926 bytes)
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Running task 11.0 in stage 2.0 (TID 41)
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Finished task 10.0 in stage 2.0 (TID 40) in 2433 ms on localhost (executor driver) (11/15)
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/等级优惠商品明细表20180101—20180424.txt:0+1007100
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] attempt_20180427180926_0002_m_000011_0: Committed
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41). 1878 bytes result sent to driver
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Starting task 12.0 in stage 2.0 (TID 42, localhost, executor driver, partition 12, ANY, 4907 bytes)
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Running task 12.0 in stage 2.0 (TID 42)
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Finished task 11.0 in stage 2.0 (TID 41) in 223 ms on localhost (executor driver) (12/15)
[INFO ] [2018-04-27 18:09:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合优惠商品明细表.txt:0+5532218
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "promotion_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_good_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_shop_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_end",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "promotion_banner",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary promotion_code (UTF8);
  optional binary promotion_good_id (UTF8);
  optional binary promotion_shop_id (UTF8);
  optional binary promotion_type (UTF8);
  optional int64 promotion_start;
  optional int64 promotion_end;
  optional binary promotion_banner (UTF8);
}

       
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] attempt_20180427180927_0002_m_000012_0: Committed
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42). 1792 bytes result sent to driver
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Starting task 13.0 in stage 2.0 (TID 43, localhost, executor driver, partition 13, ANY, 4892 bytes)
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Running task 13.0 in stage 2.0 (TID 43)
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Finished task 12.0 in stage 2.0 (TID 42) in 467 ms on localhost (executor driver) (13/15)
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/组合商品.txt:0+6682758
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180927_0002_m_000013_0
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43). 1796 bytes result sent to driver
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Starting task 14.0 in stage 2.0 (TID 44, localhost, executor driver, partition 14, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Running task 14.0 in stage 2.0 (TID 44)
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Finished task 13.0 in stage 2.0 (TID 43) in 128 ms on localhost (executor driver) (14/15)
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] No need to commit output of task because needsTaskCommit=false: attempt_20180427180927_0002_m_000014_0
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44). 1753 bytes result sent to driver
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Finished task 14.0 in stage 2.0 (TID 44) in 75 ms on localhost (executor driver) (15/15)
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] ResultStage 2 (saveAsTable at HistoryDataToA.scala:119) finished in 12.147 s
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Job 2 finished: saveAsTable at HistoryDataToA.scala:119, took 12.177377 s
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 18:09:27] [Logging$class:logInfo:54] Persisting file based data source table `ba_model`.`banner_promotion` into Hive metastore in Hive compatible format.
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Recover all the partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Found 1 partitions in hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/banner_promotion
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Finished to gather the fast stats for all 1 partitions.
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Recovered all partitions (1).
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 18:09:28] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-23939b5f-6634-416e-875e-37b455dfe6f8
[INFO ] [2018-04-27 18:19:57] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 54144.
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-a6085bfe-deaf-41f8-829a-da97dbf97c0a
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 18:19:58] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54153.
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Server created on 192.168.0.152:54153
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 54153, None)
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:54153 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 54153, None)
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 54153, None)
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 54153, None)
[INFO ] [2018-04-27 18:19:59] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:20:00] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:20:00] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:54153 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:20:00] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:20:00] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 18:20:00] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 18:20:00] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:54153 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:20:00] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:20:02] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 18:20:02] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 18:20:02] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 18:20:02] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 18:20:06] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:20:06] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:20:06] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:07] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:08] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:12] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:20:13] [Logging$class:logInfo:54] Code generated in 244.763039 ms
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:54153 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 11.9 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:54153 (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Code generated in 11.395031 ms
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 73022 bytes result sent to driver
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 429 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.447 s
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.520401 s
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Code generated in 8.426864 ms
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 204.8 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:54153 (size: 204.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Code generated in 26.587778 ms
[INFO ] [2018-04-27 18:20:14] [Logging$class:logInfo:54] Code generated in 16.627178 ms
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:54153 in memory (size: 6.0 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 12.3 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:54153 (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 17.3 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:54153 (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Code generated in 14.520273 ms
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Code generated in 15.35133 ms
[INFO ] [2018-04-27 18:20:15] [Logging$class:logInfo:54] Code generated in 77.050787 ms
[ERROR] [2018-04-27 18:20:16] [Logging$class:logError:91] Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, flow_no_m), StringType), true) AS flow_no_m#40
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, pay_type), StringType), true) AS pay_type#41
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, is_useful), IntegerType) AS is_useful#42
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr2$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 13 more
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[WARN ] [2018-04-27 18:20:16] [Logging$class:logWarning:66] Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, flow_no_m), StringType), true) AS flow_no_m#40
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, pay_type), StringType), true) AS pay_type#41
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, is_useful), IntegerType) AS is_useful#42
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr2$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 13 more

[ERROR] [2018-04-27 18:20:16] [Logging$class:logError:70] Task 0 in stage 1.0 failed 1 times; aborting job
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Cancelling stage 1
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) failed in 0.969 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, flow_no_m), StringType), true) AS flow_no_m#40
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, pay_type), StringType), true) AS pay_type#41
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, is_useful), IntegerType) AS is_useful#42
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr2$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 13 more

Driver stacktrace:
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Stage 2 was cancelled
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Executor is trying to kill task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) failed in 0.947 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, flow_no_m), StringType), true) AS flow_no_m#40
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, pay_type), StringType), true) AS pay_type#41
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, is_useful), IntegerType) AS is_useful#42
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr2$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 13 more

Driver stacktrace:
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:98, took 0.999751 s
[ERROR] [2018-04-27 18:20:16] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, flow_no_m), StringType), true) AS flow_no_m#40
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, pay_type), StringType), true) AS pay_type#41
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, is_useful), IntegerType) AS is_useful#42
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr2$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 13 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:167)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:98)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, flow_no_m), StringType), true) AS flow_no_m#40
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, pay_type), StringType), true) AS pay_type#41
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, is_useful), IntegerType) AS is_useful#42
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr2$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 13 more
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Code generated in 22.266358 ms
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Code generated in 15.483106 ms
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Executor killed task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[WARN ] [2018-04-27 18:20:16] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 18:20:16] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-e6706e3b-627b-4e11-8b15-a2c1b32eb0c5
[INFO ] [2018-04-27 18:27:23] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 54251.
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-187afa1c-cd89-47a5-9c6a-86dc6cdb313a
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 18:27:24] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54260.
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Server created on 192.168.0.152:54260
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 54260, None)
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:54260 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 54260, None)
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 54260, None)
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 54260, None)
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:27:25] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:27:26] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:54260 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:27:26] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:27:26] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 18:27:26] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 18:27:26] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:54260 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:27:26] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:27:27] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 18:27:27] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 18:27:27] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 18:27:27] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 18:27:32] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:33] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:27:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Code generated in 240.619838 ms
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:54260 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 11.9 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:54260 (size: 6.0 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 18:27:39] [Logging$class:logInfo:54] Code generated in 11.912696 ms
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 73022 bytes result sent to driver
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 393 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.408 s
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.488825 s
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Code generated in 8.618297 ms
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 204.8 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:54260 (size: 204.8 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Code generated in 30.703415 ms
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Code generated in 15.61828 ms
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:54260 in memory (size: 6.0 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 12.3 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:54260 (size: 6.4 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 17.3 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1926.6 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:54260 (size: 7.5 KB, free: 1991.7 MB)
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Code generated in 11.261367 ms
[INFO ] [2018-04-27 18:27:40] [Logging$class:logInfo:54] Code generated in 15.423448 ms
[INFO ] [2018-04-27 18:27:41] [Logging$class:logInfo:54] Code generated in 40.439731 ms
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1582 bytes result sent to driver
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 1484 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) finished in 1.485 s
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Code generated in 15.100616 ms
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Code generated in 11.691056 ms
[ERROR] [2018-04-27 18:27:42] [Logging$class:logError:91] Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.ArrayIndexOutOfBoundsException: 10
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:54)
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:50)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] [2018-04-27 18:27:42] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 10
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:54)
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:50)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 18:27:42] [Logging$class:logError:70] Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) failed in 1.557 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 10
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:54)
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:50)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:98, took 1.607454 s
[ERROR] [2018-04-27 18:27:42] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 10
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:54)
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:50)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:167)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:98)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 10
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:54)
	at com.dr.banner.posProductProcessor$$anonfun$4.apply(posProductProcessor.scala:50)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 18:27:42] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-57a4cc87-c89d-4596-bbb3-d361ff9911be
[INFO ] [2018-04-27 18:37:55] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 18:37:55] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 18:37:55] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 18:37:55] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 18:37:55] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 18:37:55] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 18:37:55] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 54382.
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-613580ef-2059-4daf-afe1-4424a3380f61
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54391.
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Server created on 192.168.0.152:54391
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 54391, None)
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:54391 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 54391, None)
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 54391, None)
[INFO ] [2018-04-27 18:37:56] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 54391, None)
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:54391 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:54391 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:37:57] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:37:59] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 18:37:59] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 18:37:59] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 18:37:59] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 18:38:03] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:38:03] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:38:03] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 18:38:03] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 18:38:04] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-6c34190d-cd7f-401d-bdad-0bba915863af
[INFO ] [2018-04-27 18:50:57] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 54838.
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-4798b588-440e-4017-b3a6-7fb80d1eec3d
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54847.
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Server created on 192.168.0.152:54847
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 54847, None)
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:54847 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 54847, None)
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 54847, None)
[INFO ] [2018-04-27 18:50:58] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 54847, None)
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:54847 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:54847 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 18:50:59] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 18:51:01] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 18:51:01] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 18:51:01] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 18:51:01] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 18:51:05] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 18:51:06] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:07] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 18:51:12] [Logging$class:logInfo:54] Code generated in 216.341349 ms
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:54847 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1991.3 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:54847 (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Code generated in 11.93082 ms
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 218658 bytes result sent to driver
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 422 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.437 s
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.517578 s
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Code generated in 6.814968 ms
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 337.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:54847 (size: 337.8 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Code generated in 29.567272 ms
[INFO ] [2018-04-27 18:51:13] [Logging$class:logInfo:54] Code generated in 14.371884 ms
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 16.8 KB, free 1926.4 MB)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:54847 (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 12.3 KB, free 1926.4 MB)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:54847 (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:54847 in memory (size: 6.1 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Code generated in 28.584051 ms
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Code generated in 23.925073 ms
[ERROR] [2018-04-27 18:51:14] [Logging$class:logError:91] Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.StringIndexOutOfBoundsException: String index out of range: 4
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[WARN ] [2018-04-27 18:51:14] [Logging$class:logWarning:66] Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 4
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 18:51:14] [Logging$class:logError:70] Task 0 in stage 1.0 failed 1 times; aborting job
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Cancelling stage 1
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) failed in 0.250 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 4
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Stage 2 was cancelled
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) failed in 0.222 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 4
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Executor is trying to kill task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:98, took 0.359509 s
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Code generated in 17.121055 ms
[ERROR] [2018-04-27 18:51:14] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 4
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:167)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:98)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 4
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Executor killed task 0.0 in stage 2.0 (TID 2), reason: stage cancelled
[WARN ] [2018-04-27 18:51:14] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): TaskKilled (stage cancelled)
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 18:51:14] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-674ce048-3e04-4c71-ac01-4aa04a28b1c4
[INFO ] [2018-04-27 19:00:47] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 19:00:47] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 19:00:47] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 19:00:47] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 19:00:47] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 19:00:47] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 19:00:47] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 54968.
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-227b51b6-9537-4f26-a30a-344b0ebf3166
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54981.
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Server created on 192.168.0.152:54981
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 54981, None)
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:54981 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 54981, None)
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 54981, None)
[INFO ] [2018-04-27 19:00:48] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 54981, None)
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:54981 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:54981 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:00:49] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:00:50] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 19:00:50] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 19:00:50] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 19:00:51] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 19:00:54] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 19:00:55] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:00:56] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:01:01] [Logging$class:logInfo:54] Code generated in 235.060328 ms
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:54981 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:54981 (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Code generated in 14.062267 ms
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 218658 bytes result sent to driver
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 412 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.430 s
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.503316 s
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Code generated in 6.214236 ms
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 337.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:54981 (size: 337.8 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:54981 in memory (size: 6.1 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:01:02] [Logging$class:logInfo:54] Code generated in 38.574479 ms
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Code generated in 16.613585 ms
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 12.3 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:54981 (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:54981 (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Code generated in 10.451833 ms
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Code generated in 13.335801 ms
[INFO ] [2018-04-27 19:01:03] [Logging$class:logInfo:54] Code generated in 62.305475 ms
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1582 bytes result sent to driver
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 1292 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) finished in 1.293 s
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Code generated in 14.931082 ms
[ERROR] [2018-04-27 19:01:04] [Logging$class:logError:91] Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] [2018-04-27 19:01:04] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 19:01:04] [Logging$class:logError:70] Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) failed in 1.365 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:98, took 1.410124 s
[ERROR] [2018-04-27 19:01:04] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:167)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:98)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 19:01:04] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-0d533ac0-0a81-4ebf-b48c-f5ead2a254c1
[INFO ] [2018-04-27 19:04:40] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 55043.
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-48e530c4-8665-48a3-9a89-0684a6047204
[INFO ] [2018-04-27 19:04:41] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55052.
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Server created on 192.168.0.152:55052
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 55052, None)
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:55052 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 55052, None)
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 55052, None)
[INFO ] [2018-04-27 19:04:42] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 55052, None)
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:55052 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:55052 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:04:43] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:04:44] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 19:04:44] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 19:04:44] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 19:04:45] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 19:04:49] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:50] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:04:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Code generated in 228.62483 ms
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:55052 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:55052 (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 19:04:56] [Logging$class:logInfo:54] Code generated in 12.275174 ms
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 218658 bytes result sent to driver
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 413 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.427 s
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.504066 s
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Code generated in 7.04265 ms
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:55052 in memory (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 337.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:55052 (size: 337.8 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Code generated in 32.553187 ms
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Code generated in 15.653773 ms
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 12.3 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:55052 (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:55052 (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Code generated in 13.005038 ms
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Code generated in 19.077304 ms
[INFO ] [2018-04-27 19:04:57] [Logging$class:logInfo:54] Code generated in 57.453175 ms
[INFO ] [2018-04-27 19:04:58] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1625 bytes result sent to driver
[INFO ] [2018-04-27 19:04:58] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:04:58] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 1326 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) finished in 1.328 s
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Code generated in 14.289949 ms
[ERROR] [2018-04-27 19:04:59] [Logging$class:logError:91] Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] [2018-04-27 19:04:59] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 19:04:59] [Logging$class:logError:70] Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) failed in 1.383 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:98, took 1.428917 s
[ERROR] [2018-04-27 19:04:59] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:167)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:98)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:59)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 19:04:59] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-370b4b13-95f8-4cf4-993f-3028b76be9d5
[INFO ] [2018-04-27 19:15:49] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 19:15:49] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 19:15:49] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 19:15:49] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 19:15:49] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 19:15:49] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 19:15:49] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 55265.
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-4b4bedfb-79bd-4b19-9800-207a898dc269
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55274.
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Server created on 192.168.0.152:55274
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 55274, None)
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:55274 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 55274, None)
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 55274, None)
[INFO ] [2018-04-27 19:15:50] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 55274, None)
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:55274 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:55274 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:15:51] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:15:53] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 19:15:53] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 19:15:53] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 19:15:53] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 19:15:59] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:15:59] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:15:59] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:00] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:01] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:16:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Code generated in 364.640628 ms
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:55274 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:55274 (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 19:16:07] [Logging$class:logInfo:54] Code generated in 15.053419 ms
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 218615 bytes result sent to driver
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 515 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.535 s
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.609508 s
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Code generated in 9.362888 ms
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 337.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:55274 (size: 337.8 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Code generated in 29.520075 ms
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Code generated in 32.88357 ms
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:55274 in memory (size: 6.1 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 12.3 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:55274 (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:55274 (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Code generated in 14.230291 ms
[INFO ] [2018-04-27 19:16:08] [Logging$class:logInfo:54] Code generated in 15.032274 ms
[INFO ] [2018-04-27 19:16:09] [Logging$class:logInfo:54] Code generated in 38.496697 ms
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1539 bytes result sent to driver
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 1683 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) finished in 1.685 s
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Code generated in 15.844074 ms
[ERROR] [2018-04-27 19:16:10] [Logging$class:logError:91] Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] [2018-04-27 19:16:10] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 19:16:10] [Logging$class:logError:70] Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) failed in 1.730 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:98, took 1.795520 s
[ERROR] [2018-04-27 19:16:10] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:167)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:98)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 19:16:10] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-6d699b34-942b-4b05-82dd-2e1cbbcf34e0
[INFO ] [2018-04-27 19:21:01] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 19:21:01] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 19:21:01] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 19:21:01] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 19:21:01] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 19:21:01] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 19:21:01] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 55346.
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-776370ac-c986-4800-be3f-f67b66d2559e
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55355.
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Server created on 192.168.0.152:55355
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 55355, None)
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:55355 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 55355, None)
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 55355, None)
[INFO ] [2018-04-27 19:21:02] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 55355, None)
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:55355 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:55355 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:21:03] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:21:05] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 19:21:05] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 19:21:05] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 19:21:05] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 19:21:09] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:21:09] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:21:09] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:10] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:11] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:21:15] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Code generated in 239.883932 ms
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:55355 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:55355 (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 19:21:16] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Code generated in 11.641592 ms
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 218615 bytes result sent to driver
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 409 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.424 s
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.497988 s
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Code generated in 7.512739 ms
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:55355 in memory (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 337.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:55355 (size: 337.8 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Code generated in 27.997289 ms
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Code generated in 14.296746 ms
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 12.3 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:55355 (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:55355 (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Code generated in 11.757509 ms
[INFO ] [2018-04-27 19:21:17] [Logging$class:logInfo:54] Code generated in 13.169664 ms
[INFO ] [2018-04-27 19:21:18] [Logging$class:logInfo:54] Code generated in 41.188853 ms
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1625 bytes result sent to driver
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 1318 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) finished in 1.322 s
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Code generated in 15.15612 ms
[ERROR] [2018-04-27 19:21:19] [Logging$class:logError:91] Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN ] [2018-04-27 19:21:19] [Logging$class:logWarning:66] Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR] [2018-04-27 19:21:19] [Logging$class:logError:70] Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Cancelling stage 2
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) failed in 1.368 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Job 1 failed: saveAsTable at HistoryDataToA.scala:98, took 1.429496 s
[ERROR] [2018-04-27 19:21:19] [Logging$class:logError:91] Aborting job null.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:451)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:167)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:419)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at com.dr.work.HistoryDataToA$.main(HistoryDataToA.scala:98)
	at com.dr.work.HistoryDataToA.main(HistoryDataToA.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 10
	at java.lang.String.substring(String.java:1963)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:63)
	at com.dr.banner.posProductProcessor$$anonfun$5.apply(posProductProcessor.scala:62)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Invoking stop() from shutdown hook
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 19:21:19] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-cd25d433-4ba5-4a90-95ed-6b8f86cedf8b
[INFO ] [2018-04-27 19:25:31] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 19:25:31] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 19:25:31] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 19:25:31] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 19:25:31] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 19:25:31] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 19:25:31] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 55441.
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-d0fcd03a-3727-4617-9066-7c77333e9062
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55450.
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Server created on 192.168.0.152:55450
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 55450, None)
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:55450 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 55450, None)
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 55450, None)
[INFO ] [2018-04-27 19:25:32] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 55450, None)
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:55450 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:55450 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:25:33] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:25:35] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 19:25:35] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 19:25:35] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 19:25:35] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 19:25:39] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 19:25:40] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:41] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:46] [Logging$class:logInfo:54] Code generated in 205.507403 ms
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:55450 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:55450 (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Code generated in 12.183422 ms
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 218615 bytes result sent to driver
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 418 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.435 s
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.513505 s
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Code generated in 6.71 ms
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 337.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:55450 (size: 337.8 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Code generated in 29.08057 ms
[INFO ] [2018-04-27 19:25:47] [Logging$class:logInfo:54] Code generated in 13.962208 ms
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:55450 in memory (size: 6.1 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 16.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:55450 (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 12.3 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:55450 (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Code generated in 17.65571 ms
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Code generated in 13.791541 ms
[INFO ] [2018-04-27 19:25:48] [Logging$class:logInfo:54] Code generated in 93.931699 ms
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1582 bytes result sent to driver
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 3563 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) finished in 3.565 s
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Code generated in 8.568456 ms
[INFO ] [2018-04-27 19:25:51] [Logging$class:logInfo:54] Code generated in 33.628917 ms
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2). 1496 bytes result sent to driver
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2) in 957 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) finished in 4.500 s
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] running: Set()
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Submitting ResultStage 3 (MapPartitionsRDD[32] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Block broadcast_7 stored as values in memory (estimated size 114.2 KB, free 1926.3 MB)
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 43.4 KB, free 1926.3 MB)
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Added broadcast_7_piece0 in memory on 192.168.0.152:55450 (size: 43.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[32] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Adding task set 3.0 with 200 tasks
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Running task 0.0 in stage 3.0 (TID 3)
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:52] [Logging$class:logInfo:54] Started 0 remote fetches in 5 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 13.024673 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 9.439915 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 14.476096 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 4.817562 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 6.687346 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 6.831959 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Removed broadcast_6_piece0 on 192.168.0.152:55450 in memory (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Removed broadcast_5_piece0 on 192.168.0.152:55450 in memory (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 7.345848 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 22.150062 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Code generated in 9.780871 ms
[INFO ] [2018-04-27 19:25:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] attempt_20180427192553_0003_m_000000_0: Committed
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3). 3818 bytes result sent to driver
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3) in 1293 ms on localhost (executor driver) (1/200)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Running task 1.0 in stage 3.0 (TID 4)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] attempt_20180427192554_0003_m_000001_0: Committed
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Starting task 2.0 in stage 3.0 (TID 5, localhost, executor driver, partition 2, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4) in 169 ms on localhost (executor driver) (2/200)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Running task 2.0 in stage 3.0 (TID 5)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] attempt_20180427192554_0003_m_000002_0: Committed
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Starting task 3.0 in stage 3.0 (TID 6, localhost, executor driver, partition 3, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Running task 3.0 in stage 3.0 (TID 6)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5) in 196 ms on localhost (executor driver) (3/200)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] attempt_20180427192554_0003_m_000003_0: Committed
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Starting task 4.0 in stage 3.0 (TID 7, localhost, executor driver, partition 4, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Running task 4.0 in stage 3.0 (TID 7)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6) in 178 ms on localhost (executor driver) (4/200)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] attempt_20180427192554_0003_m_000004_0: Committed
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Starting task 5.0 in stage 3.0 (TID 8, localhost, executor driver, partition 5, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Running task 5.0 in stage 3.0 (TID 8)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7) in 155 ms on localhost (executor driver) (5/200)
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192554_0003_m_000005_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8). 3775 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 6.0 in stage 3.0 (TID 9, localhost, executor driver, partition 6, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8) in 168 ms on localhost (executor driver) (6/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 6.0 in stage 3.0 (TID 9)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192555_0003_m_000006_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 7.0 in stage 3.0 (TID 10, localhost, executor driver, partition 7, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 7.0 in stage 3.0 (TID 10)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9) in 157 ms on localhost (executor driver) (7/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192555_0003_m_000007_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 8.0 in stage 3.0 (TID 11, localhost, executor driver, partition 8, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 8.0 in stage 3.0 (TID 11)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10) in 160 ms on localhost (executor driver) (8/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192555_0003_m_000008_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 9.0 in stage 3.0 (TID 12, localhost, executor driver, partition 9, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 9.0 in stage 3.0 (TID 12)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11) in 119 ms on localhost (executor driver) (9/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192555_0003_m_000009_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 10.0 in stage 3.0 (TID 13, localhost, executor driver, partition 10, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 10.0 in stage 3.0 (TID 13)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12) in 178 ms on localhost (executor driver) (10/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192555_0003_m_000010_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 11.0 in stage 3.0 (TID 14, localhost, executor driver, partition 11, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 11.0 in stage 3.0 (TID 14)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13) in 99 ms on localhost (executor driver) (11/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192555_0003_m_000011_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 12.0 in stage 3.0 (TID 15, localhost, executor driver, partition 12, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 12.0 in stage 3.0 (TID 15)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14) in 72 ms on localhost (executor driver) (12/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] attempt_20180427192555_0003_m_000012_0: Committed
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Starting task 13.0 in stage 3.0 (TID 16, localhost, executor driver, partition 13, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Running task 13.0 in stage 3.0 (TID 16)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15) in 137 ms on localhost (executor driver) (13/200)
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000013_0: Committed
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Starting task 14.0 in stage 3.0 (TID 17, localhost, executor driver, partition 14, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Running task 14.0 in stage 3.0 (TID 17)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16) in 157 ms on localhost (executor driver) (14/200)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000014_0: Committed
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Starting task 15.0 in stage 3.0 (TID 18, localhost, executor driver, partition 15, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Running task 15.0 in stage 3.0 (TID 18)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17) in 122 ms on localhost (executor driver) (15/200)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000015_0: Committed
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Starting task 16.0 in stage 3.0 (TID 19, localhost, executor driver, partition 16, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Running task 16.0 in stage 3.0 (TID 19)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18) in 157 ms on localhost (executor driver) (16/200)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000016_0: Committed
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19). 3775 bytes result sent to driver
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Starting task 17.0 in stage 3.0 (TID 20, localhost, executor driver, partition 17, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Running task 17.0 in stage 3.0 (TID 20)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19) in 156 ms on localhost (executor driver) (17/200)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000017_0: Committed
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Starting task 18.0 in stage 3.0 (TID 21, localhost, executor driver, partition 18, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Running task 18.0 in stage 3.0 (TID 21)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20) in 166 ms on localhost (executor driver) (18/200)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000018_0: Committed
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Starting task 19.0 in stage 3.0 (TID 22, localhost, executor driver, partition 19, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21) in 127 ms on localhost (executor driver) (19/200)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Running task 19.0 in stage 3.0 (TID 22)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000019_0: Committed
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Starting task 20.0 in stage 3.0 (TID 23, localhost, executor driver, partition 20, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22) in 116 ms on localhost (executor driver) (20/200)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Running task 20.0 in stage 3.0 (TID 23)
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] attempt_20180427192556_0003_m_000020_0: Committed
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Starting task 21.0 in stage 3.0 (TID 24, localhost, executor driver, partition 21, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23) in 103 ms on localhost (executor driver) (21/200)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Running task 21.0 in stage 3.0 (TID 24)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] attempt_20180427192557_0003_m_000021_0: Committed
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Starting task 22.0 in stage 3.0 (TID 25, localhost, executor driver, partition 22, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Running task 22.0 in stage 3.0 (TID 25)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24) in 121 ms on localhost (executor driver) (22/200)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] attempt_20180427192557_0003_m_000022_0: Committed
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Starting task 23.0 in stage 3.0 (TID 26, localhost, executor driver, partition 23, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Running task 23.0 in stage 3.0 (TID 26)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25) in 156 ms on localhost (executor driver) (23/200)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] attempt_20180427192557_0003_m_000023_0: Committed
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Starting task 24.0 in stage 3.0 (TID 27, localhost, executor driver, partition 24, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Running task 24.0 in stage 3.0 (TID 27)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26) in 153 ms on localhost (executor driver) (24/200)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] attempt_20180427192557_0003_m_000024_0: Committed
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Starting task 25.0 in stage 3.0 (TID 28, localhost, executor driver, partition 25, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27) in 141 ms on localhost (executor driver) (25/200)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Running task 25.0 in stage 3.0 (TID 28)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] attempt_20180427192557_0003_m_000025_0: Committed
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Starting task 26.0 in stage 3.0 (TID 29, localhost, executor driver, partition 26, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28) in 125 ms on localhost (executor driver) (26/200)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Running task 26.0 in stage 3.0 (TID 29)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] attempt_20180427192557_0003_m_000026_0: Committed
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Starting task 27.0 in stage 3.0 (TID 30, localhost, executor driver, partition 27, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29) in 156 ms on localhost (executor driver) (27/200)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Running task 27.0 in stage 3.0 (TID 30)
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] attempt_20180427192557_0003_m_000027_0: Committed
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Starting task 28.0 in stage 3.0 (TID 31, localhost, executor driver, partition 28, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30) in 177 ms on localhost (executor driver) (28/200)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Running task 28.0 in stage 3.0 (TID 31)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] attempt_20180427192558_0003_m_000028_0: Committed
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Starting task 29.0 in stage 3.0 (TID 32, localhost, executor driver, partition 29, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Running task 29.0 in stage 3.0 (TID 32)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31) in 510 ms on localhost (executor driver) (29/200)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] attempt_20180427192558_0003_m_000029_0: Committed
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Starting task 30.0 in stage 3.0 (TID 33, localhost, executor driver, partition 30, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Running task 30.0 in stage 3.0 (TID 33)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32) in 121 ms on localhost (executor driver) (30/200)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] attempt_20180427192558_0003_m_000030_0: Committed
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Starting task 31.0 in stage 3.0 (TID 34, localhost, executor driver, partition 31, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33) in 158 ms on localhost (executor driver) (31/200)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Running task 31.0 in stage 3.0 (TID 34)
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192558_0003_m_000031_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 32.0 in stage 3.0 (TID 35, localhost, executor driver, partition 32, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 32.0 in stage 3.0 (TID 35)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34) in 166 ms on localhost (executor driver) (32/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000032_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 33.0 in stage 3.0 (TID 36, localhost, executor driver, partition 33, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 33.0 in stage 3.0 (TID 36)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35) in 168 ms on localhost (executor driver) (33/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000033_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 34.0 in stage 3.0 (TID 37, localhost, executor driver, partition 34, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 34.0 in stage 3.0 (TID 37)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36) in 177 ms on localhost (executor driver) (34/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000034_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 35.0 in stage 3.0 (TID 38, localhost, executor driver, partition 35, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 35.0 in stage 3.0 (TID 38)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37) in 121 ms on localhost (executor driver) (35/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000035_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 36.0 in stage 3.0 (TID 39, localhost, executor driver, partition 36, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 36.0 in stage 3.0 (TID 39)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38) in 134 ms on localhost (executor driver) (36/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000036_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 37.0 in stage 3.0 (TID 40, localhost, executor driver, partition 37, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39) in 89 ms on localhost (executor driver) (37/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 37.0 in stage 3.0 (TID 40)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000037_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 38.0 in stage 3.0 (TID 41, localhost, executor driver, partition 38, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 38.0 in stage 3.0 (TID 41)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40) in 107 ms on localhost (executor driver) (38/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000038_0: Committed
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Starting task 39.0 in stage 3.0 (TID 42, localhost, executor driver, partition 39, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Running task 39.0 in stage 3.0 (TID 42)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41) in 127 ms on localhost (executor driver) (39/200)
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:25:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192559_0003_m_000039_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 40.0 in stage 3.0 (TID 43, localhost, executor driver, partition 40, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 40.0 in stage 3.0 (TID 43)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42) in 155 ms on localhost (executor driver) (40/200)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192600_0003_m_000040_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 41.0 in stage 3.0 (TID 44, localhost, executor driver, partition 41, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 41.0 in stage 3.0 (TID 44)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43) in 124 ms on localhost (executor driver) (41/200)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192600_0003_m_000041_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 42.0 in stage 3.0 (TID 45, localhost, executor driver, partition 42, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 42.0 in stage 3.0 (TID 45)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44) in 145 ms on localhost (executor driver) (42/200)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192600_0003_m_000042_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 43.0 in stage 3.0 (TID 46, localhost, executor driver, partition 43, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 43.0 in stage 3.0 (TID 46)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45) in 157 ms on localhost (executor driver) (43/200)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192600_0003_m_000043_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 44.0 in stage 3.0 (TID 47, localhost, executor driver, partition 44, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 44.0 in stage 3.0 (TID 47)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46) in 133 ms on localhost (executor driver) (44/200)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192600_0003_m_000044_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 45.0 in stage 3.0 (TID 48, localhost, executor driver, partition 45, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 45.0 in stage 3.0 (TID 48)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47) in 121 ms on localhost (executor driver) (45/200)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192600_0003_m_000045_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 46.0 in stage 3.0 (TID 49, localhost, executor driver, partition 46, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 46.0 in stage 3.0 (TID 49)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48) in 111 ms on localhost (executor driver) (46/200)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] attempt_20180427192600_0003_m_000046_0: Committed
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Starting task 47.0 in stage 3.0 (TID 50, localhost, executor driver, partition 47, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Running task 47.0 in stage 3.0 (TID 50)
[INFO ] [2018-04-27 19:26:00] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49) in 90 ms on localhost (executor driver) (47/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000047_0: Committed
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Starting task 48.0 in stage 3.0 (TID 51, localhost, executor driver, partition 48, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Running task 48.0 in stage 3.0 (TID 51)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50) in 131 ms on localhost (executor driver) (48/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000048_0: Committed
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Starting task 49.0 in stage 3.0 (TID 52, localhost, executor driver, partition 49, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51) in 181 ms on localhost (executor driver) (49/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Running task 49.0 in stage 3.0 (TID 52)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000049_0: Committed
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Starting task 50.0 in stage 3.0 (TID 53, localhost, executor driver, partition 50, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Running task 50.0 in stage 3.0 (TID 53)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52) in 111 ms on localhost (executor driver) (50/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000050_0: Committed
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Starting task 51.0 in stage 3.0 (TID 54, localhost, executor driver, partition 51, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Running task 51.0 in stage 3.0 (TID 54)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53) in 77 ms on localhost (executor driver) (51/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000051_0: Committed
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Starting task 52.0 in stage 3.0 (TID 55, localhost, executor driver, partition 52, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54) in 141 ms on localhost (executor driver) (52/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Running task 52.0 in stage 3.0 (TID 55)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000052_0: Committed
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Starting task 53.0 in stage 3.0 (TID 56, localhost, executor driver, partition 53, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Running task 53.0 in stage 3.0 (TID 56)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55) in 137 ms on localhost (executor driver) (53/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000053_0: Committed
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Starting task 54.0 in stage 3.0 (TID 57, localhost, executor driver, partition 54, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Running task 54.0 in stage 3.0 (TID 57)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56) in 131 ms on localhost (executor driver) (54/200)
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:01] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192601_0003_m_000054_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 55.0 in stage 3.0 (TID 58, localhost, executor driver, partition 55, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 55.0 in stage 3.0 (TID 58)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57) in 103 ms on localhost (executor driver) (55/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000055_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 56.0 in stage 3.0 (TID 59, localhost, executor driver, partition 56, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58) in 78 ms on localhost (executor driver) (56/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 56.0 in stage 3.0 (TID 59)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000056_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 57.0 in stage 3.0 (TID 60, localhost, executor driver, partition 57, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 57.0 in stage 3.0 (TID 60)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59) in 52 ms on localhost (executor driver) (57/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000057_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 58.0 in stage 3.0 (TID 61, localhost, executor driver, partition 58, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 58.0 in stage 3.0 (TID 61)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60) in 94 ms on localhost (executor driver) (58/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000058_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 59.0 in stage 3.0 (TID 62, localhost, executor driver, partition 59, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 59.0 in stage 3.0 (TID 62)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61) in 77 ms on localhost (executor driver) (59/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000059_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 60.0 in stage 3.0 (TID 63, localhost, executor driver, partition 60, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 60.0 in stage 3.0 (TID 63)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62) in 111 ms on localhost (executor driver) (60/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000060_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 61.0 in stage 3.0 (TID 64, localhost, executor driver, partition 61, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 61.0 in stage 3.0 (TID 64)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63) in 155 ms on localhost (executor driver) (61/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000061_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 62.0 in stage 3.0 (TID 65, localhost, executor driver, partition 62, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64) in 144 ms on localhost (executor driver) (62/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 62.0 in stage 3.0 (TID 65)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000062_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 63.0 in stage 3.0 (TID 66, localhost, executor driver, partition 63, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 63.0 in stage 3.0 (TID 66)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65) in 114 ms on localhost (executor driver) (63/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000063_0: Committed
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Starting task 64.0 in stage 3.0 (TID 67, localhost, executor driver, partition 64, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Running task 64.0 in stage 3.0 (TID 67)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66) in 139 ms on localhost (executor driver) (64/200)
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:02] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192602_0003_m_000064_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 65.0 in stage 3.0 (TID 68, localhost, executor driver, partition 65, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 65.0 in stage 3.0 (TID 68)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67) in 113 ms on localhost (executor driver) (65/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000065_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 66.0 in stage 3.0 (TID 69, localhost, executor driver, partition 66, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 66.0 in stage 3.0 (TID 69)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68) in 120 ms on localhost (executor driver) (66/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000066_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 67.0 in stage 3.0 (TID 70, localhost, executor driver, partition 67, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 67.0 in stage 3.0 (TID 70)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69) in 125 ms on localhost (executor driver) (67/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000067_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 68.0 in stage 3.0 (TID 71, localhost, executor driver, partition 68, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 68.0 in stage 3.0 (TID 71)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70) in 50 ms on localhost (executor driver) (68/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000068_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 69.0 in stage 3.0 (TID 72, localhost, executor driver, partition 69, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 69.0 in stage 3.0 (TID 72)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71) in 74 ms on localhost (executor driver) (69/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000069_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 70.0 in stage 3.0 (TID 73, localhost, executor driver, partition 70, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 70.0 in stage 3.0 (TID 73)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72) in 90 ms on localhost (executor driver) (70/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000070_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 71.0 in stage 3.0 (TID 74, localhost, executor driver, partition 71, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 71.0 in stage 3.0 (TID 74)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73) in 88 ms on localhost (executor driver) (71/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000071_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 72.0 in stage 3.0 (TID 75, localhost, executor driver, partition 72, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 72.0 in stage 3.0 (TID 75)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74) in 134 ms on localhost (executor driver) (72/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000072_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 73.0 in stage 3.0 (TID 76, localhost, executor driver, partition 73, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 73.0 in stage 3.0 (TID 76)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75) in 77 ms on localhost (executor driver) (73/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000073_0: Committed
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Starting task 74.0 in stage 3.0 (TID 77, localhost, executor driver, partition 74, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Running task 74.0 in stage 3.0 (TID 77)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76) in 123 ms on localhost (executor driver) (74/200)
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:03] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192603_0003_m_000074_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 75.0 in stage 3.0 (TID 78, localhost, executor driver, partition 75, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 75.0 in stage 3.0 (TID 78)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77) in 99 ms on localhost (executor driver) (75/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000075_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 76.0 in stage 3.0 (TID 79, localhost, executor driver, partition 76, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 76.0 in stage 3.0 (TID 79)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78) in 125 ms on localhost (executor driver) (76/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000076_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 77.0 in stage 3.0 (TID 80, localhost, executor driver, partition 77, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 77.0 in stage 3.0 (TID 80)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79) in 85 ms on localhost (executor driver) (77/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000077_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 78.0 in stage 3.0 (TID 81, localhost, executor driver, partition 78, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80) in 126 ms on localhost (executor driver) (78/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 78.0 in stage 3.0 (TID 81)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000078_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 79.0 in stage 3.0 (TID 82, localhost, executor driver, partition 79, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 79.0 in stage 3.0 (TID 82)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81) in 79 ms on localhost (executor driver) (79/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000079_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 80.0 in stage 3.0 (TID 83, localhost, executor driver, partition 80, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 80.0 in stage 3.0 (TID 83)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82) in 122 ms on localhost (executor driver) (80/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000080_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 81.0 in stage 3.0 (TID 84, localhost, executor driver, partition 81, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 81.0 in stage 3.0 (TID 84)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83) in 55 ms on localhost (executor driver) (81/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000081_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 82.0 in stage 3.0 (TID 85, localhost, executor driver, partition 82, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 82.0 in stage 3.0 (TID 85)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84) in 90 ms on localhost (executor driver) (82/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000082_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 83.0 in stage 3.0 (TID 86, localhost, executor driver, partition 83, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 83.0 in stage 3.0 (TID 86)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85) in 89 ms on localhost (executor driver) (83/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000083_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 84.0 in stage 3.0 (TID 87, localhost, executor driver, partition 84, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 84.0 in stage 3.0 (TID 87)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86) in 91 ms on localhost (executor driver) (84/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] attempt_20180427192604_0003_m_000084_0: Committed
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Starting task 85.0 in stage 3.0 (TID 88, localhost, executor driver, partition 85, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Running task 85.0 in stage 3.0 (TID 88)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87) in 86 ms on localhost (executor driver) (85/200)
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:04] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000085_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 86.0 in stage 3.0 (TID 89, localhost, executor driver, partition 86, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 86.0 in stage 3.0 (TID 89)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88) in 124 ms on localhost (executor driver) (86/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000086_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 87.0 in stage 3.0 (TID 90, localhost, executor driver, partition 87, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 87.0 in stage 3.0 (TID 90)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89) in 167 ms on localhost (executor driver) (87/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000087_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 88.0 in stage 3.0 (TID 91, localhost, executor driver, partition 88, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 88.0 in stage 3.0 (TID 91)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90) in 121 ms on localhost (executor driver) (88/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000088_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 89.0 in stage 3.0 (TID 92, localhost, executor driver, partition 89, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 89.0 in stage 3.0 (TID 92)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91) in 47 ms on localhost (executor driver) (89/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000089_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 90.0 in stage 3.0 (TID 93, localhost, executor driver, partition 90, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92) in 97 ms on localhost (executor driver) (90/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 90.0 in stage 3.0 (TID 93)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000090_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 91.0 in stage 3.0 (TID 94, localhost, executor driver, partition 91, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93) in 78 ms on localhost (executor driver) (91/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 91.0 in stage 3.0 (TID 94)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000091_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 92.0 in stage 3.0 (TID 95, localhost, executor driver, partition 92, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 92.0 in stage 3.0 (TID 95)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94) in 90 ms on localhost (executor driver) (92/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000092_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 93.0 in stage 3.0 (TID 96, localhost, executor driver, partition 93, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95) in 67 ms on localhost (executor driver) (93/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 93.0 in stage 3.0 (TID 96)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000093_0: Committed
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Starting task 94.0 in stage 3.0 (TID 97, localhost, executor driver, partition 94, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96) in 90 ms on localhost (executor driver) (94/200)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Running task 94.0 in stage 3.0 (TID 97)
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:05] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] attempt_20180427192605_0003_m_000094_0: Committed
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Starting task 95.0 in stage 3.0 (TID 98, localhost, executor driver, partition 95, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Running task 95.0 in stage 3.0 (TID 98)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97) in 167 ms on localhost (executor driver) (95/200)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] attempt_20180427192606_0003_m_000095_0: Committed
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Starting task 96.0 in stage 3.0 (TID 99, localhost, executor driver, partition 96, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Running task 96.0 in stage 3.0 (TID 99)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98) in 76 ms on localhost (executor driver) (96/200)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] attempt_20180427192606_0003_m_000096_0: Committed
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Starting task 97.0 in stage 3.0 (TID 100, localhost, executor driver, partition 97, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Running task 97.0 in stage 3.0 (TID 100)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99) in 112 ms on localhost (executor driver) (97/200)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] attempt_20180427192606_0003_m_000097_0: Committed
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Starting task 98.0 in stage 3.0 (TID 101, localhost, executor driver, partition 98, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Running task 98.0 in stage 3.0 (TID 101)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100) in 87 ms on localhost (executor driver) (98/200)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] attempt_20180427192606_0003_m_000098_0: Committed
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Starting task 99.0 in stage 3.0 (TID 102, localhost, executor driver, partition 99, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101) in 124 ms on localhost (executor driver) (99/200)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Running task 99.0 in stage 3.0 (TID 102)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] attempt_20180427192606_0003_m_000099_0: Committed
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Starting task 100.0 in stage 3.0 (TID 103, localhost, executor driver, partition 100, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102) in 99 ms on localhost (executor driver) (100/200)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Running task 100.0 in stage 3.0 (TID 103)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] attempt_20180427192606_0003_m_000100_0: Committed
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Starting task 101.0 in stage 3.0 (TID 104, localhost, executor driver, partition 101, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Running task 101.0 in stage 3.0 (TID 104)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103) in 192 ms on localhost (executor driver) (101/200)
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:06] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] attempt_20180427192606_0003_m_000101_0: Committed
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104). 3775 bytes result sent to driver
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Starting task 102.0 in stage 3.0 (TID 105, localhost, executor driver, partition 102, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Running task 102.0 in stage 3.0 (TID 105)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104) in 285 ms on localhost (executor driver) (102/200)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] attempt_20180427192607_0003_m_000102_0: Committed
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Starting task 103.0 in stage 3.0 (TID 106, localhost, executor driver, partition 103, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Running task 103.0 in stage 3.0 (TID 106)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105) in 85 ms on localhost (executor driver) (103/200)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] attempt_20180427192607_0003_m_000103_0: Committed
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Starting task 104.0 in stage 3.0 (TID 107, localhost, executor driver, partition 104, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Running task 104.0 in stage 3.0 (TID 107)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106) in 119 ms on localhost (executor driver) (104/200)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] attempt_20180427192607_0003_m_000104_0: Committed
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Starting task 105.0 in stage 3.0 (TID 108, localhost, executor driver, partition 105, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Running task 105.0 in stage 3.0 (TID 108)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107) in 100 ms on localhost (executor driver) (105/200)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] attempt_20180427192607_0003_m_000105_0: Committed
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Starting task 106.0 in stage 3.0 (TID 109, localhost, executor driver, partition 106, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Running task 106.0 in stage 3.0 (TID 109)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108) in 88 ms on localhost (executor driver) (106/200)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] attempt_20180427192607_0003_m_000106_0: Committed
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Starting task 107.0 in stage 3.0 (TID 110, localhost, executor driver, partition 107, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Running task 107.0 in stage 3.0 (TID 110)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109) in 134 ms on localhost (executor driver) (107/200)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] attempt_20180427192607_0003_m_000107_0: Committed
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Starting task 108.0 in stage 3.0 (TID 111, localhost, executor driver, partition 108, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Running task 108.0 in stage 3.0 (TID 111)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110) in 166 ms on localhost (executor driver) (108/200)
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:07] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192607_0003_m_000108_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 109.0 in stage 3.0 (TID 112, localhost, executor driver, partition 109, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 109.0 in stage 3.0 (TID 112)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111) in 530 ms on localhost (executor driver) (109/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000109_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 110.0 in stage 3.0 (TID 113, localhost, executor driver, partition 110, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112) in 50 ms on localhost (executor driver) (110/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 110.0 in stage 3.0 (TID 113)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000110_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 111.0 in stage 3.0 (TID 114, localhost, executor driver, partition 111, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 111.0 in stage 3.0 (TID 114)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113) in 103 ms on localhost (executor driver) (111/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000111_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 112.0 in stage 3.0 (TID 115, localhost, executor driver, partition 112, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 112.0 in stage 3.0 (TID 115)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114) in 83 ms on localhost (executor driver) (112/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000112_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 113.0 in stage 3.0 (TID 116, localhost, executor driver, partition 113, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115) in 131 ms on localhost (executor driver) (113/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 113.0 in stage 3.0 (TID 116)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000113_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 114.0 in stage 3.0 (TID 117, localhost, executor driver, partition 114, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116) in 92 ms on localhost (executor driver) (114/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 114.0 in stage 3.0 (TID 117)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000114_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 115.0 in stage 3.0 (TID 118, localhost, executor driver, partition 115, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 115.0 in stage 3.0 (TID 118)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117) in 133 ms on localhost (executor driver) (115/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000115_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 116.0 in stage 3.0 (TID 119, localhost, executor driver, partition 116, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 116.0 in stage 3.0 (TID 119)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118) in 89 ms on localhost (executor driver) (116/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000116_0: Committed
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Starting task 117.0 in stage 3.0 (TID 120, localhost, executor driver, partition 117, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Running task 117.0 in stage 3.0 (TID 120)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119) in 76 ms on localhost (executor driver) (117/200)
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:08] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192608_0003_m_000117_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 118.0 in stage 3.0 (TID 121, localhost, executor driver, partition 118, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 118.0 in stage 3.0 (TID 121)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120) in 64 ms on localhost (executor driver) (118/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000118_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 119.0 in stage 3.0 (TID 122, localhost, executor driver, partition 119, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121) in 179 ms on localhost (executor driver) (119/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 119.0 in stage 3.0 (TID 122)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000119_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 120.0 in stage 3.0 (TID 123, localhost, executor driver, partition 120, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 120.0 in stage 3.0 (TID 123)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122) in 168 ms on localhost (executor driver) (120/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000120_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 121.0 in stage 3.0 (TID 124, localhost, executor driver, partition 121, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 121.0 in stage 3.0 (TID 124)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123) in 113 ms on localhost (executor driver) (121/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000121_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 122.0 in stage 3.0 (TID 125, localhost, executor driver, partition 122, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 122.0 in stage 3.0 (TID 125)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124) in 135 ms on localhost (executor driver) (122/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000122_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 123.0 in stage 3.0 (TID 126, localhost, executor driver, partition 123, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 123.0 in stage 3.0 (TID 126)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125) in 48 ms on localhost (executor driver) (123/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000123_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 124.0 in stage 3.0 (TID 127, localhost, executor driver, partition 124, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 124.0 in stage 3.0 (TID 127)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126) in 75 ms on localhost (executor driver) (124/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000124_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 125.0 in stage 3.0 (TID 128, localhost, executor driver, partition 125, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 125.0 in stage 3.0 (TID 128)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127) in 89 ms on localhost (executor driver) (125/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000125_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 126.0 in stage 3.0 (TID 129, localhost, executor driver, partition 126, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 126.0 in stage 3.0 (TID 129)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128) in 53 ms on localhost (executor driver) (126/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000126_0: Committed
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Starting task 127.0 in stage 3.0 (TID 130, localhost, executor driver, partition 127, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Running task 127.0 in stage 3.0 (TID 130)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129) in 57 ms on localhost (executor driver) (127/200)
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:09] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192609_0003_m_000127_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 128.0 in stage 3.0 (TID 131, localhost, executor driver, partition 128, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 128.0 in stage 3.0 (TID 131)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130) in 93 ms on localhost (executor driver) (128/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000128_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 129.0 in stage 3.0 (TID 132, localhost, executor driver, partition 129, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 129.0 in stage 3.0 (TID 132)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131) in 53 ms on localhost (executor driver) (129/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000129_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 130.0 in stage 3.0 (TID 133, localhost, executor driver, partition 130, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 130.0 in stage 3.0 (TID 133)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132) in 222 ms on localhost (executor driver) (130/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000130_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 131.0 in stage 3.0 (TID 134, localhost, executor driver, partition 131, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 131.0 in stage 3.0 (TID 134)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133) in 84 ms on localhost (executor driver) (131/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000131_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 132.0 in stage 3.0 (TID 135, localhost, executor driver, partition 132, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 132.0 in stage 3.0 (TID 135)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134) in 301 ms on localhost (executor driver) (132/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000132_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 133.0 in stage 3.0 (TID 136, localhost, executor driver, partition 133, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 133.0 in stage 3.0 (TID 136)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135) in 62 ms on localhost (executor driver) (133/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000133_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 134.0 in stage 3.0 (TID 137, localhost, executor driver, partition 134, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 134.0 in stage 3.0 (TID 137)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136) in 40 ms on localhost (executor driver) (134/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000134_0: Committed
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Starting task 135.0 in stage 3.0 (TID 138, localhost, executor driver, partition 135, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Running task 135.0 in stage 3.0 (TID 138)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137) in 124 ms on localhost (executor driver) (135/200)
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:10] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192610_0003_m_000135_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 136.0 in stage 3.0 (TID 139, localhost, executor driver, partition 136, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 136.0 in stage 3.0 (TID 139)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138) in 102 ms on localhost (executor driver) (136/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000136_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 137.0 in stage 3.0 (TID 140, localhost, executor driver, partition 137, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 137.0 in stage 3.0 (TID 140)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139) in 119 ms on localhost (executor driver) (137/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000137_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 138.0 in stage 3.0 (TID 141, localhost, executor driver, partition 138, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 138.0 in stage 3.0 (TID 141)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140) in 64 ms on localhost (executor driver) (138/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000138_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 139.0 in stage 3.0 (TID 142, localhost, executor driver, partition 139, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141) in 92 ms on localhost (executor driver) (139/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 139.0 in stage 3.0 (TID 142)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000139_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 140.0 in stage 3.0 (TID 143, localhost, executor driver, partition 140, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 140.0 in stage 3.0 (TID 143)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142) in 51 ms on localhost (executor driver) (140/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000140_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 141.0 in stage 3.0 (TID 144, localhost, executor driver, partition 141, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 141.0 in stage 3.0 (TID 144)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143) in 105 ms on localhost (executor driver) (141/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000141_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 142.0 in stage 3.0 (TID 145, localhost, executor driver, partition 142, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 142.0 in stage 3.0 (TID 145)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144) in 67 ms on localhost (executor driver) (142/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000142_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 143.0 in stage 3.0 (TID 146, localhost, executor driver, partition 143, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 143.0 in stage 3.0 (TID 146)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145) in 49 ms on localhost (executor driver) (143/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000143_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 144.0 in stage 3.0 (TID 147, localhost, executor driver, partition 144, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 144.0 in stage 3.0 (TID 147)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146) in 53 ms on localhost (executor driver) (144/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000144_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 145.0 in stage 3.0 (TID 148, localhost, executor driver, partition 145, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 145.0 in stage 3.0 (TID 148)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147) in 74 ms on localhost (executor driver) (145/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000145_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 146.0 in stage 3.0 (TID 149, localhost, executor driver, partition 146, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 146.0 in stage 3.0 (TID 149)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148) in 81 ms on localhost (executor driver) (146/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000146_0: Committed
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Starting task 147.0 in stage 3.0 (TID 150, localhost, executor driver, partition 147, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Running task 147.0 in stage 3.0 (TID 150)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149) in 74 ms on localhost (executor driver) (147/200)
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:11] [Logging$class:logInfo:54] attempt_20180427192611_0003_m_000147_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 148.0 in stage 3.0 (TID 151, localhost, executor driver, partition 148, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 148.0 in stage 3.0 (TID 151)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150) in 150 ms on localhost (executor driver) (148/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000148_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 149.0 in stage 3.0 (TID 152, localhost, executor driver, partition 149, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151) in 85 ms on localhost (executor driver) (149/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 149.0 in stage 3.0 (TID 152)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000149_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 150.0 in stage 3.0 (TID 153, localhost, executor driver, partition 150, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 150.0 in stage 3.0 (TID 153)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152) in 55 ms on localhost (executor driver) (150/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000150_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 151.0 in stage 3.0 (TID 154, localhost, executor driver, partition 151, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 151.0 in stage 3.0 (TID 154)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153) in 44 ms on localhost (executor driver) (151/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000151_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 152.0 in stage 3.0 (TID 155, localhost, executor driver, partition 152, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 152.0 in stage 3.0 (TID 155)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154) in 111 ms on localhost (executor driver) (152/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000152_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 153.0 in stage 3.0 (TID 156, localhost, executor driver, partition 153, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 153.0 in stage 3.0 (TID 156)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155) in 56 ms on localhost (executor driver) (153/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000153_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 154.0 in stage 3.0 (TID 157, localhost, executor driver, partition 154, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 154.0 in stage 3.0 (TID 157)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156) in 136 ms on localhost (executor driver) (154/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000154_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 155.0 in stage 3.0 (TID 158, localhost, executor driver, partition 155, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 155.0 in stage 3.0 (TID 158)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157) in 89 ms on localhost (executor driver) (155/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000155_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 156.0 in stage 3.0 (TID 159, localhost, executor driver, partition 156, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 156.0 in stage 3.0 (TID 159)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158) in 100 ms on localhost (executor driver) (156/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000156_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 157.0 in stage 3.0 (TID 160, localhost, executor driver, partition 157, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 157.0 in stage 3.0 (TID 160)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159) in 122 ms on localhost (executor driver) (157/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000157_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 158.0 in stage 3.0 (TID 161, localhost, executor driver, partition 158, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 158.0 in stage 3.0 (TID 161)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160) in 74 ms on localhost (executor driver) (158/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000158_0: Committed
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Starting task 159.0 in stage 3.0 (TID 162, localhost, executor driver, partition 159, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Running task 159.0 in stage 3.0 (TID 162)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161) in 93 ms on localhost (executor driver) (159/200)
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:12] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192612_0003_m_000159_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 160.0 in stage 3.0 (TID 163, localhost, executor driver, partition 160, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 160.0 in stage 3.0 (TID 163)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162) in 88 ms on localhost (executor driver) (160/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000160_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 161.0 in stage 3.0 (TID 164, localhost, executor driver, partition 161, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 161.0 in stage 3.0 (TID 164)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163) in 123 ms on localhost (executor driver) (161/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000161_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 162.0 in stage 3.0 (TID 165, localhost, executor driver, partition 162, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 162.0 in stage 3.0 (TID 165)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164) in 276 ms on localhost (executor driver) (162/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000162_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 163.0 in stage 3.0 (TID 166, localhost, executor driver, partition 163, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 163.0 in stage 3.0 (TID 166)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165) in 89 ms on localhost (executor driver) (163/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000163_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 164.0 in stage 3.0 (TID 167, localhost, executor driver, partition 164, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 164.0 in stage 3.0 (TID 167)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166) in 155 ms on localhost (executor driver) (164/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000164_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 165.0 in stage 3.0 (TID 168, localhost, executor driver, partition 165, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 165.0 in stage 3.0 (TID 168)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167) in 47 ms on localhost (executor driver) (165/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000165_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 166.0 in stage 3.0 (TID 169, localhost, executor driver, partition 166, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 166.0 in stage 3.0 (TID 169)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168) in 80 ms on localhost (executor driver) (166/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000166_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 167.0 in stage 3.0 (TID 170, localhost, executor driver, partition 167, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 167.0 in stage 3.0 (TID 170)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169) in 54 ms on localhost (executor driver) (167/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000167_0: Committed
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Starting task 168.0 in stage 3.0 (TID 171, localhost, executor driver, partition 168, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Running task 168.0 in stage 3.0 (TID 171)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170) in 111 ms on localhost (executor driver) (168/200)
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:13] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192613_0003_m_000168_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 169.0 in stage 3.0 (TID 172, localhost, executor driver, partition 169, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 169.0 in stage 3.0 (TID 172)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171) in 83 ms on localhost (executor driver) (169/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000169_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 170.0 in stage 3.0 (TID 173, localhost, executor driver, partition 170, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 170.0 in stage 3.0 (TID 173)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172) in 112 ms on localhost (executor driver) (170/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000170_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 171.0 in stage 3.0 (TID 174, localhost, executor driver, partition 171, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 171.0 in stage 3.0 (TID 174)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173) in 93 ms on localhost (executor driver) (171/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000171_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 172.0 in stage 3.0 (TID 175, localhost, executor driver, partition 172, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 172.0 in stage 3.0 (TID 175)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174) in 102 ms on localhost (executor driver) (172/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000172_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 173.0 in stage 3.0 (TID 176, localhost, executor driver, partition 173, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 173.0 in stage 3.0 (TID 176)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175) in 49 ms on localhost (executor driver) (173/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000173_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 174.0 in stage 3.0 (TID 177, localhost, executor driver, partition 174, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 174.0 in stage 3.0 (TID 177)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176) in 51 ms on localhost (executor driver) (174/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000174_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 175.0 in stage 3.0 (TID 178, localhost, executor driver, partition 175, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 175.0 in stage 3.0 (TID 178)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177) in 150 ms on localhost (executor driver) (175/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000175_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 176.0 in stage 3.0 (TID 179, localhost, executor driver, partition 176, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 176.0 in stage 3.0 (TID 179)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178) in 86 ms on localhost (executor driver) (176/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000176_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 177.0 in stage 3.0 (TID 180, localhost, executor driver, partition 177, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 177.0 in stage 3.0 (TID 180)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179) in 47 ms on localhost (executor driver) (177/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000177_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 178.0 in stage 3.0 (TID 181, localhost, executor driver, partition 178, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 178.0 in stage 3.0 (TID 181)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180) in 115 ms on localhost (executor driver) (178/200)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] attempt_20180427192614_0003_m_000178_0: Committed
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Starting task 179.0 in stage 3.0 (TID 182, localhost, executor driver, partition 179, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Running task 179.0 in stage 3.0 (TID 182)
[INFO ] [2018-04-27 19:26:14] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181) in 133 ms on localhost (executor driver) (179/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000179_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 180.0 in stage 3.0 (TID 183, localhost, executor driver, partition 180, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 180.0 in stage 3.0 (TID 183)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182) in 89 ms on localhost (executor driver) (180/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000180_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 181.0 in stage 3.0 (TID 184, localhost, executor driver, partition 181, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183) in 79 ms on localhost (executor driver) (181/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 181.0 in stage 3.0 (TID 184)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000181_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 182.0 in stage 3.0 (TID 185, localhost, executor driver, partition 182, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 182.0 in stage 3.0 (TID 185)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184) in 140 ms on localhost (executor driver) (182/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000182_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 183.0 in stage 3.0 (TID 186, localhost, executor driver, partition 183, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 183.0 in stage 3.0 (TID 186)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185) in 60 ms on localhost (executor driver) (183/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000183_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 184.0 in stage 3.0 (TID 187, localhost, executor driver, partition 184, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 184.0 in stage 3.0 (TID 187)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186) in 62 ms on localhost (executor driver) (184/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000184_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 185.0 in stage 3.0 (TID 188, localhost, executor driver, partition 185, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 185.0 in stage 3.0 (TID 188)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187) in 82 ms on localhost (executor driver) (185/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000185_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 186.0 in stage 3.0 (TID 189, localhost, executor driver, partition 186, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188) in 124 ms on localhost (executor driver) (186/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 186.0 in stage 3.0 (TID 189)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000186_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 187.0 in stage 3.0 (TID 190, localhost, executor driver, partition 187, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 187.0 in stage 3.0 (TID 190)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189) in 76 ms on localhost (executor driver) (187/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000187_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 188.0 in stage 3.0 (TID 191, localhost, executor driver, partition 188, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190) in 92 ms on localhost (executor driver) (188/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 188.0 in stage 3.0 (TID 191)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000188_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 189.0 in stage 3.0 (TID 192, localhost, executor driver, partition 189, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 189.0 in stage 3.0 (TID 192)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191) in 47 ms on localhost (executor driver) (189/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000189_0: Committed
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Starting task 190.0 in stage 3.0 (TID 193, localhost, executor driver, partition 190, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192) in 91 ms on localhost (executor driver) (190/200)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Running task 190.0 in stage 3.0 (TID 193)
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:15] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192615_0003_m_000190_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 191.0 in stage 3.0 (TID 194, localhost, executor driver, partition 191, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 191.0 in stage 3.0 (TID 194)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193) in 83 ms on localhost (executor driver) (191/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000191_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 192.0 in stage 3.0 (TID 195, localhost, executor driver, partition 192, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194) in 74 ms on localhost (executor driver) (192/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 192.0 in stage 3.0 (TID 195)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000192_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 193.0 in stage 3.0 (TID 196, localhost, executor driver, partition 193, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 193.0 in stage 3.0 (TID 196)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195) in 50 ms on localhost (executor driver) (193/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000193_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 194.0 in stage 3.0 (TID 197, localhost, executor driver, partition 194, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 194.0 in stage 3.0 (TID 197)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196) in 52 ms on localhost (executor driver) (194/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000194_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 195.0 in stage 3.0 (TID 198, localhost, executor driver, partition 195, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197) in 49 ms on localhost (executor driver) (195/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 195.0 in stage 3.0 (TID 198)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000195_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198). 3732 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 196.0 in stage 3.0 (TID 199, localhost, executor driver, partition 196, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 196.0 in stage 3.0 (TID 199)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198) in 121 ms on localhost (executor driver) (196/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000196_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 197.0 in stage 3.0 (TID 200, localhost, executor driver, partition 197, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 197.0 in stage 3.0 (TID 200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199) in 45 ms on localhost (executor driver) (197/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000197_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200). 3646 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 198.0 in stage 3.0 (TID 201, localhost, executor driver, partition 198, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200) in 52 ms on localhost (executor driver) (198/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 198.0 in stage 3.0 (TID 201)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000198_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Starting task 199.0 in stage 3.0 (TID 202, localhost, executor driver, partition 199, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Running task 199.0 in stage 3.0 (TID 202)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201) in 53 ms on localhost (executor driver) (199/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] attempt_20180427192616_0003_m_000199_0: Committed
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202). 3689 bytes result sent to driver
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202) in 119 ms on localhost (executor driver) (200/200)
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] ResultStage 3 (saveAsTable at HistoryDataToA.scala:98) finished in 23.698 s
[INFO ] [2018-04-27 19:26:16] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:98, took 28.389179 s
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:26:22] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 19:26:23] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-03b95344-63ed-470f-8263-ac81dd78397c
[INFO ] [2018-04-27 19:40:08] [Logging$class:logInfo:54] Running Spark version 2.2.1
[INFO ] [2018-04-27 19:40:08] [Logging$class:logInfo:54] Submitted application: anda_etl_pos_history_job
[INFO ] [2018-04-27 19:40:08] [Logging$class:logInfo:54] Changing view acls to: SAM
[INFO ] [2018-04-27 19:40:08] [Logging$class:logInfo:54] Changing modify acls to: SAM
[INFO ] [2018-04-27 19:40:08] [Logging$class:logInfo:54] Changing view acls groups to: 
[INFO ] [2018-04-27 19:40:08] [Logging$class:logInfo:54] Changing modify acls groups to: 
[INFO ] [2018-04-27 19:40:08] [Logging$class:logInfo:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(SAM); groups with view permissions: Set(); users  with modify permissions: Set(SAM); groups with modify permissions: Set()
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Successfully started service 'sparkDriver' on port 55809.
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Registering MapOutputTracker
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Registering BlockManagerMaster
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] BlockManagerMasterEndpoint up
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Created local directory at C:\Users\SAM\AppData\Local\Temp\blockmgr-1008a05b-1b18-4630-8aef-6a2af116a20e
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] MemoryStore started with capacity 1992.0 MB
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Registering OutputCommitCoordinator
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Successfully started service 'SparkUI' on port 4040.
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Starting executor ID driver on host localhost
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55818.
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Server created on 192.168.0.152:55818
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Registering BlockManager BlockManagerId(driver, 192.168.0.152, 55818, None)
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Registering block manager 192.168.0.152:55818 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.0.152, 55818, None)
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Registered BlockManager BlockManagerId(driver, 192.168.0.152, 55818, None)
[INFO ] [2018-04-27 19:40:09] [Logging$class:logInfo:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.152, 55818, None)
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Block broadcast_0 stored as values in memory (estimated size 214.5 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.8 MB)
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Added broadcast_0_piece0 in memory on 192.168.0.152:55818 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Created broadcast 0 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Block broadcast_1 stored as values in memory (estimated size 214.6 KB, free 1991.6 MB)
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1991.5 MB)
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Added broadcast_1_piece0 in memory on 192.168.0.152:55818 (size: 20.6 KB, free: 1992.0 MB)
[INFO ] [2018-04-27 19:40:10] [Logging$class:logInfo:54] Created broadcast 1 from hadoopFile at LoadDataUtil.scala:25
[INFO ] [2018-04-27 19:40:12] [Logging$class:logInfo:54] loading hive config file: file:/D:/IdeaProjects/hg_etl_pos_daily/target/classes/hive-site.xml
[INFO ] [2018-04-27 19:40:12] [Logging$class:logInfo:54] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse').
[INFO ] [2018-04-27 19:40:12] [Logging$class:logInfo:54] Warehouse path is 'file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse'.
[INFO ] [2018-04-27 19:40:12] [Logging$class:logInfo:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO ] [2018-04-27 19:40:16] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:40:16] [Logging$class:logInfo:54] Warehouse location for Hive client (version 1.2.1) is file:/D:/IdeaProjects/hg_etl_pos_daily/spark-warehouse
[INFO ] [2018-04-27 19:40:16] [Logging$class:logInfo:54] Registered StateStoreCoordinator endpoint
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: ba_model.dim_gid_drid_rel
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: array<string>
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: banner_code='R10003'
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: ba_model.original_sale_detail
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:17] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:18] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:40:22] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:40:23] [Logging$class:logInfo:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:23] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:23] [Logging$class:logInfo:54] Code generated in 204.783957 ms
[INFO ] [2018-04-27 19:40:23] [Logging$class:logInfo:54] Block broadcast_2 stored as values in memory (estimated size 224.7 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:40:23] [Logging$class:logInfo:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 21.5 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Added broadcast_2_piece0 in memory on 192.168.0.152:55818 (size: 21.5 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Created broadcast 2 from 
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Starting job: run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Got job 0 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1149)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Parents of final stage: List()
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Missing parents: List()
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149), which has no missing parents
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Block broadcast_3 stored as values in memory (estimated size 12.3 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1991.3 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Added broadcast_3_piece0 in memory on 192.168.0.152:55818 (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Adding task set 0.0 with 1 tasks
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4902 bytes)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Running task 0.0 in stage 0.0 (TID 0)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/user/hive/warehouse/ba_model.db/dim_gid_drid_rel/000000_0:0+2843378
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Code generated in 20.167759 ms
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0). 218615 bytes result sent to driver
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Finished task 0.0 in stage 0.0 (TID 0) in 505 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] ResultStage 0 (run at ThreadPoolExecutor.java:1149) finished in 0.522 s
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Job 0 finished: run at ThreadPoolExecutor.java:1149, took 0.596778 s
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Code generated in 9.255655 ms
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Removed broadcast_3_piece0 on 192.168.0.152:55818 in memory (size: 6.1 KB, free: 1991.9 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Block broadcast_4 stored as values in memory (estimated size 64.5 MB, free 1926.8 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 337.8 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Added broadcast_4_piece0 in memory on 192.168.0.152:55818 (size: 337.8 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Created broadcast 4 from run at ThreadPoolExecutor.java:1149
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Code generated in 32.374591 ms
[INFO ] [2018-04-27 19:40:24] [Logging$class:logInfo:54] Code generated in 18.82508 ms
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Starting job: saveAsTable at HistoryDataToA.scala:98
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Registering RDD 27 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Registering RDD 23 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Got job 1 (saveAsTable at HistoryDataToA.scala:98) with 200 output partitions
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Final stage: ResultStage 3 (saveAsTable at HistoryDataToA.scala:98)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Submitting ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Block broadcast_5 stored as values in memory (estimated size 12.3 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1926.5 MB)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Added broadcast_5_piece0 in memory on 192.168.0.152:55818 (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[27] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Adding task set 1.0 with 1 tasks
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Submitting ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1926.4 MB)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Added broadcast_6_piece0 in memory on 192.168.0.152:55818 (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Running task 0.0 in stage 1.0 (TID 1)
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/金额流水2018.04.24.txt:0+2890924
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[23] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0))
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Adding task set 2.0 with 1 tasks
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Code generated in 10.739173 ms
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Code generated in 12.925369 ms
[INFO ] [2018-04-27 19:40:25] [Logging$class:logInfo:54] Code generated in 66.797561 ms
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1). 1625 bytes result sent to driver
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4891 bytes)
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Running task 0.0 in stage 2.0 (TID 2)
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Finished task 0.0 in stage 1.0 (TID 1) in 1418 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] ShuffleMapStage 1 (saveAsTable at HistoryDataToA.scala:98) finished in 1.422 s
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] running: Set(ShuffleMapStage 2)
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Input split: hdfs://192.168.0.151:9000/usr/samgao/input/anda/20180424/实物流水2018.04.24.txt:0+8238018
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Code generated in 13.498915 ms
[INFO ] [2018-04-27 19:40:26] [Logging$class:logInfo:54] Code generated in 70.847501 ms
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2). 1539 bytes result sent to driver
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Finished task 0.0 in stage 2.0 (TID 2) in 3175 ms on localhost (executor driver) (1/1)
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] ShuffleMapStage 2 (saveAsTable at HistoryDataToA.scala:98) finished in 4.566 s
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] looking for newly runnable stages
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] running: Set()
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] waiting: Set(ResultStage 3)
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] failed: Set()
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Submitting ResultStage 3 (MapPartitionsRDD[32] at saveAsTable at HistoryDataToA.scala:98), which has no missing parents
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Block broadcast_7 stored as values in memory (estimated size 114.2 KB, free 1926.3 MB)
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 43.4 KB, free 1926.3 MB)
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Added broadcast_7_piece0 in memory on 192.168.0.152:55818 (size: 43.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Submitting 200 missing tasks from ResultStage 3 (MapPartitionsRDD[32] at saveAsTable at HistoryDataToA.scala:98) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Adding task set 3.0 with 200 tasks
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Running task 0.0 in stage 3.0 (TID 3)
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:29] [Logging$class:logInfo:54] Started 0 remote fetches in 6 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 11.612518 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 8.164444 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 14.143447 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 8.001329 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 11.539645 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 11.535491 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 9.152953 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Removed broadcast_6_piece0 on 192.168.0.152:55818 in memory (size: 7.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Removed broadcast_5_piece0 on 192.168.0.152:55818 in memory (size: 6.4 KB, free: 1991.6 MB)
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 20.475489 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Code generated in 14.021488 ms
[INFO ] [2018-04-27 19:40:30] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] attempt_20180427194030_0003_m_000000_0: Committed
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3). 3814 bytes result sent to driver
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Running task 1.0 in stage 3.0 (TID 4)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 0.0 in stage 3.0 (TID 3) in 1230 ms on localhost (executor driver) (1/200)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 2 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] attempt_20180427194031_0003_m_000001_0: Committed
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Starting task 2.0 in stage 3.0 (TID 5, localhost, executor driver, partition 2, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Running task 2.0 in stage 3.0 (TID 5)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 1.0 in stage 3.0 (TID 4) in 152 ms on localhost (executor driver) (2/200)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] attempt_20180427194031_0003_m_000002_0: Committed
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Starting task 3.0 in stage 3.0 (TID 6, localhost, executor driver, partition 3, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 2.0 in stage 3.0 (TID 5) in 289 ms on localhost (executor driver) (3/200)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Running task 3.0 in stage 3.0 (TID 6)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] attempt_20180427194031_0003_m_000003_0: Committed
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Starting task 4.0 in stage 3.0 (TID 7, localhost, executor driver, partition 4, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Running task 4.0 in stage 3.0 (TID 7)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 3.0 in stage 3.0 (TID 6) in 158 ms on localhost (executor driver) (4/200)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] attempt_20180427194031_0003_m_000004_0: Committed
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Starting task 5.0 in stage 3.0 (TID 8, localhost, executor driver, partition 5, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Finished task 4.0 in stage 3.0 (TID 7) in 163 ms on localhost (executor driver) (5/200)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Running task 5.0 in stage 3.0 (TID 8)
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:31] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] attempt_20180427194031_0003_m_000005_0: Committed
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Starting task 6.0 in stage 3.0 (TID 9, localhost, executor driver, partition 6, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 5.0 in stage 3.0 (TID 8) in 151 ms on localhost (executor driver) (6/200)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Running task 6.0 in stage 3.0 (TID 9)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] attempt_20180427194032_0003_m_000006_0: Committed
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Starting task 7.0 in stage 3.0 (TID 10, localhost, executor driver, partition 7, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Running task 7.0 in stage 3.0 (TID 10)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 6.0 in stage 3.0 (TID 9) in 167 ms on localhost (executor driver) (7/200)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] attempt_20180427194032_0003_m_000007_0: Committed
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Starting task 8.0 in stage 3.0 (TID 11, localhost, executor driver, partition 8, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Running task 8.0 in stage 3.0 (TID 11)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 7.0 in stage 3.0 (TID 10) in 124 ms on localhost (executor driver) (8/200)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] attempt_20180427194032_0003_m_000008_0: Committed
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Starting task 9.0 in stage 3.0 (TID 12, localhost, executor driver, partition 9, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Running task 9.0 in stage 3.0 (TID 12)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Finished task 8.0 in stage 3.0 (TID 11) in 562 ms on localhost (executor driver) (9/200)
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:32] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] attempt_20180427194032_0003_m_000009_0: Committed
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Starting task 10.0 in stage 3.0 (TID 13, localhost, executor driver, partition 10, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Running task 10.0 in stage 3.0 (TID 13)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 9.0 in stage 3.0 (TID 12) in 128 ms on localhost (executor driver) (10/200)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] attempt_20180427194033_0003_m_000010_0: Committed
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Starting task 11.0 in stage 3.0 (TID 14, localhost, executor driver, partition 11, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 10.0 in stage 3.0 (TID 13) in 144 ms on localhost (executor driver) (11/200)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Running task 11.0 in stage 3.0 (TID 14)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] attempt_20180427194033_0003_m_000011_0: Committed
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Starting task 12.0 in stage 3.0 (TID 15, localhost, executor driver, partition 12, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Running task 12.0 in stage 3.0 (TID 15)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 11.0 in stage 3.0 (TID 14) in 157 ms on localhost (executor driver) (12/200)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] attempt_20180427194033_0003_m_000012_0: Committed
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Starting task 13.0 in stage 3.0 (TID 16, localhost, executor driver, partition 13, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Running task 13.0 in stage 3.0 (TID 16)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 12.0 in stage 3.0 (TID 15) in 157 ms on localhost (executor driver) (13/200)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] attempt_20180427194033_0003_m_000013_0: Committed
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Starting task 14.0 in stage 3.0 (TID 17, localhost, executor driver, partition 14, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Running task 14.0 in stage 3.0 (TID 17)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 13.0 in stage 3.0 (TID 16) in 156 ms on localhost (executor driver) (14/200)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] attempt_20180427194033_0003_m_000014_0: Committed
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17). 3771 bytes result sent to driver
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Starting task 15.0 in stage 3.0 (TID 18, localhost, executor driver, partition 15, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Running task 15.0 in stage 3.0 (TID 18)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Finished task 14.0 in stage 3.0 (TID 17) in 306 ms on localhost (executor driver) (15/200)
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:33] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194033_0003_m_000015_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 16.0 in stage 3.0 (TID 19, localhost, executor driver, partition 16, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 16.0 in stage 3.0 (TID 19)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 15.0 in stage 3.0 (TID 18) in 98 ms on localhost (executor driver) (16/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000016_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 17.0 in stage 3.0 (TID 20, localhost, executor driver, partition 17, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 17.0 in stage 3.0 (TID 20)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 16.0 in stage 3.0 (TID 19) in 70 ms on localhost (executor driver) (17/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000017_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 18.0 in stage 3.0 (TID 21, localhost, executor driver, partition 18, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 18.0 in stage 3.0 (TID 21)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 17.0 in stage 3.0 (TID 20) in 138 ms on localhost (executor driver) (18/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000018_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 19.0 in stage 3.0 (TID 22, localhost, executor driver, partition 19, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 19.0 in stage 3.0 (TID 22)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 18.0 in stage 3.0 (TID 21) in 131 ms on localhost (executor driver) (19/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000019_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 20.0 in stage 3.0 (TID 23, localhost, executor driver, partition 20, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 19.0 in stage 3.0 (TID 22) in 155 ms on localhost (executor driver) (20/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 20.0 in stage 3.0 (TID 23)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000020_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 21.0 in stage 3.0 (TID 24, localhost, executor driver, partition 21, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 21.0 in stage 3.0 (TID 24)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 20.0 in stage 3.0 (TID 23) in 144 ms on localhost (executor driver) (21/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000021_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 22.0 in stage 3.0 (TID 25, localhost, executor driver, partition 22, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 22.0 in stage 3.0 (TID 25)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 21.0 in stage 3.0 (TID 24) in 172 ms on localhost (executor driver) (22/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000022_0: Committed
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Starting task 23.0 in stage 3.0 (TID 26, localhost, executor driver, partition 23, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Running task 23.0 in stage 3.0 (TID 26)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Finished task 22.0 in stage 3.0 (TID 25) in 121 ms on localhost (executor driver) (23/200)
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:34] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] attempt_20180427194034_0003_m_000023_0: Committed
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Starting task 24.0 in stage 3.0 (TID 27, localhost, executor driver, partition 24, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 23.0 in stage 3.0 (TID 26) in 145 ms on localhost (executor driver) (24/200)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Running task 24.0 in stage 3.0 (TID 27)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] attempt_20180427194035_0003_m_000024_0: Committed
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Starting task 25.0 in stage 3.0 (TID 28, localhost, executor driver, partition 25, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Running task 25.0 in stage 3.0 (TID 28)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 24.0 in stage 3.0 (TID 27) in 156 ms on localhost (executor driver) (25/200)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] attempt_20180427194035_0003_m_000025_0: Committed
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Starting task 26.0 in stage 3.0 (TID 29, localhost, executor driver, partition 26, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 25.0 in stage 3.0 (TID 28) in 101 ms on localhost (executor driver) (26/200)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Running task 26.0 in stage 3.0 (TID 29)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] attempt_20180427194035_0003_m_000026_0: Committed
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Starting task 27.0 in stage 3.0 (TID 30, localhost, executor driver, partition 27, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 26.0 in stage 3.0 (TID 29) in 220 ms on localhost (executor driver) (27/200)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Running task 27.0 in stage 3.0 (TID 30)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] attempt_20180427194035_0003_m_000027_0: Committed
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Starting task 28.0 in stage 3.0 (TID 31, localhost, executor driver, partition 28, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Running task 28.0 in stage 3.0 (TID 31)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 27.0 in stage 3.0 (TID 30) in 102 ms on localhost (executor driver) (28/200)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] attempt_20180427194035_0003_m_000028_0: Committed
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Starting task 29.0 in stage 3.0 (TID 32, localhost, executor driver, partition 29, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Running task 29.0 in stage 3.0 (TID 32)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Finished task 28.0 in stage 3.0 (TID 31) in 155 ms on localhost (executor driver) (29/200)
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:35] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194035_0003_m_000029_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 30.0 in stage 3.0 (TID 33, localhost, executor driver, partition 30, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 30.0 in stage 3.0 (TID 33)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 29.0 in stage 3.0 (TID 32) in 155 ms on localhost (executor driver) (30/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000030_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 31.0 in stage 3.0 (TID 34, localhost, executor driver, partition 31, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 31.0 in stage 3.0 (TID 34)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 30.0 in stage 3.0 (TID 33) in 170 ms on localhost (executor driver) (31/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000031_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34). 3771 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 32.0 in stage 3.0 (TID 35, localhost, executor driver, partition 32, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 32.0 in stage 3.0 (TID 35)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 31.0 in stage 3.0 (TID 34) in 290 ms on localhost (executor driver) (32/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000032_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 33.0 in stage 3.0 (TID 36, localhost, executor driver, partition 33, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 33.0 in stage 3.0 (TID 36)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 32.0 in stage 3.0 (TID 35) in 81 ms on localhost (executor driver) (33/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000033_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 34.0 in stage 3.0 (TID 37, localhost, executor driver, partition 34, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 34.0 in stage 3.0 (TID 37)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 33.0 in stage 3.0 (TID 36) in 72 ms on localhost (executor driver) (34/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000034_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 35.0 in stage 3.0 (TID 38, localhost, executor driver, partition 35, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 35.0 in stage 3.0 (TID 38)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 34.0 in stage 3.0 (TID 37) in 124 ms on localhost (executor driver) (35/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000035_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 36.0 in stage 3.0 (TID 39, localhost, executor driver, partition 36, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 36.0 in stage 3.0 (TID 39)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 35.0 in stage 3.0 (TID 38) in 91 ms on localhost (executor driver) (36/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000036_0: Committed
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Starting task 37.0 in stage 3.0 (TID 40, localhost, executor driver, partition 37, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Running task 37.0 in stage 3.0 (TID 40)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Finished task 36.0 in stage 3.0 (TID 39) in 100 ms on localhost (executor driver) (37/200)
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:36] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194036_0003_m_000037_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 38.0 in stage 3.0 (TID 41, localhost, executor driver, partition 38, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 38.0 in stage 3.0 (TID 41)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 37.0 in stage 3.0 (TID 40) in 100 ms on localhost (executor driver) (38/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000038_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 39.0 in stage 3.0 (TID 42, localhost, executor driver, partition 39, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 39.0 in stage 3.0 (TID 42)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 38.0 in stage 3.0 (TID 41) in 98 ms on localhost (executor driver) (39/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000039_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 40.0 in stage 3.0 (TID 43, localhost, executor driver, partition 40, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 40.0 in stage 3.0 (TID 43)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 39.0 in stage 3.0 (TID 42) in 176 ms on localhost (executor driver) (40/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000040_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 41.0 in stage 3.0 (TID 44, localhost, executor driver, partition 41, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 41.0 in stage 3.0 (TID 44)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 40.0 in stage 3.0 (TID 43) in 94 ms on localhost (executor driver) (41/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000041_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 42.0 in stage 3.0 (TID 45, localhost, executor driver, partition 42, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 42.0 in stage 3.0 (TID 45)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 41.0 in stage 3.0 (TID 44) in 101 ms on localhost (executor driver) (42/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000042_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 43.0 in stage 3.0 (TID 46, localhost, executor driver, partition 43, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 43.0 in stage 3.0 (TID 46)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 42.0 in stage 3.0 (TID 45) in 122 ms on localhost (executor driver) (43/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000043_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 44.0 in stage 3.0 (TID 47, localhost, executor driver, partition 44, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 44.0 in stage 3.0 (TID 47)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 43.0 in stage 3.0 (TID 46) in 192 ms on localhost (executor driver) (44/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000044_0: Committed
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Starting task 45.0 in stage 3.0 (TID 48, localhost, executor driver, partition 45, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Running task 45.0 in stage 3.0 (TID 48)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Finished task 44.0 in stage 3.0 (TID 47) in 112 ms on localhost (executor driver) (45/200)
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:37] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194037_0003_m_000045_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 46.0 in stage 3.0 (TID 49, localhost, executor driver, partition 46, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 45.0 in stage 3.0 (TID 48) in 99 ms on localhost (executor driver) (46/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 46.0 in stage 3.0 (TID 49)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000046_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 47.0 in stage 3.0 (TID 50, localhost, executor driver, partition 47, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 46.0 in stage 3.0 (TID 49) in 115 ms on localhost (executor driver) (47/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 47.0 in stage 3.0 (TID 50)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000047_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 48.0 in stage 3.0 (TID 51, localhost, executor driver, partition 48, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 48.0 in stage 3.0 (TID 51)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 47.0 in stage 3.0 (TID 50) in 66 ms on localhost (executor driver) (48/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000048_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 49.0 in stage 3.0 (TID 52, localhost, executor driver, partition 49, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 49.0 in stage 3.0 (TID 52)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 48.0 in stage 3.0 (TID 51) in 82 ms on localhost (executor driver) (49/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000049_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 50.0 in stage 3.0 (TID 53, localhost, executor driver, partition 50, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 50.0 in stage 3.0 (TID 53)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 49.0 in stage 3.0 (TID 52) in 144 ms on localhost (executor driver) (50/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000050_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 51.0 in stage 3.0 (TID 54, localhost, executor driver, partition 51, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 51.0 in stage 3.0 (TID 54)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 50.0 in stage 3.0 (TID 53) in 82 ms on localhost (executor driver) (51/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000051_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 52.0 in stage 3.0 (TID 55, localhost, executor driver, partition 52, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 52.0 in stage 3.0 (TID 55)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 51.0 in stage 3.0 (TID 54) in 116 ms on localhost (executor driver) (52/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000052_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 53.0 in stage 3.0 (TID 56, localhost, executor driver, partition 53, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 53.0 in stage 3.0 (TID 56)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 52.0 in stage 3.0 (TID 55) in 124 ms on localhost (executor driver) (53/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000053_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 54.0 in stage 3.0 (TID 57, localhost, executor driver, partition 54, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 54.0 in stage 3.0 (TID 57)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 53.0 in stage 3.0 (TID 56) in 145 ms on localhost (executor driver) (54/200)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] attempt_20180427194038_0003_m_000054_0: Committed
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Starting task 55.0 in stage 3.0 (TID 58, localhost, executor driver, partition 55, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Running task 55.0 in stage 3.0 (TID 58)
[INFO ] [2018-04-27 19:40:38] [Logging$class:logInfo:54] Finished task 54.0 in stage 3.0 (TID 57) in 103 ms on localhost (executor driver) (55/200)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] attempt_20180427194039_0003_m_000055_0: Committed
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Starting task 56.0 in stage 3.0 (TID 59, localhost, executor driver, partition 56, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Running task 56.0 in stage 3.0 (TID 59)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 55.0 in stage 3.0 (TID 58) in 121 ms on localhost (executor driver) (56/200)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] attempt_20180427194039_0003_m_000056_0: Committed
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Starting task 57.0 in stage 3.0 (TID 60, localhost, executor driver, partition 57, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Running task 57.0 in stage 3.0 (TID 60)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 56.0 in stage 3.0 (TID 59) in 133 ms on localhost (executor driver) (57/200)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] attempt_20180427194039_0003_m_000057_0: Committed
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Starting task 58.0 in stage 3.0 (TID 61, localhost, executor driver, partition 58, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Running task 58.0 in stage 3.0 (TID 61)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 57.0 in stage 3.0 (TID 60) in 90 ms on localhost (executor driver) (58/200)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] attempt_20180427194039_0003_m_000058_0: Committed
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Starting task 59.0 in stage 3.0 (TID 62, localhost, executor driver, partition 59, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Running task 59.0 in stage 3.0 (TID 62)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 58.0 in stage 3.0 (TID 61) in 60 ms on localhost (executor driver) (59/200)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] attempt_20180427194039_0003_m_000059_0: Committed
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Starting task 60.0 in stage 3.0 (TID 63, localhost, executor driver, partition 60, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Running task 60.0 in stage 3.0 (TID 63)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 59.0 in stage 3.0 (TID 62) in 119 ms on localhost (executor driver) (60/200)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] attempt_20180427194039_0003_m_000060_0: Committed
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Starting task 61.0 in stage 3.0 (TID 64, localhost, executor driver, partition 61, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Running task 61.0 in stage 3.0 (TID 64)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Finished task 60.0 in stage 3.0 (TID 63) in 476 ms on localhost (executor driver) (61/200)
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:39] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000061_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 62.0 in stage 3.0 (TID 65, localhost, executor driver, partition 62, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 62.0 in stage 3.0 (TID 65)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 61.0 in stage 3.0 (TID 64) in 57 ms on localhost (executor driver) (62/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000062_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 63.0 in stage 3.0 (TID 66, localhost, executor driver, partition 63, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 63.0 in stage 3.0 (TID 66)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 62.0 in stage 3.0 (TID 65) in 80 ms on localhost (executor driver) (63/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000063_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 64.0 in stage 3.0 (TID 67, localhost, executor driver, partition 64, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 64.0 in stage 3.0 (TID 67)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 63.0 in stage 3.0 (TID 66) in 120 ms on localhost (executor driver) (64/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000064_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 65.0 in stage 3.0 (TID 68, localhost, executor driver, partition 65, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 64.0 in stage 3.0 (TID 67) in 107 ms on localhost (executor driver) (65/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 65.0 in stage 3.0 (TID 68)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000065_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 66.0 in stage 3.0 (TID 69, localhost, executor driver, partition 66, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 66.0 in stage 3.0 (TID 69)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 65.0 in stage 3.0 (TID 68) in 84 ms on localhost (executor driver) (66/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000066_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 67.0 in stage 3.0 (TID 70, localhost, executor driver, partition 67, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 67.0 in stage 3.0 (TID 70)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 66.0 in stage 3.0 (TID 69) in 100 ms on localhost (executor driver) (67/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000067_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 68.0 in stage 3.0 (TID 71, localhost, executor driver, partition 68, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 68.0 in stage 3.0 (TID 71)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 67.0 in stage 3.0 (TID 70) in 98 ms on localhost (executor driver) (68/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000068_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 69.0 in stage 3.0 (TID 72, localhost, executor driver, partition 69, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 69.0 in stage 3.0 (TID 72)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 68.0 in stage 3.0 (TID 71) in 62 ms on localhost (executor driver) (69/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000069_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 70.0 in stage 3.0 (TID 73, localhost, executor driver, partition 70, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 70.0 in stage 3.0 (TID 73)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 69.0 in stage 3.0 (TID 72) in 76 ms on localhost (executor driver) (70/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000070_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 71.0 in stage 3.0 (TID 74, localhost, executor driver, partition 71, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 71.0 in stage 3.0 (TID 74)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 70.0 in stage 3.0 (TID 73) in 110 ms on localhost (executor driver) (71/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] attempt_20180427194040_0003_m_000071_0: Committed
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Starting task 72.0 in stage 3.0 (TID 75, localhost, executor driver, partition 72, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Finished task 71.0 in stage 3.0 (TID 74) in 120 ms on localhost (executor driver) (72/200)
[INFO ] [2018-04-27 19:40:40] [Logging$class:logInfo:54] Running task 72.0 in stage 3.0 (TID 75)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000072_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 73.0 in stage 3.0 (TID 76, localhost, executor driver, partition 73, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 73.0 in stage 3.0 (TID 76)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 72.0 in stage 3.0 (TID 75) in 77 ms on localhost (executor driver) (73/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000073_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 74.0 in stage 3.0 (TID 77, localhost, executor driver, partition 74, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 74.0 in stage 3.0 (TID 77)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 73.0 in stage 3.0 (TID 76) in 124 ms on localhost (executor driver) (74/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000074_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 75.0 in stage 3.0 (TID 78, localhost, executor driver, partition 75, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 75.0 in stage 3.0 (TID 78)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 74.0 in stage 3.0 (TID 77) in 133 ms on localhost (executor driver) (75/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000075_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 76.0 in stage 3.0 (TID 79, localhost, executor driver, partition 76, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 76.0 in stage 3.0 (TID 79)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 75.0 in stage 3.0 (TID 78) in 48 ms on localhost (executor driver) (76/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000076_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 77.0 in stage 3.0 (TID 80, localhost, executor driver, partition 77, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 76.0 in stage 3.0 (TID 79) in 122 ms on localhost (executor driver) (77/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 77.0 in stage 3.0 (TID 80)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000077_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 78.0 in stage 3.0 (TID 81, localhost, executor driver, partition 78, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 78.0 in stage 3.0 (TID 81)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 77.0 in stage 3.0 (TID 80) in 134 ms on localhost (executor driver) (78/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000078_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 79.0 in stage 3.0 (TID 82, localhost, executor driver, partition 79, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 79.0 in stage 3.0 (TID 82)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 78.0 in stage 3.0 (TID 81) in 121 ms on localhost (executor driver) (79/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000079_0: Committed
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Starting task 80.0 in stage 3.0 (TID 83, localhost, executor driver, partition 80, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Running task 80.0 in stage 3.0 (TID 83)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Finished task 79.0 in stage 3.0 (TID 82) in 106 ms on localhost (executor driver) (80/200)
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:41] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] attempt_20180427194041_0003_m_000080_0: Committed
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Starting task 81.0 in stage 3.0 (TID 84, localhost, executor driver, partition 81, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Running task 81.0 in stage 3.0 (TID 84)
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Finished task 80.0 in stage 3.0 (TID 83) in 527 ms on localhost (executor driver) (81/200)
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:42] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] attempt_20180427194042_0003_m_000081_0: Committed
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Starting task 82.0 in stage 3.0 (TID 85, localhost, executor driver, partition 82, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Running task 82.0 in stage 3.0 (TID 85)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Finished task 81.0 in stage 3.0 (TID 84) in 627 ms on localhost (executor driver) (82/200)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] attempt_20180427194043_0003_m_000082_0: Committed
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Starting task 83.0 in stage 3.0 (TID 86, localhost, executor driver, partition 83, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Running task 83.0 in stage 3.0 (TID 86)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Finished task 82.0 in stage 3.0 (TID 85) in 395 ms on localhost (executor driver) (83/200)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] attempt_20180427194043_0003_m_000083_0: Committed
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Starting task 84.0 in stage 3.0 (TID 87, localhost, executor driver, partition 84, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Running task 84.0 in stage 3.0 (TID 87)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Finished task 83.0 in stage 3.0 (TID 86) in 532 ms on localhost (executor driver) (84/200)
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:43] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194043_0003_m_000084_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 85.0 in stage 3.0 (TID 88, localhost, executor driver, partition 85, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 85.0 in stage 3.0 (TID 88)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 84.0 in stage 3.0 (TID 87) in 169 ms on localhost (executor driver) (85/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000085_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 86.0 in stage 3.0 (TID 89, localhost, executor driver, partition 86, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 85.0 in stage 3.0 (TID 88) in 154 ms on localhost (executor driver) (86/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 86.0 in stage 3.0 (TID 89)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000086_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 87.0 in stage 3.0 (TID 90, localhost, executor driver, partition 87, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 87.0 in stage 3.0 (TID 90)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 86.0 in stage 3.0 (TID 89) in 122 ms on localhost (executor driver) (87/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000087_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 88.0 in stage 3.0 (TID 91, localhost, executor driver, partition 88, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 88.0 in stage 3.0 (TID 91)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 87.0 in stage 3.0 (TID 90) in 78 ms on localhost (executor driver) (88/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000088_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 89.0 in stage 3.0 (TID 92, localhost, executor driver, partition 89, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 89.0 in stage 3.0 (TID 92)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 88.0 in stage 3.0 (TID 91) in 123 ms on localhost (executor driver) (89/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000089_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 90.0 in stage 3.0 (TID 93, localhost, executor driver, partition 90, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 90.0 in stage 3.0 (TID 93)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 89.0 in stage 3.0 (TID 92) in 73 ms on localhost (executor driver) (90/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000090_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 91.0 in stage 3.0 (TID 94, localhost, executor driver, partition 91, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 90.0 in stage 3.0 (TID 93) in 115 ms on localhost (executor driver) (91/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 91.0 in stage 3.0 (TID 94)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000091_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 92.0 in stage 3.0 (TID 95, localhost, executor driver, partition 92, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 92.0 in stage 3.0 (TID 95)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 91.0 in stage 3.0 (TID 94) in 74 ms on localhost (executor driver) (92/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000092_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 92.0 in stage 3.0 (TID 95) in 49 ms on localhost (executor driver) (93/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 93.0 in stage 3.0 (TID 96, localhost, executor driver, partition 93, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 93.0 in stage 3.0 (TID 96)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000093_0: Committed
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Starting task 94.0 in stage 3.0 (TID 97, localhost, executor driver, partition 94, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Finished task 93.0 in stage 3.0 (TID 96) in 98 ms on localhost (executor driver) (94/200)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Running task 94.0 in stage 3.0 (TID 97)
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:44] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] attempt_20180427194044_0003_m_000094_0: Committed
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Starting task 95.0 in stage 3.0 (TID 98, localhost, executor driver, partition 95, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Running task 95.0 in stage 3.0 (TID 98)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 94.0 in stage 3.0 (TID 97) in 101 ms on localhost (executor driver) (95/200)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] attempt_20180427194045_0003_m_000095_0: Committed
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Starting task 96.0 in stage 3.0 (TID 99, localhost, executor driver, partition 96, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 95.0 in stage 3.0 (TID 98) in 165 ms on localhost (executor driver) (96/200)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Running task 96.0 in stage 3.0 (TID 99)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] attempt_20180427194045_0003_m_000096_0: Committed
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Starting task 97.0 in stage 3.0 (TID 100, localhost, executor driver, partition 97, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 96.0 in stage 3.0 (TID 99) in 111 ms on localhost (executor driver) (97/200)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Running task 97.0 in stage 3.0 (TID 100)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] attempt_20180427194045_0003_m_000097_0: Committed
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Starting task 98.0 in stage 3.0 (TID 101, localhost, executor driver, partition 98, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Running task 98.0 in stage 3.0 (TID 101)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 97.0 in stage 3.0 (TID 100) in 79 ms on localhost (executor driver) (98/200)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] attempt_20180427194045_0003_m_000098_0: Committed
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Starting task 99.0 in stage 3.0 (TID 102, localhost, executor driver, partition 99, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Running task 99.0 in stage 3.0 (TID 102)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 98.0 in stage 3.0 (TID 101) in 125 ms on localhost (executor driver) (99/200)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] attempt_20180427194045_0003_m_000099_0: Committed
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Starting task 100.0 in stage 3.0 (TID 103, localhost, executor driver, partition 100, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 99.0 in stage 3.0 (TID 102) in 81 ms on localhost (executor driver) (100/200)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Running task 100.0 in stage 3.0 (TID 103)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] attempt_20180427194045_0003_m_000100_0: Committed
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Starting task 101.0 in stage 3.0 (TID 104, localhost, executor driver, partition 101, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Running task 101.0 in stage 3.0 (TID 104)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Finished task 100.0 in stage 3.0 (TID 103) in 87 ms on localhost (executor driver) (101/200)
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:45] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] attempt_20180427194045_0003_m_000101_0: Committed
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Starting task 102.0 in stage 3.0 (TID 105, localhost, executor driver, partition 102, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Running task 102.0 in stage 3.0 (TID 105)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 101.0 in stage 3.0 (TID 104) in 519 ms on localhost (executor driver) (102/200)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] attempt_20180427194046_0003_m_000102_0: Committed
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Starting task 103.0 in stage 3.0 (TID 106, localhost, executor driver, partition 103, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Running task 103.0 in stage 3.0 (TID 106)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 102.0 in stage 3.0 (TID 105) in 132 ms on localhost (executor driver) (103/200)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] attempt_20180427194046_0003_m_000103_0: Committed
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Starting task 104.0 in stage 3.0 (TID 107, localhost, executor driver, partition 104, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Running task 104.0 in stage 3.0 (TID 107)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 103.0 in stage 3.0 (TID 106) in 305 ms on localhost (executor driver) (104/200)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] attempt_20180427194046_0003_m_000104_0: Committed
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Starting task 105.0 in stage 3.0 (TID 108, localhost, executor driver, partition 105, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Running task 105.0 in stage 3.0 (TID 108)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 104.0 in stage 3.0 (TID 107) in 54 ms on localhost (executor driver) (105/200)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] attempt_20180427194046_0003_m_000105_0: Committed
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Starting task 106.0 in stage 3.0 (TID 109, localhost, executor driver, partition 106, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Running task 106.0 in stage 3.0 (TID 109)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 105.0 in stage 3.0 (TID 108) in 108 ms on localhost (executor driver) (106/200)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] attempt_20180427194046_0003_m_000106_0: Committed
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Starting task 107.0 in stage 3.0 (TID 110, localhost, executor driver, partition 107, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Running task 107.0 in stage 3.0 (TID 110)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Finished task 106.0 in stage 3.0 (TID 109) in 89 ms on localhost (executor driver) (107/200)
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:46] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] attempt_20180427194046_0003_m_000107_0: Committed
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Starting task 108.0 in stage 3.0 (TID 111, localhost, executor driver, partition 108, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Running task 108.0 in stage 3.0 (TID 111)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Finished task 107.0 in stage 3.0 (TID 110) in 125 ms on localhost (executor driver) (108/200)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] attempt_20180427194047_0003_m_000108_0: Committed
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Starting task 109.0 in stage 3.0 (TID 112, localhost, executor driver, partition 109, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Finished task 108.0 in stage 3.0 (TID 111) in 120 ms on localhost (executor driver) (109/200)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Running task 109.0 in stage 3.0 (TID 112)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] attempt_20180427194047_0003_m_000109_0: Committed
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Starting task 110.0 in stage 3.0 (TID 113, localhost, executor driver, partition 110, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Running task 110.0 in stage 3.0 (TID 113)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Finished task 109.0 in stage 3.0 (TID 112) in 578 ms on localhost (executor driver) (110/200)
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:47] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194047_0003_m_000110_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 111.0 in stage 3.0 (TID 114, localhost, executor driver, partition 111, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 110.0 in stage 3.0 (TID 113) in 263 ms on localhost (executor driver) (111/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 111.0 in stage 3.0 (TID 114)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000111_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 112.0 in stage 3.0 (TID 115, localhost, executor driver, partition 112, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 112.0 in stage 3.0 (TID 115)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 111.0 in stage 3.0 (TID 114) in 61 ms on localhost (executor driver) (112/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000112_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 113.0 in stage 3.0 (TID 116, localhost, executor driver, partition 113, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 113.0 in stage 3.0 (TID 116)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 112.0 in stage 3.0 (TID 115) in 56 ms on localhost (executor driver) (113/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000113_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 114.0 in stage 3.0 (TID 117, localhost, executor driver, partition 114, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 114.0 in stage 3.0 (TID 117)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 113.0 in stage 3.0 (TID 116) in 145 ms on localhost (executor driver) (114/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000114_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 115.0 in stage 3.0 (TID 118, localhost, executor driver, partition 115, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 115.0 in stage 3.0 (TID 118)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 114.0 in stage 3.0 (TID 117) in 53 ms on localhost (executor driver) (115/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000115_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 116.0 in stage 3.0 (TID 119, localhost, executor driver, partition 116, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 116.0 in stage 3.0 (TID 119)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 115.0 in stage 3.0 (TID 118) in 47 ms on localhost (executor driver) (116/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000116_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 117.0 in stage 3.0 (TID 120, localhost, executor driver, partition 117, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 117.0 in stage 3.0 (TID 120)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 116.0 in stage 3.0 (TID 119) in 47 ms on localhost (executor driver) (117/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000117_0: Committed
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Starting task 118.0 in stage 3.0 (TID 121, localhost, executor driver, partition 118, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Running task 118.0 in stage 3.0 (TID 121)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Finished task 117.0 in stage 3.0 (TID 120) in 510 ms on localhost (executor driver) (118/200)
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:48] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] attempt_20180427194048_0003_m_000118_0: Committed
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 118.0 in stage 3.0 (TID 121) in 214 ms on localhost (executor driver) (119/200)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Starting task 119.0 in stage 3.0 (TID 122, localhost, executor driver, partition 119, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Running task 119.0 in stage 3.0 (TID 122)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] attempt_20180427194049_0003_m_000119_0: Committed
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Starting task 120.0 in stage 3.0 (TID 123, localhost, executor driver, partition 120, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Running task 120.0 in stage 3.0 (TID 123)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 119.0 in stage 3.0 (TID 122) in 510 ms on localhost (executor driver) (120/200)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] attempt_20180427194049_0003_m_000120_0: Committed
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Starting task 121.0 in stage 3.0 (TID 124, localhost, executor driver, partition 121, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Running task 121.0 in stage 3.0 (TID 124)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 120.0 in stage 3.0 (TID 123) in 182 ms on localhost (executor driver) (121/200)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] attempt_20180427194049_0003_m_000121_0: Committed
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Starting task 122.0 in stage 3.0 (TID 125, localhost, executor driver, partition 122, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Running task 122.0 in stage 3.0 (TID 125)
[INFO ] [2018-04-27 19:40:49] [Logging$class:logInfo:54] Finished task 121.0 in stage 3.0 (TID 124) in 164 ms on localhost (executor driver) (122/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000122_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 123.0 in stage 3.0 (TID 126, localhost, executor driver, partition 123, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 123.0 in stage 3.0 (TID 126)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 122.0 in stage 3.0 (TID 125) in 92 ms on localhost (executor driver) (123/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000123_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 124.0 in stage 3.0 (TID 127, localhost, executor driver, partition 124, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 124.0 in stage 3.0 (TID 127)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 123.0 in stage 3.0 (TID 126) in 143 ms on localhost (executor driver) (124/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000124_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 125.0 in stage 3.0 (TID 128, localhost, executor driver, partition 125, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 125.0 in stage 3.0 (TID 128)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 124.0 in stage 3.0 (TID 127) in 48 ms on localhost (executor driver) (125/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000125_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 126.0 in stage 3.0 (TID 129, localhost, executor driver, partition 126, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 126.0 in stage 3.0 (TID 129)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 125.0 in stage 3.0 (TID 128) in 99 ms on localhost (executor driver) (126/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000126_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 127.0 in stage 3.0 (TID 130, localhost, executor driver, partition 127, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 127.0 in stage 3.0 (TID 130)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 126.0 in stage 3.0 (TID 129) in 62 ms on localhost (executor driver) (127/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000127_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 128.0 in stage 3.0 (TID 131, localhost, executor driver, partition 128, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 128.0 in stage 3.0 (TID 131)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 127.0 in stage 3.0 (TID 130) in 68 ms on localhost (executor driver) (128/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000128_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 129.0 in stage 3.0 (TID 132, localhost, executor driver, partition 129, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 129.0 in stage 3.0 (TID 132)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 128.0 in stage 3.0 (TID 131) in 160 ms on localhost (executor driver) (129/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000129_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 130.0 in stage 3.0 (TID 133, localhost, executor driver, partition 130, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 130.0 in stage 3.0 (TID 133)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 129.0 in stage 3.0 (TID 132) in 99 ms on localhost (executor driver) (130/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000130_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 131.0 in stage 3.0 (TID 134, localhost, executor driver, partition 131, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 130.0 in stage 3.0 (TID 133) in 56 ms on localhost (executor driver) (131/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 131.0 in stage 3.0 (TID 134)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000131_0: Committed
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Starting task 132.0 in stage 3.0 (TID 135, localhost, executor driver, partition 132, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Running task 132.0 in stage 3.0 (TID 135)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Finished task 131.0 in stage 3.0 (TID 134) in 69 ms on localhost (executor driver) (132/200)
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:50] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194050_0003_m_000132_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 133.0 in stage 3.0 (TID 136, localhost, executor driver, partition 133, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 133.0 in stage 3.0 (TID 136)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 132.0 in stage 3.0 (TID 135) in 175 ms on localhost (executor driver) (133/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000133_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 134.0 in stage 3.0 (TID 137, localhost, executor driver, partition 134, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 134.0 in stage 3.0 (TID 137)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 133.0 in stage 3.0 (TID 136) in 89 ms on localhost (executor driver) (134/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000134_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 135.0 in stage 3.0 (TID 138, localhost, executor driver, partition 135, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 135.0 in stage 3.0 (TID 138)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 134.0 in stage 3.0 (TID 137) in 85 ms on localhost (executor driver) (135/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000135_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 136.0 in stage 3.0 (TID 139, localhost, executor driver, partition 136, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 136.0 in stage 3.0 (TID 139)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 135.0 in stage 3.0 (TID 138) in 139 ms on localhost (executor driver) (136/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000136_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 137.0 in stage 3.0 (TID 140, localhost, executor driver, partition 137, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 137.0 in stage 3.0 (TID 140)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 136.0 in stage 3.0 (TID 139) in 120 ms on localhost (executor driver) (137/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000137_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 138.0 in stage 3.0 (TID 141, localhost, executor driver, partition 138, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 137.0 in stage 3.0 (TID 140) in 89 ms on localhost (executor driver) (138/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 138.0 in stage 3.0 (TID 141)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000138_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 139.0 in stage 3.0 (TID 142, localhost, executor driver, partition 139, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 138.0 in stage 3.0 (TID 141) in 102 ms on localhost (executor driver) (139/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 139.0 in stage 3.0 (TID 142)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000139_0: Committed
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142). 3771 bytes result sent to driver
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Starting task 140.0 in stage 3.0 (TID 143, localhost, executor driver, partition 140, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Running task 140.0 in stage 3.0 (TID 143)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Finished task 139.0 in stage 3.0 (TID 142) in 129 ms on localhost (executor driver) (140/200)
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:51] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] attempt_20180427194051_0003_m_000140_0: Committed
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Starting task 141.0 in stage 3.0 (TID 144, localhost, executor driver, partition 141, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Running task 141.0 in stage 3.0 (TID 144)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 140.0 in stage 3.0 (TID 143) in 519 ms on localhost (executor driver) (141/200)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] attempt_20180427194052_0003_m_000141_0: Committed
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Starting task 142.0 in stage 3.0 (TID 145, localhost, executor driver, partition 142, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Running task 142.0 in stage 3.0 (TID 145)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 141.0 in stage 3.0 (TID 144) in 177 ms on localhost (executor driver) (142/200)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] attempt_20180427194052_0003_m_000142_0: Committed
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Starting task 143.0 in stage 3.0 (TID 146, localhost, executor driver, partition 143, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Running task 143.0 in stage 3.0 (TID 146)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 142.0 in stage 3.0 (TID 145) in 119 ms on localhost (executor driver) (143/200)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] attempt_20180427194052_0003_m_000143_0: Committed
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Starting task 144.0 in stage 3.0 (TID 147, localhost, executor driver, partition 144, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Running task 144.0 in stage 3.0 (TID 147)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 143.0 in stage 3.0 (TID 146) in 48 ms on localhost (executor driver) (144/200)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] attempt_20180427194052_0003_m_000144_0: Committed
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Starting task 145.0 in stage 3.0 (TID 148, localhost, executor driver, partition 145, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Running task 145.0 in stage 3.0 (TID 148)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 144.0 in stage 3.0 (TID 147) in 70 ms on localhost (executor driver) (145/200)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] attempt_20180427194052_0003_m_000145_0: Committed
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Starting task 146.0 in stage 3.0 (TID 149, localhost, executor driver, partition 146, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Running task 146.0 in stage 3.0 (TID 149)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 145.0 in stage 3.0 (TID 148) in 83 ms on localhost (executor driver) (146/200)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] attempt_20180427194052_0003_m_000146_0: Committed
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Starting task 147.0 in stage 3.0 (TID 150, localhost, executor driver, partition 147, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Running task 147.0 in stage 3.0 (TID 150)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Finished task 146.0 in stage 3.0 (TID 149) in 73 ms on localhost (executor driver) (147/200)
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:52] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194052_0003_m_000147_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 148.0 in stage 3.0 (TID 151, localhost, executor driver, partition 148, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 147.0 in stage 3.0 (TID 150) in 115 ms on localhost (executor driver) (148/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 148.0 in stage 3.0 (TID 151)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000148_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 149.0 in stage 3.0 (TID 152, localhost, executor driver, partition 149, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 149.0 in stage 3.0 (TID 152)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 148.0 in stage 3.0 (TID 151) in 159 ms on localhost (executor driver) (149/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000149_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 150.0 in stage 3.0 (TID 153, localhost, executor driver, partition 150, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 150.0 in stage 3.0 (TID 153)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 149.0 in stage 3.0 (TID 152) in 94 ms on localhost (executor driver) (150/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000150_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 151.0 in stage 3.0 (TID 154, localhost, executor driver, partition 151, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 151.0 in stage 3.0 (TID 154)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 150.0 in stage 3.0 (TID 153) in 51 ms on localhost (executor driver) (151/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000151_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 152.0 in stage 3.0 (TID 155, localhost, executor driver, partition 152, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 152.0 in stage 3.0 (TID 155)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 151.0 in stage 3.0 (TID 154) in 42 ms on localhost (executor driver) (152/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000152_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 153.0 in stage 3.0 (TID 156, localhost, executor driver, partition 153, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 152.0 in stage 3.0 (TID 155) in 109 ms on localhost (executor driver) (153/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 153.0 in stage 3.0 (TID 156)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000153_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 154.0 in stage 3.0 (TID 157, localhost, executor driver, partition 154, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 154.0 in stage 3.0 (TID 157)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 153.0 in stage 3.0 (TID 156) in 139 ms on localhost (executor driver) (154/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000154_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 155.0 in stage 3.0 (TID 158, localhost, executor driver, partition 155, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 155.0 in stage 3.0 (TID 158)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 154.0 in stage 3.0 (TID 157) in 89 ms on localhost (executor driver) (155/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000155_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 156.0 in stage 3.0 (TID 159, localhost, executor driver, partition 156, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 156.0 in stage 3.0 (TID 159)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 155.0 in stage 3.0 (TID 158) in 150 ms on localhost (executor driver) (156/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000156_0: Committed
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Starting task 157.0 in stage 3.0 (TID 160, localhost, executor driver, partition 157, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Running task 157.0 in stage 3.0 (TID 160)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Finished task 156.0 in stage 3.0 (TID 159) in 72 ms on localhost (executor driver) (157/200)
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:53] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] attempt_20180427194053_0003_m_000157_0: Committed
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Starting task 158.0 in stage 3.0 (TID 161, localhost, executor driver, partition 158, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Running task 158.0 in stage 3.0 (TID 161)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 157.0 in stage 3.0 (TID 160) in 111 ms on localhost (executor driver) (158/200)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] attempt_20180427194054_0003_m_000158_0: Committed
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Starting task 159.0 in stage 3.0 (TID 162, localhost, executor driver, partition 159, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Running task 159.0 in stage 3.0 (TID 162)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 158.0 in stage 3.0 (TID 161) in 107 ms on localhost (executor driver) (159/200)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] attempt_20180427194054_0003_m_000159_0: Committed
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Starting task 160.0 in stage 3.0 (TID 163, localhost, executor driver, partition 160, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Running task 160.0 in stage 3.0 (TID 163)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 159.0 in stage 3.0 (TID 162) in 139 ms on localhost (executor driver) (160/200)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] attempt_20180427194054_0003_m_000160_0: Committed
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Starting task 161.0 in stage 3.0 (TID 164, localhost, executor driver, partition 161, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Running task 161.0 in stage 3.0 (TID 164)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 160.0 in stage 3.0 (TID 163) in 534 ms on localhost (executor driver) (161/200)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] attempt_20180427194054_0003_m_000161_0: Committed
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Starting task 162.0 in stage 3.0 (TID 165, localhost, executor driver, partition 162, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Running task 162.0 in stage 3.0 (TID 165)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Finished task 161.0 in stage 3.0 (TID 164) in 166 ms on localhost (executor driver) (162/200)
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:54] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] attempt_20180427194054_0003_m_000162_0: Committed
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Starting task 163.0 in stage 3.0 (TID 166, localhost, executor driver, partition 163, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Running task 163.0 in stage 3.0 (TID 166)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 162.0 in stage 3.0 (TID 165) in 140 ms on localhost (executor driver) (163/200)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] attempt_20180427194055_0003_m_000163_0: Committed
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Starting task 164.0 in stage 3.0 (TID 167, localhost, executor driver, partition 164, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Running task 164.0 in stage 3.0 (TID 167)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 163.0 in stage 3.0 (TID 166) in 50 ms on localhost (executor driver) (164/200)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] attempt_20180427194055_0003_m_000164_0: Committed
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Starting task 165.0 in stage 3.0 (TID 168, localhost, executor driver, partition 165, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Running task 165.0 in stage 3.0 (TID 168)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 164.0 in stage 3.0 (TID 167) in 88 ms on localhost (executor driver) (165/200)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] attempt_20180427194055_0003_m_000165_0: Committed
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168). 3642 bytes result sent to driver
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Starting task 166.0 in stage 3.0 (TID 169, localhost, executor driver, partition 166, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Running task 166.0 in stage 3.0 (TID 169)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 165.0 in stage 3.0 (TID 168) in 87 ms on localhost (executor driver) (166/200)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] attempt_20180427194055_0003_m_000166_0: Committed
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Starting task 167.0 in stage 3.0 (TID 170, localhost, executor driver, partition 167, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Running task 167.0 in stage 3.0 (TID 170)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 166.0 in stage 3.0 (TID 169) in 67 ms on localhost (executor driver) (167/200)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] attempt_20180427194055_0003_m_000167_0: Committed
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Starting task 168.0 in stage 3.0 (TID 171, localhost, executor driver, partition 168, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Running task 168.0 in stage 3.0 (TID 171)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Finished task 167.0 in stage 3.0 (TID 170) in 497 ms on localhost (executor driver) (168/200)
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:55] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194055_0003_m_000168_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 169.0 in stage 3.0 (TID 172, localhost, executor driver, partition 169, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 169.0 in stage 3.0 (TID 172)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 168.0 in stage 3.0 (TID 171) in 129 ms on localhost (executor driver) (169/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000169_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 170.0 in stage 3.0 (TID 173, localhost, executor driver, partition 170, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 170.0 in stage 3.0 (TID 173)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 169.0 in stage 3.0 (TID 172) in 131 ms on localhost (executor driver) (170/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000170_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 171.0 in stage 3.0 (TID 174, localhost, executor driver, partition 171, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 171.0 in stage 3.0 (TID 174)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 170.0 in stage 3.0 (TID 173) in 77 ms on localhost (executor driver) (171/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000171_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 172.0 in stage 3.0 (TID 175, localhost, executor driver, partition 172, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 172.0 in stage 3.0 (TID 175)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 171.0 in stage 3.0 (TID 174) in 166 ms on localhost (executor driver) (172/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000172_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 173.0 in stage 3.0 (TID 176, localhost, executor driver, partition 173, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 173.0 in stage 3.0 (TID 176)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 172.0 in stage 3.0 (TID 175) in 120 ms on localhost (executor driver) (173/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000173_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 174.0 in stage 3.0 (TID 177, localhost, executor driver, partition 174, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 174.0 in stage 3.0 (TID 177)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 173.0 in stage 3.0 (TID 176) in 126 ms on localhost (executor driver) (174/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000174_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 175.0 in stage 3.0 (TID 178, localhost, executor driver, partition 175, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 175.0 in stage 3.0 (TID 178)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 174.0 in stage 3.0 (TID 177) in 251 ms on localhost (executor driver) (175/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000175_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 176.0 in stage 3.0 (TID 179, localhost, executor driver, partition 176, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 176.0 in stage 3.0 (TID 179)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 175.0 in stage 3.0 (TID 178) in 61 ms on localhost (executor driver) (176/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000176_0: Committed
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Starting task 177.0 in stage 3.0 (TID 180, localhost, executor driver, partition 177, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Running task 177.0 in stage 3.0 (TID 180)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Finished task 176.0 in stage 3.0 (TID 179) in 53 ms on localhost (executor driver) (177/200)
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:56] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194056_0003_m_000177_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 178.0 in stage 3.0 (TID 181, localhost, executor driver, partition 178, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 178.0 in stage 3.0 (TID 181)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 177.0 in stage 3.0 (TID 180) in 46 ms on localhost (executor driver) (178/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000178_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 179.0 in stage 3.0 (TID 182, localhost, executor driver, partition 179, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 178.0 in stage 3.0 (TID 181) in 42 ms on localhost (executor driver) (179/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 179.0 in stage 3.0 (TID 182)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000179_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 180.0 in stage 3.0 (TID 183, localhost, executor driver, partition 180, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 180.0 in stage 3.0 (TID 183)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 179.0 in stage 3.0 (TID 182) in 524 ms on localhost (executor driver) (180/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000180_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 181.0 in stage 3.0 (TID 184, localhost, executor driver, partition 181, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 181.0 in stage 3.0 (TID 184)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 180.0 in stage 3.0 (TID 183) in 61 ms on localhost (executor driver) (181/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000181_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 182.0 in stage 3.0 (TID 185, localhost, executor driver, partition 182, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 182.0 in stage 3.0 (TID 185)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 181.0 in stage 3.0 (TID 184) in 83 ms on localhost (executor driver) (182/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000182_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 183.0 in stage 3.0 (TID 186, localhost, executor driver, partition 183, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 183.0 in stage 3.0 (TID 186)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 182.0 in stage 3.0 (TID 185) in 79 ms on localhost (executor driver) (183/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000183_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 184.0 in stage 3.0 (TID 187, localhost, executor driver, partition 184, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 183.0 in stage 3.0 (TID 186) in 73 ms on localhost (executor driver) (184/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 184.0 in stage 3.0 (TID 187)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000184_0: Committed
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Starting task 185.0 in stage 3.0 (TID 188, localhost, executor driver, partition 185, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Running task 185.0 in stage 3.0 (TID 188)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Finished task 184.0 in stage 3.0 (TID 187) in 51 ms on localhost (executor driver) (185/200)
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:57] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] attempt_20180427194057_0003_m_000185_0: Committed
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Starting task 186.0 in stage 3.0 (TID 189, localhost, executor driver, partition 186, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Running task 186.0 in stage 3.0 (TID 189)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 185.0 in stage 3.0 (TID 188) in 88 ms on localhost (executor driver) (186/200)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] attempt_20180427194058_0003_m_000186_0: Committed
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Starting task 187.0 in stage 3.0 (TID 190, localhost, executor driver, partition 187, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Running task 187.0 in stage 3.0 (TID 190)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 186.0 in stage 3.0 (TID 189) in 132 ms on localhost (executor driver) (187/200)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] attempt_20180427194058_0003_m_000187_0: Committed
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Starting task 188.0 in stage 3.0 (TID 191, localhost, executor driver, partition 188, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Running task 188.0 in stage 3.0 (TID 191)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 187.0 in stage 3.0 (TID 190) in 47 ms on localhost (executor driver) (188/200)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] attempt_20180427194058_0003_m_000188_0: Committed
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Starting task 189.0 in stage 3.0 (TID 192, localhost, executor driver, partition 189, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Running task 189.0 in stage 3.0 (TID 192)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 188.0 in stage 3.0 (TID 191) in 55 ms on localhost (executor driver) (189/200)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] attempt_20180427194058_0003_m_000189_0: Committed
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Starting task 190.0 in stage 3.0 (TID 193, localhost, executor driver, partition 190, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Running task 190.0 in stage 3.0 (TID 193)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 189.0 in stage 3.0 (TID 192) in 84 ms on localhost (executor driver) (190/200)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] attempt_20180427194058_0003_m_000190_0: Committed
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Starting task 191.0 in stage 3.0 (TID 194, localhost, executor driver, partition 191, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Running task 191.0 in stage 3.0 (TID 194)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 190.0 in stage 3.0 (TID 193) in 50 ms on localhost (executor driver) (191/200)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] attempt_20180427194058_0003_m_000191_0: Committed
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Starting task 192.0 in stage 3.0 (TID 195, localhost, executor driver, partition 192, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Running task 192.0 in stage 3.0 (TID 195)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Finished task 191.0 in stage 3.0 (TID 194) in 559 ms on localhost (executor driver) (192/200)
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:58] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] attempt_20180427194058_0003_m_000192_0: Committed
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Starting task 193.0 in stage 3.0 (TID 196, localhost, executor driver, partition 193, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Running task 193.0 in stage 3.0 (TID 196)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 192.0 in stage 3.0 (TID 195) in 130 ms on localhost (executor driver) (193/200)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] attempt_20180427194059_0003_m_000193_0: Committed
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Starting task 194.0 in stage 3.0 (TID 197, localhost, executor driver, partition 194, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Running task 194.0 in stage 3.0 (TID 197)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 193.0 in stage 3.0 (TID 196) in 167 ms on localhost (executor driver) (194/200)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] attempt_20180427194059_0003_m_000194_0: Committed
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Starting task 195.0 in stage 3.0 (TID 198, localhost, executor driver, partition 195, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Running task 195.0 in stage 3.0 (TID 198)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 194.0 in stage 3.0 (TID 197) in 490 ms on localhost (executor driver) (195/200)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 1 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] attempt_20180427194059_0003_m_000195_0: Committed
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Starting task 196.0 in stage 3.0 (TID 199, localhost, executor driver, partition 196, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Running task 196.0 in stage 3.0 (TID 199)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 195.0 in stage 3.0 (TID 198) in 60 ms on localhost (executor driver) (196/200)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] attempt_20180427194059_0003_m_000196_0: Committed
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Starting task 197.0 in stage 3.0 (TID 200, localhost, executor driver, partition 197, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Running task 197.0 in stage 3.0 (TID 200)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 196.0 in stage 3.0 (TID 199) in 40 ms on localhost (executor driver) (197/200)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] attempt_20180427194059_0003_m_000197_0: Committed
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Starting task 198.0 in stage 3.0 (TID 201, localhost, executor driver, partition 198, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Running task 198.0 in stage 3.0 (TID 201)
[INFO ] [2018-04-27 19:40:59] [Logging$class:logInfo:54] Finished task 197.0 in stage 3.0 (TID 200) in 160 ms on localhost (executor driver) (198/200)
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] attempt_20180427194100_0003_m_000198_0: Committed
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201). 3685 bytes result sent to driver
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Starting task 199.0 in stage 3.0 (TID 202, localhost, executor driver, partition 199, ANY, 5054 bytes)
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Running task 199.0 in stage 3.0 (TID 202)
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Finished task 198.0 in stage 3.0 (TID 201) in 519 ms on localhost (executor driver) (199/200)
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Getting 1 non-empty blocks out of 1 blocks
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Started 0 remote fetches in 0 ms
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bar_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "HIVE_TYPE_STRING" : "string"
    }
  }, {
    "name" : "flow_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "banner_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "retailer_shop_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "good_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trade_date_timestamp",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "card_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "quantity",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "amount",
    "type" : "float",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "pay_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "is_useful",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary bar_code (UTF8);
  optional binary flow_no (UTF8);
  optional binary banner_code (UTF8);
  optional binary retailer_shop_code (UTF8);
  optional binary good_code (UTF8);
  optional binary trade_date_time (UTF8);
  optional int64 trade_date_timestamp;
  optional binary card_code (UTF8);
  optional float quantity;
  optional float price;
  optional float amount;
  optional binary pay_type (UTF8);
  optional int32 is_useful;
}

       
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] attempt_20180427194100_0003_m_000199_0: Committed
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202). 3728 bytes result sent to driver
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Finished task 199.0 in stage 3.0 (TID 202) in 57 ms on localhost (executor driver) (200/200)
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] ResultStage 3 (saveAsTable at HistoryDataToA.scala:98) finished in 30.629 s
[INFO ] [2018-04-27 19:41:00] [Logging$class:logInfo:54] Job 1 finished: saveAsTable at HistoryDataToA.scala:98, took 35.325816 s
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Job null committed.
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: bigint
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: float
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: string
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Parsing command: int
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Stopped Spark web UI at http://192.168.0.152:4040
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] MapOutputTrackerMasterEndpoint stopped!
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] MemoryStore cleared
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] BlockManager stopped
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] BlockManagerMaster stopped
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] OutputCommitCoordinator stopped!
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Successfully stopped SparkContext
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Shutdown hook called
[INFO ] [2018-04-27 19:41:06] [Logging$class:logInfo:54] Deleting directory C:\Users\SAM\AppData\Local\Temp\spark-c7245ca1-3e90-4677-a34e-601693dbaa5b
